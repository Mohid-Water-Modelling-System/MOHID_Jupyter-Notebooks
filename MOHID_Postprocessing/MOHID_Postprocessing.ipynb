{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOHID Water\n",
    "\n",
    "This Jupyter Notebook aims to help analyse the results of the MOHID Water model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note 1**: Execute each cell through the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button from the top MENU (or keyboard shortcut `Shift` + `Enter`).<br>\n",
    "<br>\n",
    "**Note 2**: Use the Kernel and Cell menus to restart the kernel and clear outputs.<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Table of contents\n",
    "- [1. Import required libraries](#1.-Import-required-libraries)\n",
    "- [2. Time series](#2.-Time-series)\n",
    "    - [2.1 Convert and merge multiple MOHID time series files to csv](#2.1-Convert-and-merge-multiple-MOHID-time-series-files-to-csv)\n",
    "    - [2.2 Extract time series from HDF5 files](#2.2-Extract-time-series-from-HDF5-files)\n",
    "        - [2.2.1 Read one or multiple MOHID HDF5 files](#2.2.1-Read-one-or-multiple-MOHID-HDF5-files)\n",
    "        - [2.2.2 Load or create a new file with monitoring stations](#2.2.2-Load-or-create-a-new-file-with-monitoring-stations)\n",
    "        - [2.2.3 Adjust or define new monitoring stations on the map](#2.2.3-Adjust-or-define-new-monitoring-stations-on-the-map)\n",
    "        - [2.2.4 Create Input_table.dat](#3.1.4-Create-Input_table.dat)\n",
    "        - [2.2.5 Create InputValida4D.dat](#2.2.5-Create-InputValida4D.dat)\n",
    "        - [2.2.6 Run Valida4D tool](#2.2.6-Run-Valida4D-tool)\n",
    "        - [2.2.7 Convert OutTable.dat to csv files](#2.2.7-Convert-OutTable.dat-to-csv-files)\n",
    "    - [2.3 Load csv files](#2.3-Load-csv-files)\n",
    "    - [2.4 Plot time series](#2.4-Plot-time-series)\n",
    "    - [2.5 Compare with measurements](#2.5-Compare-with-measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from ipyleaflet import Map, TileLayer, DrawControl, GeoJSON, Marker, basemaps, Popup, Polyline, Circle, GeoData, Polygon, LayerGroup\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "from folium.plugins import MeasureControl\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import requests\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Video\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from urllib.request import Request, urlopen\n",
    "from PIL import Image\n",
    "import io\n",
    "from math import radians, cos, sin\n",
    "import matplotlib.colors as mcolors\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Convert and merge multiple MOHID time series files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dir = os.path.join(os.getcwd(),'res') #files can be inside subfolders of master_dir\n",
    "filename='res.ets' \n",
    "output_csv = os.path.join(os.getcwd(),'res','res.csv') \n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"Merge_TimeSeries\")\n",
    "script_name = os.path.join(script_folder, \"merge_timeseries.py\")\n",
    "input_file  = os.path.join(script_folder, \"input_merge_timeseries.py\")\n",
    "\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"master_dir       = r'{master_dir}'\\n\")\n",
    "    f.write(f\"filename         =r'{filename}'\\n\")\n",
    "    f.write(f\"output_csv=r'{output_csv}'\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", os.path.basename(script_name)],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Extract time series from HDF5 files\n",
    "#It uses Valida4D from MOHID tools (https://github.com/Mohid-Water-Modelling-System/Mohid/tree/master/Software/SmallTools/Valida4D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.1 Read one or multiple MOHID HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_files    = True\n",
    "backup_root       = os.path.join(os.getcwd(),'res')\n",
    "hdf5_file         =r'Hydrodynamic_2_Surface.hdf5'\n",
    "start_date_str    = '2025-9-25'\n",
    "end_date_str      = '2025-9-27'\n",
    "\n",
    "# ----------------------------------------\n",
    "def collect_hdf5_paths(root, h5file, sd, ed):\n",
    "    paths = []\n",
    "    for entry in os.scandir(root):\n",
    "        if not entry.is_dir():\n",
    "            continue\n",
    "        try:\n",
    "            day = datetime.strptime(entry.name.split('_')[0], \"%Y%m%d\").date()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if sd <= day <= ed:\n",
    "            # Look directly inside the date-folder\n",
    "            pattern = os.path.join(entry.path, h5file)\n",
    "            for f in glob.glob(pattern):\n",
    "                if os.path.isfile(f):\n",
    "                    paths.append(f)\n",
    "    return sorted(paths)\n",
    "# ----------------------------------------\n",
    "\n",
    "if multiple_files:\n",
    "    sd = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "    ed = datetime.strptime(end_date_str,   \"%Y-%m-%d\").date()\n",
    "    \n",
    "    hdf5_files = collect_hdf5_paths(backup_root, hdf5_file, sd, ed)\n",
    "else:\n",
    "    hdf5_files = [os.path.join(backup_root, hdf5_file)]\n",
    "\n",
    "if not hdf5_files:\n",
    "    raise RuntimeError(f\"No  HDF5s in {hdf5_file} between {start_date_str} and {end_date_str}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# GRID\n",
    "# ----------------------------------------\n",
    "with h5py.File(hdf5_files[0], \"r\") as h5f:\n",
    "    Xr = h5f[\"Grid\"][\"Longitude\"][:]\n",
    "    Yr = h5f[\"Grid\"][\"Latitude\"][:]\n",
    "    zi = h5f[\"Grid\"][\"Bathymetry\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.2 Load or create a new file with monitoring stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_file = 'stations.csv'\n",
    "\n",
    "# Load existing stations (format: lon lat name)\n",
    "if os.path.exists(stations_file):\n",
    "    stations_df = pd.read_csv(\n",
    "        stations_file,\n",
    "        sep=',',\n",
    "        header=None,\n",
    "        names=['lon', 'lat', 'station_name'],\n",
    "        engine='python'\n",
    "    )\n",
    "    print(f\"Loaded {len(stations_df)} stations from {stations_file}\")\n",
    "else:\n",
    "    stations_df = pd.DataFrame(columns=['lon', 'lat', 'station_name'])\n",
    "    print(f\"No '{stations_file}' found. Starting with zero stations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.3 Adjust or define new monitoring stations on the map  \n",
    "#Move or draw markers on the map to define the monitoring stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Start timing\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------------\n",
    "# Assume Xr, Yr, zi defined elsewhere\n",
    "# -------------------------------\n",
    "LonGrid = np.array(Xr)\n",
    "LatGrid = np.array(Yr)\n",
    "\n",
    "# -------------------------------\n",
    "# Build discrete colormap\n",
    "# -------------------------------\n",
    "_nbins = 10\n",
    "_bins = None\n",
    "_discrete_colors = None\n",
    "\n",
    "def map_value_to_color(value):\n",
    "    if value == -99:\n",
    "        return \"#ffffff00\"\n",
    "    idx = np.digitize(value, _bins) - 1\n",
    "    idx = int(np.clip(idx, 0, _nbins - 1))\n",
    "    return _discrete_colors[idx]\n",
    "\n",
    "def precompute_color_grid(zi, nbins=10):\n",
    "    global _bins, _nbins, _discrete_colors\n",
    "    _nbins = nbins\n",
    "    valid = zi != -99\n",
    "    if np.any(valid):\n",
    "        vmin, vmax = zi[valid].min(), zi[valid].max()\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "    _bins = np.linspace(vmin, vmax, nbins + 1)\n",
    "    cmap = plt.colormaps.get_cmap('viridis')\n",
    "    _discrete_colors = [mcolors.to_hex(c) for c in cmap(np.linspace(0, 1, nbins))]\n",
    "    return np.vectorize(map_value_to_color)(zi)\n",
    "\n",
    "color_mapped_zi = precompute_color_grid(zi)\n",
    "\n",
    "# -------------------------------\n",
    "# Create base map\n",
    "# -------------------------------\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "m = Map(center=(LatGrid.mean(), LonGrid.mean()), zoom=8)\n",
    "\n",
    "# -------------------------------\n",
    "# Split raster into GeoJSON blocks\n",
    "# -------------------------------\n",
    "lon_sw = LonGrid[:-1, :-1]; lon_se = LonGrid[:-1, 1:]\n",
    "lon_ne = LonGrid[1:, 1:];   lon_nw = LonGrid[1:, :-1]\n",
    "lat_sw = LatGrid[:-1, :-1]; lat_se = LatGrid[:-1, 1:]\n",
    "lat_ne = LatGrid[1:, 1:];   lat_nw = LatGrid[1:, :-1]\n",
    "\n",
    "block_size = 10\n",
    "block_layers = {}\n",
    "\n",
    "def generate_block_geojson(br, bc):\n",
    "    features = []\n",
    "    rows, cols = zi.shape\n",
    "    i0, i1 = br * block_size, min((br + 1) * block_size, rows)\n",
    "    j0, j1 = bc * block_size, min((bc + 1) * block_size, cols)\n",
    "    for i in range(i0, i1):\n",
    "        for j in range(j0, j1):\n",
    "            if zi[i, j] == -99:\n",
    "                continue\n",
    "            coords = [[\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])],\n",
    "                [float(lon_se[i, j]), float(lat_se[i, j])],\n",
    "                [float(lon_ne[i, j]), float(lat_ne[i, j])],\n",
    "                [float(lon_nw[i, j]), float(lat_nw[i, j])],\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])]\n",
    "            ]]\n",
    "            feat = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\"type\": \"Polygon\", \"coordinates\": coords},\n",
    "                \"properties\": {\n",
    "                    \"fill\": map_value_to_color(zi[i, j]),\n",
    "                    \"stroke\": \"#000000\",\n",
    "                    \"fill-opacity\": 0.5,\n",
    "                    \"stroke-width\": 0.2,\n",
    "                    \"i\": i, \"j\": j\n",
    "                }\n",
    "            }\n",
    "            features.append(feat)\n",
    "    return {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "def update_all_blocks():\n",
    "    for lyr in block_layers.values():\n",
    "        m.remove_layer(lyr)\n",
    "    block_layers.clear()\n",
    "    rows, cols = zi.shape\n",
    "    n_br = (rows + block_size - 1) // block_size\n",
    "    n_bc = (cols + block_size - 1) // block_size\n",
    "    for br in range(n_br):\n",
    "        for bc in range(n_bc):\n",
    "            fc = generate_block_geojson(br, bc)\n",
    "            if not fc[\"features\"]:\n",
    "                continue\n",
    "            layer = GeoJSON(\n",
    "                data=fc,\n",
    "                style_callback=lambda f: {\n",
    "                    \"fillColor\": f[\"properties\"][\"fill\"],\n",
    "                    \"color\": f[\"properties\"][\"stroke\"],\n",
    "                    \"weight\": f[\"properties\"][\"stroke-width\"],\n",
    "                    \"fillOpacity\": f[\"properties\"][\"fill-opacity\"],\n",
    "                }\n",
    "            )\n",
    "            m.add_layer(layer)\n",
    "            block_layers[(br, bc)] = layer\n",
    "\n",
    "update_all_blocks()\n",
    "print(f\"Raster layering time: {time.time() - start_time:.2f} sec\")\n",
    "\n",
    "# -------------------------------\n",
    "# Station markers\n",
    "# -------------------------------\n",
    "markers_dict = {}\n",
    "marker_counter = 0\n",
    "preloaded_ids = []   # will capture IDs of originally loaded stations\n",
    "\n",
    "# Add each preloaded station and record its ID\n",
    "for _, row in stations_df.iterrows():\n",
    "    mid = marker_counter\n",
    "    marker_counter += 1\n",
    "    preloaded_ids.append(mid)\n",
    "\n",
    "    lon, lat, nm = row.lon, row.lat, row.station_name\n",
    "    markers_dict[mid] = {'location': [lat, lon], 'name': nm}\n",
    "\n",
    "    mkr = Marker(location=[lat, lon], draggable=True)\n",
    "    mkr.marker_id = mid\n",
    "    mkr.marker_name = nm\n",
    "\n",
    "    def on_move(change, m_id=mid):\n",
    "        new_loc = change['new']\n",
    "        markers_dict[m_id]['location'] = new_loc\n",
    "        print(f\"Station '{markers_dict[m_id]['name']}' moved to {new_loc}\")\n",
    "\n",
    "    mkr.observe(on_move, names='location')\n",
    "    m.add_layer(mkr)\n",
    "\n",
    "# -------------------------------\n",
    "# Drawing new markers\n",
    "# -------------------------------\n",
    "def ask_marker_name(marker, mid):\n",
    "    name_input = widgets.Text(\n",
    "        placeholder='Enter station name',\n",
    "        description='Station name:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    confirm = widgets.Button(description='Confirm', button_style='success')\n",
    "    box = widgets.VBox([name_input, confirm])\n",
    "\n",
    "    def on_confirm(b):\n",
    "        nm = name_input.value.strip() or f\"Station {mid}\"\n",
    "        marker.marker_name = nm\n",
    "        markers_dict[mid]['name'] = nm\n",
    "        print(f\"New station {mid} named '{nm}'\")\n",
    "        box.close()\n",
    "\n",
    "    confirm.on_click(on_confirm)\n",
    "    with output:\n",
    "        display(box)\n",
    "\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global marker_counter\n",
    "    if action == \"created\":\n",
    "        lon, lat = geo_json[\"geometry\"][\"coordinates\"]\n",
    "        mid = marker_counter\n",
    "        marker_counter += 1\n",
    "        markers_dict[mid] = {'location': [lat, lon], 'name': None}\n",
    "\n",
    "        mkr = Marker(location=[lat, lon], draggable=True)\n",
    "        mkr.marker_id = mid\n",
    "        ask_marker_name(mkr, mid)\n",
    "\n",
    "        def on_move(change, m_id=mid):\n",
    "            markers_dict[m_id]['location'] = change[\"new\"]\n",
    "            print(f\"Station {m_id} moved to {change['new']}\")\n",
    "\n",
    "        mkr.observe(on_move, names=\"location\")\n",
    "        m.add_layer(mkr)\n",
    "\n",
    "        # remove Leaflet’s default point layer\n",
    "        for lyr in list(m.layers):\n",
    "            if isinstance(lyr, GeoJSON) and lyr.data.get(\"geometry\", {}).get(\"type\") == \"Point\":\n",
    "                m.remove_layer(lyr)\n",
    "\n",
    "    elif action == \"deleted\":\n",
    "        feats = geo_json.get(\"features\", []) or [geo_json]\n",
    "        for f in feats:\n",
    "            mid = f.get(\"properties\", {}).get(\"marker_id\")\n",
    "            if mid in markers_dict:\n",
    "                print(f\"Deleted station {mid} named '{markers_dict[mid]['name']}'\")\n",
    "                markers_dict.pop(mid)\n",
    "        print(\"Remaining stations:\", markers_dict)\n",
    "\n",
    "draw_control = DrawControl(\n",
    "    polygon={}, polyline={}, rectangle={}, circle={}, circlemarker={},\n",
    "    marker={\"repeatMode\": False}\n",
    ")\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# -------------------------------\n",
    "# Save stations, preserving original names/order\n",
    "# -------------------------------\n",
    "def save_stations(path):\n",
    "    \"\"\"\n",
    "    Write stations in two phases:\n",
    "      1) Preloaded stations in their original order with unchanged names\n",
    "      2) Newly drawn stations appended afterwards\n",
    "    \"\"\"\n",
    "    new_ids = [mid for mid in markers_dict if mid not in preloaded_ids]\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        # 1) Write preloaded stations first\n",
    "        for mid in preloaded_ids:\n",
    "            data = markers_dict.get(mid)\n",
    "            if not data:\n",
    "                continue\n",
    "            lat, lon = data['location']\n",
    "            name = data['name']\n",
    "            f.write(f\"{lon},{lat},{name}\\n\")\n",
    "\n",
    "        # 2) Append any new stations\n",
    "        for mid in new_ids:\n",
    "            data = markers_dict[mid]\n",
    "            lat, lon = data['location']\n",
    "            name = data['name'] or f\"Station_{mid}\"\n",
    "            f.write(f\"{lon},{lat},{name}\\n\")\n",
    "\n",
    "    print(f\"Saved {len(markers_dict)} stations to '{path}'\")\n",
    "\n",
    "save_btn = widgets.Button(description='Save stations', button_style='info')\n",
    "save_btn.on_click(lambda b: save_stations(stations_file))\n",
    "\n",
    "with output:\n",
    "    display(save_btn)\n",
    "\n",
    "# -------------------------------\n",
    "# Show the map\n",
    "# -------------------------------\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(markers_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.4 Create Input_table.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = pd.read_csv(\n",
    "    stations_file,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['lon', 'lat', 'station_name'],\n",
    "    engine='python',\n",
    "    dtype={'lon': float, 'lat': float, 'station_name': str}\n",
    ")\n",
    "print(f\"Loaded {len(stations_df)} stations from {stations_file}\")\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"valida4D\")\n",
    "script_name = os.path.join(script_folder, \"Valida4D.exe\")\n",
    "input_table = os.path.join(script_folder, \"Input_table.dat\")\n",
    "\n",
    "float_fmt='{:.6f}'\n",
    "delimiter = ' ' \n",
    "encoding = 'utf-8'  # change if needed\n",
    "\n",
    "sd = datetime.datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "lines = []\n",
    "lines.append('SERIE_INITIAL_DATA :' + sd.strftime('%Y %m %d %H %M %S'))\n",
    "lines.append('<BeginTable>')\n",
    "\n",
    "for _, row in stations_df.iterrows():\n",
    "    # Safely format lon/lat and clean station name\n",
    "    lon = float_fmt.format(row['lon'])\n",
    "    lat = float_fmt.format(row['lat']) \n",
    "    name = str(row['station_name']).strip()\n",
    "    # Escape delimiter in station name by replacing it with a space\n",
    "    if delimiter and delimiter in name:\n",
    "        name = name.replace(delimiter, ' ')\n",
    "    lines.append(delimiter.join([lon, lat, str(Z_DEPTHS), ',', name]))\n",
    "\n",
    "lines.append('<EndTable>')\n",
    "\n",
    "# Atomic write: write to temp file then replace\n",
    "dest_dir = os.path.dirname(input_table) or '.'\n",
    "\n",
    "fd, tmp_path = tempfile.mkstemp(dir=dest_dir, prefix='._tmp_dat_', text=True)\n",
    "os.close(fd)\n",
    "try:\n",
    "    with open(tmp_path, 'w', encoding=encoding, newline='\\n') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "    os.replace(tmp_path, input_table)\n",
    "    print(f\"Wrote table to {input_table}\")\n",
    "finally:\n",
    "    if os.path.exists(tmp_path):\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except OSError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.5 Create InputValida4D.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get hdf5 variables and units\n",
    "\n",
    "def decode_attr(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        return x.decode(\"utf-8\")\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return tuple(decode_attr(i) for i in x)\n",
    "    return x\n",
    "\n",
    "variable = []\n",
    "with h5py.File(hdf5_files[0], \"r\") as h5f:\n",
    "    results = h5f[\"Results\"]\n",
    "    for name, group in results.items():\n",
    "        # expected child inside the group\n",
    "        child_name = f\"{name}_00001\"\n",
    "        if child_name in group:\n",
    "            item = group[child_name]\n",
    "            units = item.attrs.get(\"Units\", None)\n",
    "            units = decode_attr(units) if units is not None else None\n",
    "        else:\n",
    "            units = None\n",
    "        variable.append((name, units))\n",
    "        print(f\"{name}  units = {units}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DEPTHS = 0. #depth relative to the surface \n",
    "\n",
    "output_dir = os.path.join(os.getcwd(),'out')\n",
    "output_table = os.path.join(output_dir, \"OutTable.dat\")\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"valida4D\")\n",
    "script_name = os.path.join(script_folder, \"Valida4D.exe\")\n",
    "input_file  = os.path.join(script_folder, \"InputValida4D.dat\")\n",
    "input_table = os.path.join(script_folder, \"Input_table.dat\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "sd = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "ed = datetime.strptime(end_date_str,   \"%Y-%m-%d\").date()\n",
    "\n",
    "lines = []\n",
    "lines.append('FIELD4D         : 1')\n",
    "lines.append('EXTRAPOLATE     : 0')\n",
    "lines.append('INPUT_TABLE     : ' + input_table)\n",
    "lines.append('Z_DEPTHS        : ' + str(Z_DEPTHS))\n",
    "lines.append('X_COLUMN        : 1')\n",
    "lines.append('Y_COLUMN        : 2')\n",
    "lines.append('Z_COLUMN        : 3')\n",
    "lines.append('OUTPUT_TABLE    : ' + output_table)\n",
    "lines.append('START           :' + sd.strftime('%Y %m %d %H %M %S'))\n",
    "lines.append('END             :' + ed.strftime('%Y %m %d %H %M %S'))\n",
    "lines.append('DT              : 3600')\n",
    "\n",
    "lines.append('<BeginHDF5>')\n",
    "\n",
    "for filename in hdf5_files:\n",
    "    # Convert to str and strip whitespace\n",
    "    fname = str(filename).strip()\n",
    "    # Optionally ensure consistent path style (uncomment if needed)\n",
    "    # fname = os.path.normpath(fname)\n",
    "    lines.append(fname)\n",
    "lines.append('<EndHDF5>')\n",
    "\n",
    "col = 4\n",
    "# variable is a list of tuples, e.g. [(\"water level\", \"m\"), (\"velocity modulus\", \"m/s\")]\n",
    "for item in variable:\n",
    "    col += 1\n",
    "    name, units = item[0], item[1]\n",
    "    description = item[2] if len(item) > 2 else name\n",
    "    lines.append('<beginproperty>')\n",
    "    lines.append(f'NAME        :  {name}')\n",
    "    lines.append(f'UNITS       :  {units}')\n",
    "    lines.append(f'DESCRIPTION :  {description}')\n",
    "    lines.append(f'COLUMN      :  {col}')\n",
    "    lines.append('<endproperty>')\n",
    "\n",
    "# Atomic write: write to temp file then replace\n",
    "dest_dir = os.path.dirname(input_file) or '.'\n",
    "\n",
    "fd, tmp_path = tempfile.mkstemp(dir=dest_dir, prefix='._tmp_dat_', text=True)\n",
    "os.close(fd)\n",
    "try:\n",
    "    with open(tmp_path, 'w', newline='\\n') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "    os.replace(tmp_path, input_file)\n",
    "    print(f\"Wrote table to {input_file}\")\n",
    "finally:\n",
    "    if os.path.exists(tmp_path):\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except OSError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.6 Run Valida4D tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = subprocess.run(\n",
    "        [script_name],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    \n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(f\"Wrote results to {output_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.7 Convert OutTable.dat to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Finds the first numeric line with at least 3 numbers and interprets them as YEAR MONTH DAY.\n",
    "- Finds the table header line (contains the token 'Seconds' and 'Station' or 'StationName' case-insensitive).\n",
    "- Uses header tokens as CSV column names (keeps order).\n",
    "- Parses rows after the header, supports variable whitespace and station names containing spaces.\n",
    "- Adds a Datetime column computed as start_of_day + Seconds (Seconds must be numeric).\n",
    "- Groups rows by station and writes <Station>.csv in outdir; also writes outdir/groups.txt with detected pre-table tokens.\n",
    "\"\"\"\n",
    "\n",
    "def read_lines(path):\n",
    "    return Path(path).read_text(encoding='utf-8').splitlines()\n",
    "\n",
    "def find_start_date(lines):\n",
    "    num_re = re.compile(r'([+\\-]?\\d+(\\.\\d+)?([Ee][+\\-]?\\d+)?)')\n",
    "    for line in lines:\n",
    "        tokens = re.findall(num_re, line)\n",
    "        # tokens is list of tuples; we want the first elements\n",
    "        if len(tokens) >= 3:\n",
    "            try:\n",
    "                year = int(float(tokens[0][0]))\n",
    "                month = int(float(tokens[1][0]))\n",
    "                day = int(float(tokens[2][0]))\n",
    "                return datetime(year, month, day)\n",
    "            except Exception:\n",
    "                continue\n",
    "    raise ValueError(\"Start date not found: expected a line with at least three numeric tokens (year month day).\")\n",
    "\n",
    "def extract_pre_table(lines):\n",
    "    pre = []\n",
    "    for line in lines:\n",
    "        if line.strip().upper().startswith('<BEGINTABLE>') or line.strip().upper() == '<BEGINTABLE>':\n",
    "            break\n",
    "        pre.append(line)\n",
    "    return pre\n",
    "\n",
    "def detect_header_and_table_start(lines):\n",
    "    # header must contain 'Seconds' and a token with 'Station' substring (e.g., StationName)\n",
    "    hdr_re = re.compile(r'\\bSeconds\\b', re.IGNORECASE)\n",
    "    station_re = re.compile(r'\\bStation', re.IGNORECASE)\n",
    "    for idx, line in enumerate(lines):\n",
    "        if hdr_re.search(line) and station_re.search(line):\n",
    "            # split by whitespace to get header fields (preserve order)\n",
    "            header_tokens = re.findall(r'\\S+', line.strip())\n",
    "            return header_tokens, idx + 1  # table starts next line\n",
    "    raise ValueError(\"Table header not found: expected a line containing 'Seconds' and 'Station'.\")\n",
    "\n",
    "def parse_row_by_header(line, header):\n",
    "    parts = re.findall(r'\\S+', line.strip())\n",
    "    if not parts:\n",
    "        return None\n",
    "    # Strategy: Attempt to map from left to right, but station (last header token) may contain spaces.\n",
    "    n_hdr = len(header)\n",
    "    if len(parts) >= n_hdr:\n",
    "        # assume station (last column) may include remaining tokens beyond n_hdr-1\n",
    "        mapped = {}\n",
    "        for i in range(n_hdr - 1):\n",
    "            mapped[header[i]] = parts[i]\n",
    "        # station and any trailing tokens\n",
    "        station_val = ' '.join(parts[n_hdr - 1:])\n",
    "        mapped[header[-1]] = station_val\n",
    "        return mapped\n",
    "    # If fewer tokens than headers, skip row\n",
    "    return None\n",
    "\n",
    "def is_numeric(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def parse_table(lines, header, start_idx):\n",
    "    entries = []\n",
    "    for line in lines[start_idx:]:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        mapped = parse_row_by_header(line, header)\n",
    "        if not mapped:\n",
    "            continue\n",
    "        # Seconds must exist and be numeric\n",
    "        seconds_key = None\n",
    "        for c in header:\n",
    "            if c.lower() == 'seconds':\n",
    "                seconds_key = c\n",
    "                break\n",
    "        if not seconds_key:\n",
    "            raise ValueError(\"Header missing 'Seconds' column after detection.\")\n",
    "        sec_val = mapped.get(seconds_key, '')\n",
    "        if not is_numeric(sec_val):\n",
    "            # try to clean scientific notation like 0.000000000000000E+000\n",
    "            sec_clean = sec_val.replace('D','E').replace('d','E')\n",
    "            if not is_numeric(sec_clean):\n",
    "                continue\n",
    "            sec_val = sec_clean\n",
    "            mapped[seconds_key] = sec_val\n",
    "        # store entry\n",
    "        entries.append(mapped)\n",
    "    return entries\n",
    "\n",
    "def write_station_files(entries, header, start_dt, outdir):\n",
    "    seconds_key = next(c for c in header if c.lower() == 'seconds')\n",
    "    station_key = None\n",
    "    # find header token that contains 'station' ignoring case\n",
    "    for c in header:\n",
    "        if 'station' in c.lower():\n",
    "            station_key = c\n",
    "            break\n",
    "    if not station_key:\n",
    "        raise ValueError(\"Header missing Station column.\")\n",
    "    grouped = defaultdict(list)\n",
    "    for row in entries:\n",
    "        sec = float(row[seconds_key])\n",
    "        dt = start_dt + timedelta(seconds=sec)\n",
    "        row_with_dt = dict(row)  # copy\n",
    "        row_with_dt['Datetime'] = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        grouped[row[station_key]].append(row_with_dt)\n",
    "\n",
    "    outdir = Path(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Exclude the Seconds column from output columns\n",
    "    out_columns = ['Datetime'] + [c for c in header if c != seconds_key]\n",
    "    \n",
    "    written = []\n",
    "    for station, rows in grouped.items():\n",
    "        safe_name = re.sub(r'[^\\w\\-_. ]', '_', station).strip().replace(' ', '_')\n",
    "        fname = outdir / f\"{safe_name}.csv\"\n",
    "        with fname.open('w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=out_columns)\n",
    "            writer.writeheader()\n",
    "            for r in rows:\n",
    "                # ensure all fields present\n",
    "                outrow = {col: r.get(col, '') for col in out_columns}\n",
    "                writer.writerow(outrow)\n",
    "        written.append(station)\n",
    "    return written, out_columns\n",
    "\n",
    "def extract_pre_table_groups(pre_lines):\n",
    "    # tokens containing letters, deduplicated in order\n",
    "    tokens = []\n",
    "    for line in pre_lines:\n",
    "        clean = re.sub(r'[,:;()\\/\\[\\]]', ' ', line)\n",
    "        for t in re.findall(r'\\S+', clean):\n",
    "            t_norm = re.sub(r'[^A-Za-z0-9_\\-]', '', t)\n",
    "            if not t_norm:\n",
    "                continue\n",
    "            # skip pure numeric tokens\n",
    "            if re.fullmatch(r'[+\\-]?\\d+(\\.\\d+)?([Ee][+\\-]?\\d+)?', t_norm):\n",
    "                continue\n",
    "            tokens.append(t_norm)\n",
    "    seen = set()\n",
    "    groups = []\n",
    "    for t in tokens:\n",
    "        up = t.upper()\n",
    "        if up in seen:\n",
    "            continue\n",
    "        seen.add(up)\n",
    "        groups.append(t)\n",
    "    return groups\n",
    "\n",
    "def main():\n",
    "\n",
    "    lines = read_lines(output_table)\n",
    "    pre_table = extract_pre_table(lines)\n",
    "    groups = extract_pre_table_groups(pre_table)\n",
    "\n",
    "    try:\n",
    "        start_dt = find_start_date(lines)\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        sys.exit(3)\n",
    "\n",
    "    try:\n",
    "        header, table_start_idx = detect_header_and_table_start(lines)\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        sys.exit(4)\n",
    "\n",
    "    entries = parse_table(lines, header, table_start_idx)\n",
    "    if not entries:\n",
    "        print(\"No data rows parsed. Check table formatting.\")\n",
    "        sys.exit(5)\n",
    "\n",
    "    written_stations, out_columns = write_station_files(entries, header, start_dt, output_dir)\n",
    "\n",
    "    print(f\"Wrote {len(written_stations)} station files to {output_dir}.\")\n",
    "    print(f\"Detected header columns: {', '.join(header)}.\")\n",
    "    print(f\"Output columns (with Datetime): {', '.join(out_columns)}.\")\n",
    "    print(f\"Detected pre-table groups: {', '.join(groups) if groups else '(none)'}.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.3 Load csv files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_1 = os.path.join(os.getcwd(),'res','res.csv')\n",
    "df_1 = pd.read_csv(csv_file_1)\n",
    "\n",
    "csv_file_2 = None\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional - only read a second dataframe if you want a plot with time series from different csv files \n",
    "csv_file_2 = os.path.join(os.getcwd(),'res','obs.csv')\n",
    "df_2 = pd.read_csv(csv_file_2)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.4 Plot time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(os.getcwd(),'out')\n",
    "output_file = \"timeseries.png\"\n",
    "\n",
    "# Choose your column names (or set variable_name_2 = '' if you only want one trace)\n",
    "var1 = 'water_level'\n",
    "var2 = ''  # set to '' if you don't want a second trace\n",
    "\n",
    "days_between_ticks = 1  # number of days between ticks\n",
    "mode_var1 ='lines' #lines+markers, lines, markers\n",
    "mode_var2 ='lines' #lines+markers, lines, markers\n",
    "color_var1 = 'blue'\n",
    "color_var2 = 'red'\n",
    "legend_1 = 'res'\n",
    "legend_2 = 'obs'\n",
    "\n",
    "axis_labels = {\n",
    "  'water_level': 'Water Level (m)',\n",
    "  'velocity_modulus' : 'Velocity modulus (m/s)',\n",
    "  'temperature': 'Temperature (°C)'}\n",
    "\n",
    "date_format     = \"%d-%m-%Y\"\n",
    "dpi = 150 \n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"Plot_TimeSeries\")\n",
    "script_name = os.path.join(script_folder, \"plot_timeseries.py\")\n",
    "input_file  = os.path.join(script_folder, \"input_plot_timeseries.py\")\n",
    "\n",
    "config = {\n",
    "    'csv_file_1':      csv_file_1,\n",
    "    'csv_file_2':      csv_file_2,\n",
    "    'output_dir':      output_dir,\n",
    "    'output_file':     output_file,\n",
    "    'var1':            var1,\n",
    "    'var2':            var2,\n",
    "    'mode_var1':       mode_var1,\n",
    "    'mode_var2':       mode_var2,\n",
    "    'color_var1':      color_var1,\n",
    "    'color_var2':      color_var2,\n",
    "    'legend_1':        legend_1,\n",
    "    'legend_2':        legend_2,\n",
    "    'days_between':    days_between_ticks,\n",
    "    'axis_label_1':    axis_labels.get(var1),\n",
    "    'axis_label_2':    axis_labels.get(var2),\n",
    "    'date_format':     date_format,\n",
    "    'dpi':             dpi\n",
    "}\n",
    "\n",
    "# Write config to input_file\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# Auto-generated config\\n\\n\")\n",
    "    for key, val in config.items():\n",
    "        f.write(f\"{key} = {val!r}\\n\")\n",
    "\n",
    "print(f\"Wrote configuration to {input_file}\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", os.path.basename(script_name)],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.5 Compare with measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare observed and modeled time series by nearest-time matching (±30 min),\n",
    "compute statistics, save CSV and PNG plots.\n",
    "Assumes the first column in input DataFrames or CSVs is the time column.\n",
    "\"\"\"\n",
    "variable = \"water_level\"\n",
    "\n",
    "axis_labels = {\n",
    "  'water_level': 'Water Level (m)',\n",
    "  'velocity_modulus' : 'Velocity modulus (m/s)',\n",
    "  'temperature': 'Temperature (°C)'}\n",
    "\n",
    "# Load modelled and observed data\n",
    "df_mod = df_1\n",
    "df_obs = df_2\n",
    "\n",
    "# Settings\n",
    "TOLERANCE = pd.Timedelta(\"30min\")\n",
    "out_dir = \"out\"\n",
    "dpi = 150\n",
    "\n",
    "csv_path = os.path.join(out_dir, f\"obs_vs_mod_{variable}.csv\")\n",
    "fig_path = os.path.join(out_dir, f\"obs_vs_mod_{variable}.png\")\n",
    "residuals_path = os.path.join(out_dir, f\"residuals_{variable}.png\")\n",
    "\n",
    "def compute_metrics(obs_series, mod_series):\n",
    "    \"\"\"\n",
    "    Compute bias, RMSE, Pearson correlation, and R-squared (coefficient of determination)\n",
    "    \"\"\"\n",
    "    diff = mod_series - obs_series\n",
    "\n",
    "    ss_res = ((diff ** 2).sum())\n",
    "    ss_tot = (((obs_series - obs_series.mean()) ** 2).sum())\n",
    "    if ss_tot == 0.0:\n",
    "        r2 = float(\"nan\")\n",
    "    else:\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "\n",
    "    return {\n",
    "        \"bias\": float(diff.mean()),\n",
    "        \"rmse\": float(np.sqrt((diff**2).mean())),\n",
    "        \"corr\": float(obs_series.corr(mod_series)),\n",
    "        \"r2\": float(r2)\n",
    "    }\n",
    "def save_plots(df_cmp: pd.DataFrame, out_dir: str = OUT_DIR):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if not {\"obs\", \"mod\"}.issubset(df_cmp.columns):\n",
    "        raise ValueError(\"df_cmp must contain 'obs' and 'mod' columns\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    df_cmp[[\"obs\", \"mod\"]].plot(ax=ax)\n",
    "    ax.set_title(f\"Observed vs. Modeled\")\n",
    "    ax.set_ylabel(axis_labels.get(variable))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fig_path, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    residual = df_cmp[\"mod\"] - df_cmp[\"obs\"]\n",
    "    fig, ax = plt.subplots(figsize=(12, 2))\n",
    "    residual.plot(ax=ax, color=\"black\", legend=False)\n",
    "    ax.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    ax.set_title(f\"Residual (Model – Obs)\")\n",
    "    ax.set_ylabel(\"m\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(residuals_path, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "\n",
    "def read_firstcol_time(path_or_df, value_name):\n",
    "    \"\"\"\n",
    "    Accepts a path to CSV or a DataFrame.\n",
    "    Assumes the first column is the time column and the second column (or remaining one)\n",
    "    is the value column to return as a Series with DatetimeIndex named value_name.\n",
    "    \"\"\"\n",
    "    if isinstance(path_or_df, str):\n",
    "        df = pd.read_csv(path_or_df, header=0)\n",
    "    elif isinstance(path_or_df, pd.DataFrame):\n",
    "        df = path_or_df.copy()\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a file path or a pandas DataFrame\")\n",
    "\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"Input must have at least two columns (time + value)\")\n",
    "\n",
    "    time_col = df.columns[0]\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    series = pd.Series(df[variable].values, index=df[time_col], name=value_name)\n",
    "    series = series.sort_index()\n",
    "    return series\n",
    "\n",
    "def pair_by_nearest(obs_series: pd.Series, mod_series: pd.Series, tolerance: pd.Timedelta = TOLERANCE):\n",
    "    obs_df = obs_series.rename(\"obs\").reset_index().rename(columns={obs_series.index.name or \"index\": \"time\"})\n",
    "    mod_df = mod_series.rename(\"mod\").reset_index().rename(columns={mod_series.index.name or \"index\": \"time\"})\n",
    "    obs_df[\"time\"] = pd.to_datetime(obs_df[\"time\"])\n",
    "    mod_df[\"time\"] = pd.to_datetime(mod_df[\"time\"])\n",
    "    obs_df = obs_df.sort_values(\"time\")\n",
    "    mod_df = mod_df.sort_values(\"time\")\n",
    "    merged = pd.merge_asof(\n",
    "        mod_df,\n",
    "        obs_df,\n",
    "        on=\"time\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=tolerance,\n",
    "        suffixes=(\"_mod\", \"_obs\")\n",
    "    )\n",
    "    merged = merged.dropna(subset=[\"mod\", \"obs\"])\n",
    "    if merged.empty:\n",
    "        return pd.DataFrame(columns=[\"obs\", \"mod\"]).astype(float)\n",
    "    paired = merged.set_index(\"time\")[[\"obs\", \"mod\"]]\n",
    "    return paired\n",
    "\n",
    "def main(obs_input, mod_input, out_dir=OUT_DIR, tolerance=TOLERANCE):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    obs_series = read_firstcol_time(obs_input, value_name=\"obs\")\n",
    "    mod_series = read_firstcol_time(mod_input, value_name=\"mod\")\n",
    "    if obs_series.index.tz is not None and mod_series.index.tz is None:\n",
    "        mod_series = mod_series.tz_localize(obs_series.index.tz)\n",
    "    elif mod_series.index.tz is not None and obs_series.index.tz is None:\n",
    "        obs_series = obs_series.tz_localize(mod_series.index.tz)\n",
    "    paired = pair_by_nearest(obs_series, mod_series, tolerance=tolerance)\n",
    "    if paired.empty:\n",
    "        raise RuntimeError(\"No pairs found within tolerance. Check your indices and tolerance value.\")\n",
    "    metrics = compute_metrics(paired[\"obs\"], paired[\"mod\"])\n",
    "    csv_path = os.path.join(out_dir, f\"obs_vs_mod_{variable}.csv\")\n",
    "    paired.to_csv(csv_path)\n",
    "    save_plots(paired, out_dir=out_dir)\n",
    "    print(\"OBS range:\", obs_series.index.min(), \"→\", obs_series.index.max())\n",
    "    print(\"MOD range:\", mod_series.index.min(), \"→\", mod_series.index.max())\n",
    "    print(\"Paired points:\", len(paired))\n",
    "    print(\"Metrics (nearest snap):\", metrics)\n",
    "    print(f\"Saved:\\n{csv_path}\\n{fig_path}\\n{residuals_path}\")\n",
    "    return metrics, paired\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    metrics, paired = main(df_obs, df_mod, out_dir=OUT_DIR, tolerance=TOLERANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
