{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOHID Postprocessing\n",
    "\n",
    "This Jupyter Notebook aims to help analyse the results of the MOHID Water model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note 1**: Execute each cell through the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button from the top MENU (or keyboard shortcut `Shift` + `Enter`).<br>\n",
    "<br>\n",
    "**Note 2**: Use the Kernel and Cell menus to restart the kernel and clear outputs.<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "- [1. Import required libraries](#1.-Import-required-libraries)\n",
    "- [2. Time series](#2.-Time-series)\n",
    "    - [2.1 Convert and merge multiple MOHID time series files to csv](#2.1-Convert-and-merge-multiple-MOHID-time-series-files-to-csv)\n",
    "    - [2.2 Extract time series from HDF5 files](#2.2-Extract-time-series-from-HDF5-files)\n",
    "        - [2.2.1 Read one or multiple MOHID HDF5 files](#2.2.1-Read-one-or-multiple-MOHID-HDF5-files)\n",
    "        - [2.2.2 Load or create a new file with monitoring stations](#2.2.2-Load-or-create-a-new-file-with-monitoring-stations)\n",
    "        - [2.2.3 Adjust or define new monitoring stations on the map](#2.2.3-Adjust-or-define-new-monitoring-stations-on-the-map)\n",
    "        - [2.2.4 Create Input_table.dat](#3.1.4-Create-Input_table.dat)\n",
    "        - [2.2.5 Create InputValida4D.dat](#2.2.5-Create-InputValida4D.dat)\n",
    "        - [2.2.6 Run Valida4D tool](#2.2.6-Run-Valida4D-tool)\n",
    "        - [2.2.7 Convert OutTable.dat to csv files](#2.2.7-Convert-OutTable.dat-to-csv-files)\n",
    "    - [2.3 Load csv files](#2.3-Load-csv-files)\n",
    "    - [2.4 Statistics](#2.4-Statistics)\n",
    "    - [2.5 Plot time series](#2.5-Plot-time-series)\n",
    "    - [2.6 Compare with measurements](#2.6-Compare-with-measurements)\n",
    "    - [2.7 Harmonic analysis](#2.7-Harmonic-analysis)\n",
    "        - [2.7.1 Calculate the anomaly](#2.7.1-Calculate-the-anomaly)\n",
    "        - [2.7.2 Solve to obtain the coefficients](#2.7.2-Solve-to-obtain-the-coefficients)\n",
    "        - [2.7.3 Save the amplitudes and phases](#2.7.3-Save-the-amplitudes-and-phases)\n",
    "        - [2.7.4 Generate the astronomical tide](#2.7.4-Generate-the-astronomical-tide)\n",
    "        - [2.7.5 Plot and save](#2.7.5-Plot-and-save)\n",
    "- [3. Maps](#3.-Maps)\n",
    "    - [3.1 Plot maps](#3.1-Plot-maps)\n",
    "    - [3.2 Plot statistics](#3.2-Plot-statistics)\n",
    "    - [3.3 Plot vertical cut](#3.3-Plot-vertical-cut)\n",
    "        - [3.3.1 Define paths and file names](#3.3.1-Define-paths-and-file-names)\n",
    "        - [3.3.2 Load or create a new file with a polyline](#3.3.2-Load-or-create-a-new-file-with-a-polyline)\n",
    "        - [3.3.3 Visualise or define a new polyline on the map](#3.3.3-Visualise-or-define-a-new-polyline-on-the-map)\n",
    "        - [3.3.4 Run script](#3.3.4-Run-script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from ipyleaflet import Map, TileLayer, DrawControl, GeoJSON, Marker, basemaps, Popup, Polyline, Circle, GeoData, Polygon, LayerGroup\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "from folium.plugins import MeasureControl\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import requests\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Video\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from urllib.request import Request, urlopen\n",
    "from PIL import Image\n",
    "import io\n",
    "from math import radians, cos, sin\n",
    "import matplotlib.colors as mcolors\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from typing import Iterable, Optional\n",
    "import utide\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Convert and merge multiple MOHID time series files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dir = os.path.join(os.getcwd(),'res') #files can be inside subfolders of master_dir\n",
    "filename='FPOLIS.srh'\n",
    "\n",
    "out_dir   = os.path.join(os.getcwd(),'out')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "output_csv = os.path.join(os.getcwd(),out_dir,'FPOLIS.csv') \n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"Merge_TimeSeries\")\n",
    "script_name = os.path.join(script_folder, \"merge_timeseries.py\")\n",
    "input_file  = os.path.join(script_folder, \"input_merge_timeseries.py\")\n",
    "\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"master_dir       = r'{master_dir}'\\n\")\n",
    "    f.write(f\"filename         =r'{filename}'\\n\")\n",
    "    f.write(f\"output_csv=r'{output_csv}'\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", os.path.basename(script_name)],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Extract time series from HDF5 files\n",
    "#Based on Valida4D from MOHID tools (https://github.com/Mohid-Water-Modelling-System/Mohid/tree/master/Software/SmallTools/Valida4D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.1 Read one or multiple MOHID HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_files    = True\n",
    "backup_root       = os.path.join(os.getcwd(),'res')\n",
    "hdf5_file         =r'Hydrodynamic_2_Surface.hdf5'\n",
    "start_date_str    = '2025-9-25'\n",
    "end_date_str      = '2025-9-27'\n",
    "\n",
    "# ----------------------------------------\n",
    "def collect_hdf5_paths(root, h5file, sd, ed):\n",
    "    paths = []\n",
    "    for entry in os.scandir(root):\n",
    "        if not entry.is_dir():\n",
    "            continue\n",
    "        try:\n",
    "            day = datetime.strptime(entry.name.split('_')[0], \"%Y%m%d\").date()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if sd <= day <= ed:\n",
    "            # Look directly inside the date-folder\n",
    "            pattern = os.path.join(entry.path, h5file)\n",
    "            for f in glob.glob(pattern):\n",
    "                if os.path.isfile(f):\n",
    "                    paths.append(f)\n",
    "    return sorted(paths)\n",
    "# ----------------------------------------\n",
    "\n",
    "if multiple_files:\n",
    "    sd = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "    ed = datetime.strptime(end_date_str,   \"%Y-%m-%d\").date()\n",
    "    \n",
    "    hdf5_files = collect_hdf5_paths(backup_root, hdf5_file, sd, ed)\n",
    "else:\n",
    "    hdf5_files = [os.path.join(backup_root, hdf5_file)]\n",
    "\n",
    "if not hdf5_files:\n",
    "    raise RuntimeError(f\"No  HDF5s in {hdf5_file} between {start_date_str} and {end_date_str}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# GRID\n",
    "# ----------------------------------------\n",
    "with h5py.File(hdf5_files[0], \"r\") as h5f:\n",
    "    Xr = h5f[\"Grid\"][\"Longitude\"][:]\n",
    "    Yr = h5f[\"Grid\"][\"Latitude\"][:]\n",
    "    zi = h5f[\"Grid\"][\"Bathymetry\"][:]\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.2 Load or create a new file with monitoring stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_file = 'stations.csv'\n",
    "\n",
    "# Load existing stations (format: lon lat name)\n",
    "if os.path.exists(stations_file):\n",
    "    stations_df = pd.read_csv(\n",
    "        stations_file,\n",
    "        sep=',',\n",
    "        header=None,\n",
    "        names=['lon', 'lat', 'station_name'],\n",
    "        engine='python'\n",
    "    )\n",
    "    print(f\"Loaded {len(stations_df)} stations from {stations_file}\")\n",
    "else:\n",
    "    stations_df = pd.DataFrame(columns=['lon', 'lat', 'station_name'])\n",
    "    print(f\"No '{stations_file}' found. Starting with zero stations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.3 Adjust or define new monitoring stations on the map  \n",
    "#Move or draw markers on the map to define the monitoring stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Start timing\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------------\n",
    "# Assume Xr, Yr, zi defined elsewhere\n",
    "# -------------------------------\n",
    "LonGrid = np.array(Xr)\n",
    "LatGrid = np.array(Yr)\n",
    "\n",
    "# -------------------------------\n",
    "# Build discrete colormap\n",
    "# -------------------------------\n",
    "_nbins = 10\n",
    "_bins = None\n",
    "_discrete_colors = None\n",
    "\n",
    "def map_value_to_color(value):\n",
    "    if value == -99:\n",
    "        return \"#ffffff00\"\n",
    "    idx = np.digitize(value, _bins) - 1\n",
    "    idx = int(np.clip(idx, 0, _nbins - 1))\n",
    "    return _discrete_colors[idx]\n",
    "\n",
    "def precompute_color_grid(zi, nbins=10):\n",
    "    global _bins, _nbins, _discrete_colors\n",
    "    _nbins = nbins\n",
    "    valid = zi != -99\n",
    "    if np.any(valid):\n",
    "        vmin, vmax = zi[valid].min(), zi[valid].max()\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "    _bins = np.linspace(vmin, vmax, nbins + 1)\n",
    "    cmap = plt.colormaps.get_cmap('viridis')\n",
    "    _discrete_colors = [mcolors.to_hex(c) for c in cmap(np.linspace(0, 1, nbins))]\n",
    "    return np.vectorize(map_value_to_color)(zi)\n",
    "\n",
    "color_mapped_zi = precompute_color_grid(zi)\n",
    "\n",
    "# -------------------------------\n",
    "# Create base map\n",
    "# -------------------------------\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "m = Map(center=(LatGrid.mean(), LonGrid.mean()), zoom=8)\n",
    "\n",
    "# -------------------------------\n",
    "# Split raster into GeoJSON blocks\n",
    "# -------------------------------\n",
    "lon_sw = LonGrid[:-1, :-1]; lon_se = LonGrid[:-1, 1:]\n",
    "lon_ne = LonGrid[1:, 1:];   lon_nw = LonGrid[1:, :-1]\n",
    "lat_sw = LatGrid[:-1, :-1]; lat_se = LatGrid[:-1, 1:]\n",
    "lat_ne = LatGrid[1:, 1:];   lat_nw = LatGrid[1:, :-1]\n",
    "\n",
    "block_size = 10\n",
    "block_layers = {}\n",
    "\n",
    "def generate_block_geojson(br, bc):\n",
    "    features = []\n",
    "    rows, cols = zi.shape\n",
    "    i0, i1 = br * block_size, min((br + 1) * block_size, rows)\n",
    "    j0, j1 = bc * block_size, min((bc + 1) * block_size, cols)\n",
    "    for i in range(i0, i1):\n",
    "        for j in range(j0, j1):\n",
    "            if zi[i, j] == -99:\n",
    "                continue\n",
    "            coords = [[\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])],\n",
    "                [float(lon_se[i, j]), float(lat_se[i, j])],\n",
    "                [float(lon_ne[i, j]), float(lat_ne[i, j])],\n",
    "                [float(lon_nw[i, j]), float(lat_nw[i, j])],\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])]\n",
    "            ]]\n",
    "            feat = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\"type\": \"Polygon\", \"coordinates\": coords},\n",
    "                \"properties\": {\n",
    "                    \"fill\": map_value_to_color(zi[i, j]),\n",
    "                    \"stroke\": \"#000000\",\n",
    "                    \"fill-opacity\": 0.5,\n",
    "                    \"stroke-width\": 0.2,\n",
    "                    \"i\": i, \"j\": j\n",
    "                }\n",
    "            }\n",
    "            features.append(feat)\n",
    "    return {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "def update_all_blocks():\n",
    "    for lyr in block_layers.values():\n",
    "        m.remove_layer(lyr)\n",
    "    block_layers.clear()\n",
    "    rows, cols = zi.shape\n",
    "    n_br = (rows + block_size - 1) // block_size\n",
    "    n_bc = (cols + block_size - 1) // block_size\n",
    "    for br in range(n_br):\n",
    "        for bc in range(n_bc):\n",
    "            fc = generate_block_geojson(br, bc)\n",
    "            if not fc[\"features\"]:\n",
    "                continue\n",
    "            layer = GeoJSON(\n",
    "                data=fc,\n",
    "                style_callback=lambda f: {\n",
    "                    \"fillColor\": f[\"properties\"][\"fill\"],\n",
    "                    \"color\": f[\"properties\"][\"stroke\"],\n",
    "                    \"weight\": f[\"properties\"][\"stroke-width\"],\n",
    "                    \"fillOpacity\": f[\"properties\"][\"fill-opacity\"],\n",
    "                }\n",
    "            )\n",
    "            m.add_layer(layer)\n",
    "            block_layers[(br, bc)] = layer\n",
    "\n",
    "update_all_blocks()\n",
    "print(f\"Raster layering time: {time.time() - start_time:.2f} sec\")\n",
    "\n",
    "# -------------------------------\n",
    "# Station markers\n",
    "# -------------------------------\n",
    "markers_dict = {}\n",
    "marker_counter = 0\n",
    "preloaded_ids = []   # will capture IDs of originally loaded stations\n",
    "\n",
    "# Add each preloaded station and record its ID\n",
    "for _, row in stations_df.iterrows():\n",
    "    mid = marker_counter\n",
    "    marker_counter += 1\n",
    "    preloaded_ids.append(mid)\n",
    "\n",
    "    lon, lat, nm = row.lon, row.lat, row.station_name\n",
    "    markers_dict[mid] = {'location': [lat, lon], 'name': nm}\n",
    "\n",
    "    mkr = Marker(location=[lat, lon], draggable=True)\n",
    "    mkr.marker_id = mid\n",
    "    mkr.marker_name = nm\n",
    "\n",
    "    def on_move(change, m_id=mid):\n",
    "        new_loc = change['new']\n",
    "        markers_dict[m_id]['location'] = new_loc\n",
    "        print(f\"Station '{markers_dict[m_id]['name']}' moved to {new_loc}\")\n",
    "\n",
    "    mkr.observe(on_move, names='location')\n",
    "    m.add_layer(mkr)\n",
    "\n",
    "# -------------------------------\n",
    "# Drawing new markers\n",
    "# -------------------------------\n",
    "def ask_marker_name(marker, mid):\n",
    "    name_input = widgets.Text(\n",
    "        placeholder='Enter station name',\n",
    "        description='Station name:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    confirm = widgets.Button(description='Confirm', button_style='success')\n",
    "    box = widgets.VBox([name_input, confirm])\n",
    "\n",
    "    def on_confirm(b):\n",
    "        nm = name_input.value.strip() or f\"Station {mid}\"\n",
    "        marker.marker_name = nm\n",
    "        markers_dict[mid]['name'] = nm\n",
    "        print(f\"New station {mid} named '{nm}'\")\n",
    "        box.close()\n",
    "\n",
    "    confirm.on_click(on_confirm)\n",
    "    with output:\n",
    "        display(box)\n",
    "\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global marker_counter\n",
    "    if action == \"created\":\n",
    "        lon, lat = geo_json[\"geometry\"][\"coordinates\"]\n",
    "        mid = marker_counter\n",
    "        marker_counter += 1\n",
    "        markers_dict[mid] = {'location': [lat, lon], 'name': None}\n",
    "\n",
    "        mkr = Marker(location=[lat, lon], draggable=True)\n",
    "        mkr.marker_id = mid\n",
    "        ask_marker_name(mkr, mid)\n",
    "\n",
    "        def on_move(change, m_id=mid):\n",
    "            markers_dict[m_id]['location'] = change[\"new\"]\n",
    "            print(f\"Station {m_id} moved to {change['new']}\")\n",
    "\n",
    "        mkr.observe(on_move, names=\"location\")\n",
    "        m.add_layer(mkr)\n",
    "\n",
    "        # remove Leaflet’s default point layer\n",
    "        for lyr in list(m.layers):\n",
    "            if isinstance(lyr, GeoJSON) and lyr.data.get(\"geometry\", {}).get(\"type\") == \"Point\":\n",
    "                m.remove_layer(lyr)\n",
    "\n",
    "    elif action == \"deleted\":\n",
    "        feats = geo_json.get(\"features\", []) or [geo_json]\n",
    "        for f in feats:\n",
    "            mid = f.get(\"properties\", {}).get(\"marker_id\")\n",
    "            if mid in markers_dict:\n",
    "                print(f\"Deleted station {mid} named '{markers_dict[mid]['name']}'\")\n",
    "                markers_dict.pop(mid)\n",
    "        print(\"Remaining stations:\", markers_dict)\n",
    "\n",
    "draw_control = DrawControl(\n",
    "    polygon={}, polyline={}, rectangle={}, circle={}, circlemarker={},\n",
    "    marker={\"repeatMode\": False}\n",
    ")\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# -------------------------------\n",
    "# Save stations, preserving original names/order\n",
    "# -------------------------------\n",
    "def save_stations(path):\n",
    "    \"\"\"\n",
    "    Write stations in two phases:\n",
    "      1) Preloaded stations in their original order with unchanged names\n",
    "      2) Newly drawn stations appended afterwards\n",
    "    \"\"\"\n",
    "    new_ids = [mid for mid in markers_dict if mid not in preloaded_ids]\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        # 1) Write preloaded stations first\n",
    "        for mid in preloaded_ids:\n",
    "            data = markers_dict.get(mid)\n",
    "            if not data:\n",
    "                continue\n",
    "            lat, lon = data['location']\n",
    "            name = data['name']\n",
    "            f.write(f\"{lon},{lat},{name}\\n\")\n",
    "\n",
    "        # 2) Append any new stations\n",
    "        for mid in new_ids:\n",
    "            data = markers_dict[mid]\n",
    "            lat, lon = data['location']\n",
    "            name = data['name'] or f\"Station_{mid}\"\n",
    "            f.write(f\"{lon},{lat},{name}\\n\")\n",
    "\n",
    "    print(f\"Saved {len(markers_dict)} stations to '{path}'\")\n",
    "\n",
    "save_btn = widgets.Button(description='Save stations', button_style='info')\n",
    "save_btn.on_click(lambda b: save_stations(stations_file))\n",
    "\n",
    "with output:\n",
    "    display(save_btn)\n",
    "\n",
    "# -------------------------------\n",
    "# Show the map\n",
    "# -------------------------------\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(markers_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.4 Create Input_table.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DEPTHS = 0. #depth relative to the surface\n",
    "\n",
    "stations_df = pd.read_csv(\n",
    "    stations_file,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['lon', 'lat', 'station_name'],\n",
    "    engine='python',\n",
    "    dtype={'lon': float, 'lat': float, 'station_name': str}\n",
    ")\n",
    "print(f\"Loaded {len(stations_df)} stations from {stations_file}\")\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"valida4D\")\n",
    "script_name = os.path.join(script_folder, \"Valida4D.exe\")\n",
    "input_table = os.path.join(script_folder, \"Input_table.dat\")\n",
    "\n",
    "float_fmt='{:.6f}'\n",
    "delimiter = ' ' \n",
    "encoding = 'utf-8'  # change if needed\n",
    "\n",
    "sd = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "lines = []\n",
    "lines.append('SERIE_INITIAL_DATA :' + sd.strftime('%Y %m %d %H %M %S'))\n",
    "lines.append('<BeginTable>')\n",
    "\n",
    "for _, row in stations_df.iterrows():\n",
    "    # Safely format lon/lat and clean station name\n",
    "    lon = float_fmt.format(row['lon'])\n",
    "    lat = float_fmt.format(row['lat']) \n",
    "    name = str(row['station_name']).strip()\n",
    "    # Escape delimiter in station name by replacing it with a space\n",
    "    if delimiter and delimiter in name:\n",
    "        name = name.replace(delimiter, ' ')\n",
    "    lines.append(delimiter.join([lon, lat, str(Z_DEPTHS), ',', name]))\n",
    "\n",
    "lines.append('<EndTable>')\n",
    "\n",
    "# Atomic write: write to temp file then replace\n",
    "dest_dir = os.path.dirname(input_table) or '.'\n",
    "\n",
    "fd, tmp_path = tempfile.mkstemp(dir=dest_dir, prefix='._tmp_dat_', text=True)\n",
    "os.close(fd)\n",
    "try:\n",
    "    with open(tmp_path, 'w', encoding=encoding, newline='\\n') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "    os.replace(tmp_path, input_table)\n",
    "    print(f\"Wrote table to {input_table}\")\n",
    "finally:\n",
    "    if os.path.exists(tmp_path):\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except OSError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.5 Create InputValida4D.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(os.getcwd(),'out')\n",
    "output_table = os.path.join(output_dir, \"OutTable.dat\")\n",
    "\n",
    "#Get hdf5 variables and units\n",
    "\n",
    "def decode_attr(x):\n",
    "    if isinstance(x, (bytes, np.bytes_)):\n",
    "        return x.decode(\"utf-8\")\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        return tuple(decode_attr(i) for i in x)\n",
    "    return x\n",
    "\n",
    "variable = []\n",
    "with h5py.File(hdf5_files[0], \"r\") as h5f:\n",
    "    results = h5f[\"Results\"]\n",
    "    for name, group in results.items():\n",
    "        # expected child inside the group\n",
    "        child_name = f\"{name}_00001\"\n",
    "        if child_name in group:\n",
    "            item = group[child_name]\n",
    "            units = item.attrs.get(\"Units\", None)\n",
    "            units = decode_attr(units) if units is not None else None\n",
    "        else:\n",
    "            units = None\n",
    "        variable.append((name, units))\n",
    "        print(f\"{name}  units = {units}\")\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"valida4D\")\n",
    "script_name = os.path.join(script_folder, \"Valida4D.exe\")\n",
    "input_file  = os.path.join(script_folder, \"InputValida4D.dat\")\n",
    "input_table = os.path.join(script_folder, \"Input_table.dat\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "sd = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "ed = datetime.strptime(end_date_str,   \"%Y-%m-%d\").date()\n",
    "\n",
    "lines = []\n",
    "lines.append('FIELD4D         : 1')\n",
    "lines.append('EXTRAPOLATE     : 0')\n",
    "lines.append('INPUT_TABLE     : ' + input_table)\n",
    "lines.append('Z_DEPTHS        : ' + str(Z_DEPTHS))\n",
    "lines.append('X_COLUMN        : 1')\n",
    "lines.append('Y_COLUMN        : 2')\n",
    "lines.append('Z_COLUMN        : 3')\n",
    "lines.append('OUTPUT_TABLE    : ' + output_table)\n",
    "lines.append('START           :' + sd.strftime('%Y %m %d %H %M %S'))\n",
    "lines.append('END             :' + ed.strftime('%Y %m %d %H %M %S'))\n",
    "lines.append('DT              : 3600')\n",
    "\n",
    "lines.append('<BeginHDF5>')\n",
    "\n",
    "for filename in hdf5_files:\n",
    "    # Convert to str and strip whitespace\n",
    "    fname = str(filename).strip()\n",
    "    # Optionally ensure consistent path style (uncomment if needed)\n",
    "    # fname = os.path.normpath(fname)\n",
    "    lines.append(fname)\n",
    "lines.append('<EndHDF5>')\n",
    "\n",
    "col = 4\n",
    "# variable is a list of tuples, e.g. [(\"water level\", \"m\"), (\"velocity modulus\", \"m/s\")]\n",
    "for item in variable:\n",
    "    col += 1\n",
    "    name, units = item[0], item[1]\n",
    "    description = item[2] if len(item) > 2 else name\n",
    "    lines.append('<beginproperty>')\n",
    "    lines.append(f'NAME        :  {name}')\n",
    "    lines.append(f'UNITS       :  {units}')\n",
    "    lines.append(f'DESCRIPTION :  {description}')\n",
    "    lines.append(f'COLUMN      :  {col}')\n",
    "    lines.append('<endproperty>')\n",
    "\n",
    "# Atomic write: write to temp file then replace\n",
    "dest_dir = os.path.dirname(input_file) or '.'\n",
    "\n",
    "fd, tmp_path = tempfile.mkstemp(dir=dest_dir, prefix='._tmp_dat_', text=True)\n",
    "os.close(fd)\n",
    "try:\n",
    "    with open(tmp_path, 'w', newline='\\n') as f:\n",
    "        for line in lines:\n",
    "            f.write(line + '\\n')\n",
    "    os.replace(tmp_path, input_file)\n",
    "    print(f\"Wrote table to {input_file}\")\n",
    "finally:\n",
    "    if os.path.exists(tmp_path):\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except OSError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.6 Run Valida4D tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = subprocess.run(\n",
    "        [script_name],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    \n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(f\"Wrote results to {output_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2.7 Convert OutTable.dat to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Finds the first numeric line with at least 3 numbers and interprets them as YEAR MONTH DAY.\n",
    "- Finds the table header line (contains the token 'Seconds' and 'Station' or 'StationName' case-insensitive).\n",
    "- Uses header tokens as CSV column names (keeps order).\n",
    "- Parses rows after the header, supports variable whitespace and station names containing spaces.\n",
    "- Adds a Datetime column computed as start_of_day + Seconds (Seconds must be numeric).\n",
    "- Groups rows by station and writes <Station>.csv in outdir; also writes outdir/groups.txt with detected pre-table tokens.\n",
    "\"\"\"\n",
    "\n",
    "def read_lines(path):\n",
    "    return Path(path).read_text(encoding='utf-8').splitlines()\n",
    "\n",
    "def find_start_date(lines):\n",
    "    num_re = re.compile(r'([+\\-]?\\d+(\\.\\d+)?([Ee][+\\-]?\\d+)?)')\n",
    "    for line in lines:\n",
    "        tokens = re.findall(num_re, line)\n",
    "        # tokens is list of tuples; we want the first elements\n",
    "        if len(tokens) >= 3:\n",
    "            try:\n",
    "                year = int(float(tokens[0][0]))\n",
    "                month = int(float(tokens[1][0]))\n",
    "                day = int(float(tokens[2][0]))\n",
    "                return datetime(year, month, day)\n",
    "            except Exception:\n",
    "                continue\n",
    "    raise ValueError(\"Start date not found: expected a line with at least three numeric tokens (year month day).\")\n",
    "\n",
    "def extract_pre_table(lines):\n",
    "    pre = []\n",
    "    for line in lines:\n",
    "        if line.strip().upper().startswith('<BEGINTABLE>') or line.strip().upper() == '<BEGINTABLE>':\n",
    "            break\n",
    "        pre.append(line)\n",
    "    return pre\n",
    "\n",
    "def detect_header_and_table_start(lines):\n",
    "    # header must contain 'Seconds' and a token with 'Station' substring (e.g., StationName)\n",
    "    hdr_re = re.compile(r'\\bSeconds\\b', re.IGNORECASE)\n",
    "    station_re = re.compile(r'\\bStation', re.IGNORECASE)\n",
    "    for idx, line in enumerate(lines):\n",
    "        if hdr_re.search(line) and station_re.search(line):\n",
    "            # split by whitespace to get header fields (preserve order)\n",
    "            header_tokens = re.findall(r'\\S+', line.strip())\n",
    "            return header_tokens, idx + 1  # table starts next line\n",
    "    raise ValueError(\"Table header not found: expected a line containing 'Seconds' and 'Station'.\")\n",
    "\n",
    "def parse_row_by_header(line, header):\n",
    "    parts = re.findall(r'\\S+', line.strip())\n",
    "    if not parts:\n",
    "        return None\n",
    "    # Strategy: Attempt to map from left to right, but station (last header token) may contain spaces.\n",
    "    n_hdr = len(header)\n",
    "    if len(parts) >= n_hdr:\n",
    "        # assume station (last column) may include remaining tokens beyond n_hdr-1\n",
    "        mapped = {}\n",
    "        for i in range(n_hdr - 1):\n",
    "            mapped[header[i]] = parts[i]\n",
    "        # station and any trailing tokens\n",
    "        station_val = ' '.join(parts[n_hdr - 1:])\n",
    "        mapped[header[-1]] = station_val\n",
    "        return mapped\n",
    "    # If fewer tokens than headers, skip row\n",
    "    return None\n",
    "\n",
    "def is_numeric(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def parse_table(lines, header, start_idx):\n",
    "    entries = []\n",
    "    for line in lines[start_idx:]:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        mapped = parse_row_by_header(line, header)\n",
    "        if not mapped:\n",
    "            continue\n",
    "        # Seconds must exist and be numeric\n",
    "        seconds_key = None\n",
    "        for c in header:\n",
    "            if c.lower() == 'seconds':\n",
    "                seconds_key = c\n",
    "                break\n",
    "        if not seconds_key:\n",
    "            raise ValueError(\"Header missing 'Seconds' column after detection.\")\n",
    "        sec_val = mapped.get(seconds_key, '')\n",
    "        if not is_numeric(sec_val):\n",
    "            # try to clean scientific notation like 0.000000000000000E+000\n",
    "            sec_clean = sec_val.replace('D','E').replace('d','E')\n",
    "            if not is_numeric(sec_clean):\n",
    "                continue\n",
    "            sec_val = sec_clean\n",
    "            mapped[seconds_key] = sec_val\n",
    "        # store entry\n",
    "        entries.append(mapped)\n",
    "    return entries\n",
    "\n",
    "def write_station_files(entries, header, start_dt, outdir):\n",
    "    seconds_key = next(c for c in header if c.lower() == 'seconds')\n",
    "    station_key = None\n",
    "    # find header token that contains 'station' ignoring case\n",
    "    for c in header:\n",
    "        if 'station' in c.lower():\n",
    "            station_key = c\n",
    "            break\n",
    "    if not station_key:\n",
    "        raise ValueError(\"Header missing Station column.\")\n",
    "    grouped = defaultdict(list)\n",
    "    for row in entries:\n",
    "        sec = float(row[seconds_key])\n",
    "        dt = start_dt + timedelta(seconds=sec)\n",
    "        row_with_dt = dict(row)  # copy\n",
    "        row_with_dt['Datetime'] = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        grouped[row[station_key]].append(row_with_dt)\n",
    "\n",
    "    outdir = Path(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Exclude the Seconds column from output columns\n",
    "    out_columns = ['Datetime'] + [c for c in header if c != seconds_key]\n",
    "    \n",
    "    written = []\n",
    "    for station, rows in grouped.items():\n",
    "        safe_name = re.sub(r'[^\\w\\-_. ]', '_', station).strip().replace(' ', '_')\n",
    "        fname = outdir / f\"{safe_name}.csv\"\n",
    "        with fname.open('w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=out_columns)\n",
    "            writer.writeheader()\n",
    "            for r in rows:\n",
    "                # ensure all fields present\n",
    "                outrow = {col: r.get(col, '') for col in out_columns}\n",
    "                writer.writerow(outrow)\n",
    "        written.append(station)\n",
    "    return written, out_columns\n",
    "\n",
    "def extract_pre_table_groups(pre_lines):\n",
    "    # tokens containing letters, deduplicated in order\n",
    "    tokens = []\n",
    "    for line in pre_lines:\n",
    "        clean = re.sub(r'[,:;()\\/\\[\\]]', ' ', line)\n",
    "        for t in re.findall(r'\\S+', clean):\n",
    "            t_norm = re.sub(r'[^A-Za-z0-9_\\-]', '', t)\n",
    "            if not t_norm:\n",
    "                continue\n",
    "            # skip pure numeric tokens\n",
    "            if re.fullmatch(r'[+\\-]?\\d+(\\.\\d+)?([Ee][+\\-]?\\d+)?', t_norm):\n",
    "                continue\n",
    "            tokens.append(t_norm)\n",
    "    seen = set()\n",
    "    groups = []\n",
    "    for t in tokens:\n",
    "        up = t.upper()\n",
    "        if up in seen:\n",
    "            continue\n",
    "        seen.add(up)\n",
    "        groups.append(t)\n",
    "    return groups\n",
    "\n",
    "def main():\n",
    "\n",
    "    lines = read_lines(output_table)\n",
    "    pre_table = extract_pre_table(lines)\n",
    "    groups = extract_pre_table_groups(pre_table)\n",
    "\n",
    "    try:\n",
    "        start_dt = find_start_date(lines)\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        sys.exit(3)\n",
    "\n",
    "    try:\n",
    "        header, table_start_idx = detect_header_and_table_start(lines)\n",
    "    except ValueError as e:\n",
    "        print(\"Error:\", e)\n",
    "        sys.exit(4)\n",
    "\n",
    "    entries = parse_table(lines, header, table_start_idx)\n",
    "    if not entries:\n",
    "        print(\"No data rows parsed. Check table formatting.\")\n",
    "        sys.exit(5)\n",
    "\n",
    "    written_stations, out_columns = write_station_files(entries, header, start_dt, output_dir)\n",
    "\n",
    "    print(f\"Wrote {len(written_stations)} station files to {output_dir}.\")\n",
    "    print(f\"Detected header columns: {', '.join(header)}.\")\n",
    "    print(f\"Output columns (with Datetime): {', '.join(out_columns)}.\")\n",
    "    print(f\"Detected pre-table groups: {', '.join(groups) if groups else '(none)'}.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.3 Load csv files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file_1 = os.path.join(os.getcwd(),'out','PR.csv')\n",
    "#csv_file_1 = os.path.join(os.getcwd(),'out','FPOLIS.csv')\n",
    "#csv_file_1 = os.path.join(os.getcwd(),'res','res.csv')\n",
    "csv_file_1 = os.path.join(os.getcwd(),'res','nivel_itapema.csv')\n",
    "\n",
    "df_1 = pd.read_csv(csv_file_1)\n",
    "\n",
    "csv_file_2 = csv_file_1\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional - load a second dataframe if you want a plot with time series from different csv files or compare with measurements\n",
    "csv_file_2 = os.path.join(os.getcwd(),'res','obs.csv')\n",
    "df_2 = pd.read_csv(csv_file_2)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.4 Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_1\n",
    "\n",
    "out_dir = \"out\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "csv_path = os.path.join(os.getcwd(),out_dir, \"statistics.csv\")\n",
    "\n",
    "def column_stats_to_csv_exclude(\n",
    "    df: pd.DataFrame,\n",
    "    out_csv: str,\n",
    "    exclude: Optional[Iterable[str]] = None,\n",
    "    columns: Optional[Iterable[str]] = None,\n",
    "    decimals: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute stats (max, p99, p95, p90, median, mean, min) for numeric columns,\n",
    "    excluding columns in `exclude`. Save result to CSV and return DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    - df: input pandas DataFrame\n",
    "    - out_csv: path to output CSV file\n",
    "    - exclude: columns to exclude (e.g., ['x','y','z'])\n",
    "    - columns: optional list of columns to restrict to before exclusion\n",
    "    - decimals: optional integer to round results; None means no rounding\n",
    "    \"\"\"\n",
    "    exclude = set(exclude or [])\n",
    "    if columns is not None:\n",
    "        cols = [c for c in columns if c in df.columns and c not in exclude]\n",
    "    else:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        cols = [c for c in numeric_cols if c not in exclude]\n",
    "\n",
    "    if not cols:\n",
    "        raise ValueError(\"No numeric columns to compute after applying exclusion.\")\n",
    "\n",
    "    stats = {\n",
    "        \"max\": df[cols].max(skipna=True),\n",
    "        \"p99\": df[cols].quantile(0.99),\n",
    "        \"p95\": df[cols].quantile(0.95),\n",
    "        \"p90\": df[cols].quantile(0.90),\n",
    "        \"median\": df[cols].median(skipna=True),\n",
    "        \"mean\": df[cols].mean(skipna=True),\n",
    "        \"min\": df[cols].min(skipna=True),\n",
    "    }\n",
    "\n",
    "    result = pd.DataFrame(stats, index=cols)\n",
    "    result = result[[\"max\", \"p99\", \"p95\", \"p90\", \"median\", \"mean\", \"min\"]]\n",
    "    if decimals is not None:\n",
    "        result = result.round(decimals)\n",
    "\n",
    "    result.index.name = \"column\"\n",
    "    result.to_csv(out_csv)\n",
    "\n",
    "    return result\n",
    "\n",
    "stats_df = column_stats_to_csv_exclude(df, csv_path, exclude=[\"X\",\"Y\",\"Z\"], decimals=2)\n",
    "\n",
    "print(f\"Saved:\\n{csv_path}\")\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.5 Plot time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(os.getcwd(),'out')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "output_file = \"timeseries.png\"\n",
    "\n",
    "# Choose your column names (or set variable_name_2 = '' if you only want one trace)\n",
    "var1 = 'water_level'\n",
    "var2 = ''  # set to '' if you don't want a second trace\n",
    "\n",
    "days_between_ticks = 1  # number of days between ticks\n",
    "mode_var1 ='lines' #lines+markers, lines, markers\n",
    "mode_var2 ='lines' #lines+markers, lines, markers\n",
    "color_var1 = 'blue'\n",
    "color_var2 = 'red'\n",
    "legend_1 = 'res'\n",
    "legend_2 = 'obs'\n",
    "\n",
    "axis_labels = {\n",
    "  'water_level': 'Water Level (m)',\n",
    "  'velocity_modulus' : 'Velocity modulus (m/s)',\n",
    "  'temperature': 'Temperature (°C)'}\n",
    "\n",
    "date_format     = \"%d-%m-%Y\"\n",
    "dpi = 150 \n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"Plot_TimeSeries\")\n",
    "script_name = os.path.join(script_folder, \"plot_timeseries.py\")\n",
    "input_file  = os.path.join(script_folder, \"input_plot_timeseries.py\")\n",
    "\n",
    "config = {\n",
    "    'csv_file_1':      csv_file_1,\n",
    "    'csv_file_2':      csv_file_2,\n",
    "    'output_dir':      output_dir,\n",
    "    'output_file':     output_file,\n",
    "    'var1':            var1,\n",
    "    'var2':            var2,\n",
    "    'mode_var1':       mode_var1,\n",
    "    'mode_var2':       mode_var2,\n",
    "    'color_var1':      color_var1,\n",
    "    'color_var2':      color_var2,\n",
    "    'legend_1':        legend_1,\n",
    "    'legend_2':        legend_2,\n",
    "    'days_between':    days_between_ticks,\n",
    "    'axis_label_1':    axis_labels.get(var1),\n",
    "    'axis_label_2':    axis_labels.get(var2),\n",
    "    'date_format':     date_format,\n",
    "    'dpi':             dpi\n",
    "}\n",
    "\n",
    "# Write config to input_file\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# Auto-generated config\\n\\n\")\n",
    "    for key, val in config.items():\n",
    "        f.write(f\"{key} = {val!r}\\n\")\n",
    "\n",
    "print(f\"Wrote configuration to {input_file}\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", os.path.basename(script_name)],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.6 Compare with measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare observed and modelled time series by nearest-time matching (±30 min),\n",
    "compute statistics, save CSV and PNG plots.\n",
    "Assumes the first column in input DataFrames or CSVs is the time column.\n",
    "\"\"\"\n",
    "variable = \"water_level\"\n",
    "\n",
    "axis_labels = {\n",
    "  'water_level': 'Water Level (m)',\n",
    "  'velocity_modulus' : 'Velocity modulus (m/s)',\n",
    "  'temperature': 'Temperature (°C)'}\n",
    "\n",
    "# Load modelled and observed data\n",
    "df_mod = df_1\n",
    "df_obs = df_2\n",
    "\n",
    "# Settings\n",
    "TOLERANCE = pd.Timedelta(\"30min\")\n",
    "out_dir = \"out\"\n",
    "dpi = 150\n",
    "\n",
    "csv_path = os.path.join(os.getcwd(),out_dir, f\"obs_vs_mod_{variable}.csv\")\n",
    "fig_path = os.path.join(os.getcwd(),out_dir, f\"obs_vs_mod_{variable}.png\")\n",
    "residuals_path = os.path.join(os.getcwd(),out_dir, f\"residuals_{variable}.png\")\n",
    "\n",
    "def compute_metrics(obs_series, mod_series):\n",
    "    \"\"\"\n",
    "    Compute bias, RMSE, Pearson correlation, and R-squared (coefficient of determination)\n",
    "    \"\"\"\n",
    "    diff = mod_series - obs_series\n",
    "\n",
    "    ss_res = ((diff ** 2).sum())\n",
    "    ss_tot = (((obs_series - obs_series.mean()) ** 2).sum())\n",
    "    if ss_tot == 0.0:\n",
    "        r2 = float(\"nan\")\n",
    "    else:\n",
    "        r2 = 1.0 - ss_res / ss_tot\n",
    "\n",
    "    return {\n",
    "        \"bias\": float(diff.mean()),\n",
    "        \"rmse\": float(np.sqrt((diff**2).mean())),\n",
    "        \"corr\": float(obs_series.corr(mod_series)),\n",
    "        \"r2\": float(r2)\n",
    "    }\n",
    "def save_plots(df_cmp: pd.DataFrame, out_dir: str = out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if not {\"obs\", \"mod\"}.issubset(df_cmp.columns):\n",
    "        raise ValueError(\"df_cmp must contain 'obs' and 'mod' columns\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    df_cmp[[\"obs\", \"mod\"]].plot(ax=ax)\n",
    "    ax.set_title(f\"Observed vs. Modeled\")\n",
    "    ax.set_ylabel(axis_labels.get(variable))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fig_path, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    residual = df_cmp[\"mod\"] - df_cmp[\"obs\"]\n",
    "    fig, ax = plt.subplots(figsize=(12, 2))\n",
    "    residual.plot(ax=ax, color=\"black\", legend=False)\n",
    "    ax.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    ax.set_title(f\"Residual (Model – Obs)\")\n",
    "    ax.set_ylabel(\"m\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(residuals_path, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "\n",
    "def read_firstcol_time(path_or_df, value_name):\n",
    "    \"\"\"\n",
    "    Accepts a path to CSV or a DataFrame.\n",
    "    Assumes the first column is the time column and the second column (or remaining one)\n",
    "    is the value column to return as a Series with DatetimeIndex named value_name.\n",
    "    \"\"\"\n",
    "    if isinstance(path_or_df, str):\n",
    "        df = pd.read_csv(path_or_df, header=0)\n",
    "    elif isinstance(path_or_df, pd.DataFrame):\n",
    "        df = path_or_df.copy()\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a file path or a pandas DataFrame\")\n",
    "\n",
    "    if df.shape[1] < 2:\n",
    "        raise ValueError(\"Input must have at least two columns (time + value)\")\n",
    "\n",
    "    time_col = df.columns[0]\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    series = pd.Series(df[variable].values, index=df[time_col], name=value_name)\n",
    "    series = series.sort_index()\n",
    "    return series\n",
    "\n",
    "def pair_by_nearest(obs_series: pd.Series, mod_series: pd.Series, tolerance: pd.Timedelta = TOLERANCE):\n",
    "    obs_df = obs_series.rename(\"obs\").reset_index().rename(columns={obs_series.index.name or \"index\": \"time\"})\n",
    "    mod_df = mod_series.rename(\"mod\").reset_index().rename(columns={mod_series.index.name or \"index\": \"time\"})\n",
    "    obs_df[\"time\"] = pd.to_datetime(obs_df[\"time\"])\n",
    "    mod_df[\"time\"] = pd.to_datetime(mod_df[\"time\"])\n",
    "    obs_df = obs_df.sort_values(\"time\")\n",
    "    mod_df = mod_df.sort_values(\"time\")\n",
    "    merged = pd.merge_asof(\n",
    "        mod_df,\n",
    "        obs_df,\n",
    "        on=\"time\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=tolerance,\n",
    "        suffixes=(\"_mod\", \"_obs\")\n",
    "    )\n",
    "    merged = merged.dropna(subset=[\"mod\", \"obs\"])\n",
    "    if merged.empty:\n",
    "        return pd.DataFrame(columns=[\"obs\", \"mod\"]).astype(float)\n",
    "    paired = merged.set_index(\"time\")[[\"obs\", \"mod\"]]\n",
    "    return paired\n",
    "\n",
    "def main(obs_input, mod_input, out_dir=out_dir, tolerance=TOLERANCE):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    obs_series = read_firstcol_time(obs_input, value_name=\"obs\")\n",
    "    mod_series = read_firstcol_time(mod_input, value_name=\"mod\")\n",
    "    if obs_series.index.tz is not None and mod_series.index.tz is None:\n",
    "        mod_series = mod_series.tz_localize(obs_series.index.tz)\n",
    "    elif mod_series.index.tz is not None and obs_series.index.tz is None:\n",
    "        obs_series = obs_series.tz_localize(mod_series.index.tz)\n",
    "    paired = pair_by_nearest(obs_series, mod_series, tolerance=tolerance)\n",
    "    if paired.empty:\n",
    "        raise RuntimeError(\"No pairs found within tolerance. Check your indices and tolerance value.\")\n",
    "    metrics = compute_metrics(paired[\"obs\"], paired[\"mod\"])\n",
    "    paired.to_csv(csv_path)\n",
    "    save_plots(paired, out_dir=out_dir)\n",
    "    print(\"OBS range:\", obs_series.index.min(), \"→\", obs_series.index.max())\n",
    "    print(\"MOD range:\", mod_series.index.min(), \"→\", mod_series.index.max())\n",
    "    print(\"Paired points:\", len(paired))\n",
    "    print(\"Metrics (nearest snap):\", metrics)\n",
    "    print(f\"Saved:\\n{csv_path}\\n{fig_path}\\n{residuals_path}\")\n",
    "    return metrics, paired\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    metrics, paired = main(df_obs, df_mod, out_dir=out_dir, tolerance=TOLERANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.7 Harmonic analysis\n",
    "#Based on a Python distribution of the MatLab package UTide (https://github.com/wesleybowman/UTide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.7.1 Calculate the anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calculate the deviations of the elevations from their mean (stored in a new column called \"anomaly\"), \n",
    "and then interpolate to fill in the nan values in the anomaly.\n",
    "'''\n",
    "obs = df_1\n",
    "variable = \"water_level\"\n",
    "\n",
    "bad = obs[variable] == -99\n",
    "\n",
    "obs.loc[bad, variable] = np.nan\n",
    "obs[\"anomaly\"] = obs[variable] - obs[variable].mean()\n",
    "obs[\"anomaly\"] = obs[\"anomaly\"].interpolate()\n",
    "print(f\"{bad.sum()} points were flagged 'bad' and interpolated\")\n",
    "\n",
    "obs[\"Datetime\"] = pd.to_datetime(obs[\"Datetime\"])\n",
    "obs = obs.set_index(\"Datetime\")\n",
    "\n",
    "obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.7.2 Solve to obtain the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = -27 #define the correct latitude of your data\n",
    "\n",
    "coef = utide.solve(\n",
    "    obs.index,\n",
    "    obs[\"anomaly\"],\n",
    "    lat=lat,\n",
    "    method=\"ols\",\n",
    "    conf_int=\"MC\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.7.3 Save the amplitudes and phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the amplitudes and phases of tidal constituents to a csv file\n",
    "\n",
    "out_dir = \"out\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "csv_path = os.path.join(os.getcwd(),out_dir, \"tidal_constituents.csv\")\n",
    "\n",
    "names = coef['name']            # array of constituent names\n",
    "amplitudes = coef['A']         # amplitude (same units as input)\n",
    "phases_deg = coef['g']         # phase in degrees (astronomical convention)\n",
    "\n",
    "for nm, A, g in zip(names, amplitudes, phases_deg):\n",
    "    print(f\"{nm:6s}  amplitude = {A:.4f} m  phase = {g:.2f} deg\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name': coef['name'],\n",
    "    'amplitude': pd.Series(coef['A']).round(4),\n",
    "    'phase_deg': pd.Series(coef['g']).round(2)\n",
    "})\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Saved:\\n{csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.7.4 Generate the astronomical tide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The amplitudes and phases from the fit are now in the coef data structure, \n",
    "which can be used directly in the reconstruct function to generate the tides.\n",
    "'''\n",
    "\n",
    "tide = utide.reconstruct(obs.index, coef, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.7.5 Plot and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot and save anomaly, astronomical tide, and residual\n",
    "\n",
    "out_dir = \"out\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "fig_path = os.path.join(os.getcwd(),out_dir, \"tidal_analysis.png\")\n",
    "csv_path = os.path.join(os.getcwd(),out_dir, \"tidal_analysis.csv\")\n",
    "\n",
    "dpi = 150\n",
    "\n",
    "t = obs.index.to_pydatetime()\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(figsize=(17, 5), nrows=2, sharey=True, sharex=True)\n",
    "\n",
    "ax0.plot(t, obs.anomaly, label=\"Data\", color=\"C0\")\n",
    "ax0.plot(t, tide.h, label=\"Astronomical tide\", color=\"C1\")\n",
    "ax0.set_ylabel(\"Water level (m)\")\n",
    "\n",
    "residual = obs.anomaly - tide.h\n",
    "ax1.plot(t, residual, label=\"Residual\", color=\"C2\")\n",
    "ax1.grid(which=\"major\", axis=\"y\", linestyle=\"--\", color=\"0.8\", linewidth=0.8)\n",
    "\n",
    "ax1.set_ylabel(\"Data - Astronomical tide (m)\")\n",
    "\n",
    "fig.legend(ncol=3, loc=\"upper center\");\n",
    "\n",
    "fig.savefig(fig_path, dpi=dpi)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Datetime': t,\n",
    "    'Anomaly': obs.anomaly.round(2),\n",
    "    'Astronomical_tide': tide.h.round(2),\n",
    "    'Rsidual': residual.round(2)\n",
    "})\n",
    "df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Saved:\\n{fig_path}\\n{csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1 Plot maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SET-UP: Define paths and file names\n",
    "# ============================\n",
    "\n",
    "backup_root       = os.path.join(os.getcwd(),'res')\n",
    "hdf5_file         =r'Hydrodynamic_2_Surface.hdf5'\n",
    "hdf5_file_vectors =r'Hydrodynamic_2_Surface.hdf5'\n",
    "start_date_str    = '2025-9-25'\n",
    "end_date_str      = '2025-9-27'\n",
    "\n",
    "variable = \"velocity modulus\"  # Change as needed\n",
    "\n",
    "out_dir   = os.path.join(os.getcwd(),'out', \"maps\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# DEFINE VARIABLE-LABEL DICTIONARY\n",
    "# ============================\n",
    "variable_label_dict = {\n",
    "    \"velocity modulus\": \"Velocity Modulus(m/s)\",\n",
    "    \"salinity\": \"Salinity(psu)\",\n",
    "    \"temperature\": \"Temperature(°C)\",\n",
    "    \"water level\": \"Water Level(m)\"\n",
    "}\n",
    "\n",
    "variable_vector = [\"velocity U\",\"velocity V\"]\n",
    "\n",
    "label = variable_label_dict.get(variable, \"Unknown Variable\")  # Fetch label from dictionary\n",
    "\n",
    "start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "# Option to enable or disable vector overlay and image frame saving\n",
    "show_vectors = True      # Set to False to disable wind vectors in the animation\n",
    "save_frames = True       # Set to False to disable saving individual image frames\n",
    "\n",
    "# User-specified parameters for skipping time steps, adjusting extent, vectors, etc.\n",
    "skip_time = 3           # Sample every nth time step\n",
    "extent_cells = 1        # Number of extra cells added to the plot extent\n",
    "increase_zoom_level = 1 # Increase computed zoom level by this amount to improve background image resolution\n",
    "skip_vector = 5         # Skip factor when plotting vectors (to reduce clutter)\n",
    "vector_scale = 10       # Scale for the current vector arrows\n",
    "vector_color = 'white'  # Color for the wind vectors\n",
    "transparency_factor = 1.\n",
    "dpi = 150               #specify the DPI\n",
    "cmap = \"jet\"       # colour scale (jet, viridis,...)\n",
    "\n",
    "#Input file to plot_hdf5.py\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"Plot_HDF5\")\n",
    "script_name = os.path.join(script_folder, \"plot_hdf5.py\")\n",
    "input_file  = os.path.join(script_folder, \"Input_Plot_HDF5.py\")\n",
    "\n",
    "# -----------------------\n",
    "# WRITE Input_Plot_HDF5.py\n",
    "# -----------------------\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"backup_root       = r'{backup_root}'\\n\")\n",
    "    f.write(f\"hdf5_file         =r'{hdf5_file}'\\n\")\n",
    "    f.write(f\"hdf5_file_vectors=r'{hdf5_file_vectors}'\\n\")\n",
    "    f.write(f\"figures_folder    = r'{out_dir}'\\n\")\n",
    "    f.write(f\"start_date_str    = '{start_date_str}'\\n\")\n",
    "    f.write(f\"end_date_str      = '{end_date_str}'\\n\")\n",
    "    f.write(f\"variable          = '{variable}'\\n\")\n",
    "    f.write(f\"label             = '{label}'\\n\")\n",
    "    f.write(f\"variable_vector   = {variable_vector}\\n\")\n",
    "    f.write(f\"show_vectors      = {show_vectors}\\n\")\n",
    "    f.write(f\"save_frames       = {save_frames}\\n\")\n",
    "    f.write(f\"skip_time         = {skip_time}\\n\")\n",
    "    f.write(f\"extent_cells      = {extent_cells}\\n\")\n",
    "    f.write(f\"increase_zoom_level = {increase_zoom_level}\\n\")\n",
    "    f.write(f\"skip_vector       = {skip_vector}\\n\")\n",
    "    f.write(f\"vector_scale      = {vector_scale}\\n\")\n",
    "    f.write(f\"vector_color      = '{vector_color}'\\n\")\n",
    "    f.write(f\"transparency_factor = {transparency_factor}\\n\")\n",
    "    f.write(f\"dpi               = {dpi}\\n\")\n",
    "    f.write(f\"cmap               = '{cmap}'\\n\")\n",
    "# -----------------------\n",
    "# RUN THE ANIMATION SCRIPT ONCE\n",
    "# -----------------------\n",
    "print(\"Starting multi-day animation…\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", os.path.basename(script_name)],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Animation completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: plot_hdf5.py exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2 Plot statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SET-UP: Define paths and file names\n",
    "# ============================\n",
    "backup_root       = os.path.join(os.getcwd(),'res')\n",
    "hdf5_file         =r'Hydrodynamic_2_Surface.hdf5'\n",
    "hdf5_file_vectors =r'Hydrodynamic_2_Surface.hdf5'\n",
    "start_date_str    = '2025-9-25'\n",
    "end_date_str      = '2025-9-27'\n",
    "\n",
    "variable = \"velocity modulus\"  # Change as needed\n",
    "variable_vector = [\"velocity U\",\"velocity V\"]\n",
    "\n",
    "out_dir   = os.path.join(os.getcwd(),'out', \"statistics\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "percentil = 50 #max = 100, min = 0, median = 50\n",
    "countour_levels   = '[]' #'[0.1, 0.2]'\n",
    "vmin = 0.\n",
    "vmax = 1.0\n",
    "\n",
    "#if 3D\n",
    "percentil_map = \"surface\" #max_value, surface, layer\n",
    "\n",
    "#if percentil_map = layer\n",
    "nlayer = 1\n",
    "\n",
    "# ============================\n",
    "# DEFINE VARIABLE-LABEL DICTIONARY\n",
    "# ============================\n",
    "variable_label_dict = {\n",
    "    \"velocity modulus\": \"Velocity Modulus(m/s)\",\n",
    "    \"salinity\": \"Salinity(psu)\",\n",
    "    \"temperature\": \"Temperature(°C)\",\n",
    "    \"water level\": \"Water Level(m)\"\n",
    "}\n",
    "\n",
    "label = variable_label_dict.get(variable, \"Unknown Variable\")  # Fetch label from dictionary\n",
    "\n",
    "# User-specified parameters\n",
    "extent_cells = 1        # Number of extra cells added to the plot extent\n",
    "increase_zoom_level = 1 # Increase computed zoom level by this amount to improve background image resolution\n",
    "transparency_factor = 1.\n",
    "dpi = 150               #specify the DPI\n",
    "cmap = \"jet\"       # colour scale (jet, viridis,...)\n",
    "\n",
    "show_vectors      = True\n",
    "variable_vector   = ['velocity U', 'velocity V']\n",
    "skip_vector       = 5\n",
    "vector_scale      = 10\n",
    "vector_color      = 'white'\n",
    "\n",
    "#Input file to plot_hdf5.py\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"Plot_HDF5_Statistics\")\n",
    "script_name = os.path.join(script_folder, \"plot_hdf5_statistics.py\")\n",
    "input_file  = os.path.join(script_folder, \"input_plot_hdf5_statistics.py\")\n",
    "\n",
    "start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "# -----------------------\n",
    "# WRITE input_plot_hdf5_statistics.py\n",
    "# -----------------------\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"backup_root       = r'{backup_root}'\\n\")\n",
    "    f.write(f\"hdf5_file         =r'{hdf5_file}'\\n\")\n",
    "    f.write(f\"hdf5_file_vectors=r'{hdf5_file_vectors}'\\n\")\n",
    "    f.write(f\"out_dir           = r'{out_dir}'\\n\")\n",
    "    f.write(f\"start_date_str    = '{start_date_str}'\\n\")\n",
    "    f.write(f\"end_date_str      = '{end_date_str}'\\n\")\n",
    "    f.write(f\"variable          = '{variable}'\\n\")\n",
    "    f.write(f\"label             = '{label}'\\n\")\n",
    "    f.write(f\"percentil         = {percentil}\\n\")\n",
    "    f.write(f\"countour_levels   = {countour_levels}\\n\")\n",
    "    f.write(f\"vmin              = {vmin}\\n\")\n",
    "    f.write(f\"vmax              = {vmax}\\n\")\n",
    "    f.write(f\"percentil_map     = '{percentil_map}'\\n\")\n",
    "    f.write(f\"nlayer            = {nlayer}\\n\")\n",
    "    f.write(f\"extent_cells      = {extent_cells}\\n\")\n",
    "    f.write(f\"increase_zoom_level = {increase_zoom_level}\\n\")\n",
    "    f.write(f\"transparency_factor = {transparency_factor}\\n\")\n",
    "    f.write(f\"dpi               = {dpi}\\n\")\n",
    "    f.write(f\"cmap              = '{cmap}'\\n\")\n",
    "    f.write(f\"skip_vector       = {skip_vector}\\n\")\n",
    "    f.write(f\"vector_scale      = {vector_scale}\\n\")\n",
    "    f.write(f\"vector_color      = '{vector_color}'\\n\")\n",
    "    f.write(f\"variable_vector   = {variable_vector}\\n\")\n",
    "    f.write(f\"show_vectors      = {show_vectors}\\n\")\n",
    "    \n",
    "# -----------------------\n",
    "# RUN THE SCRIPT\n",
    "# -----------------------\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", os.path.basename(script_name)],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"Statistics completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: plot_hdf5_statistics.py exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Plot vertical cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3.1 Define paths and file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup_root       = os.path.join(os.getcwd(),'res')\n",
    "backup_root       = r'C:\\Users\\aquaf\\OneDrive\\Projetos\\Aquaflow\\Hidromod\\2025\\Proj_575_Consulgal_Soyo\\Aplica\\MOHID_Water\\run_cases\\Soyo\\backup'\n",
    "hdf5_file         =r'WaterProperties_2.hdf5'\n",
    "hdf5_file_vectors =r'Hydrodynamic_2.hdf5'\n",
    "start_date_str    = '2025-1-6'\n",
    "end_date_str      = '2025-1-7'\n",
    "\n",
    "variable = \"salinity\"  # Change as needed\n",
    "variable_vector = [\"velocity U\",\"velocity V\"]\n",
    "\n",
    "out_dir   = os.path.join(os.getcwd(),'out', \"vertical_cut\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "def collect_hdf5_paths(root, h5file, sd, ed):\n",
    "    paths = []\n",
    "    for entry in os.scandir(root):\n",
    "        if not entry.is_dir():\n",
    "            continue\n",
    "        try:\n",
    "            day = datetime.strptime(entry.name.split('_')[0], \"%Y%m%d\").date()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if sd <= day <= ed:\n",
    "            # Look directly inside the date-folder\n",
    "            pattern = os.path.join(entry.path, h5file)\n",
    "            for f in glob.glob(pattern):\n",
    "                if os.path.isfile(f):\n",
    "                    paths.append(f)\n",
    "    return sorted(paths)\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "sd = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "ed = datetime.strptime(end_date_str,   \"%Y-%m-%d\").date()\n",
    "\n",
    "hdf5_files = collect_hdf5_paths(backup_root, hdf5_file, sd, ed)\n",
    "\n",
    "if not hdf5_files:\n",
    "    raise RuntimeError(f\"No  {hdf5_file} between {start_date_str} and {end_date_str}\")\n",
    "    \n",
    "# ----------------------------------------\n",
    "# GRID\n",
    "# ----------------------------------------\n",
    "with h5py.File(hdf5_files[0], \"r\") as h5f:\n",
    "    Xr = h5f[\"Grid\"][\"Longitude\"][:]\n",
    "    Yr = h5f[\"Grid\"][\"Latitude\"][:]\n",
    "    zi = h5f[\"Grid\"][\"Bathymetry\"][:]\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3.2 Load or create a new file with a polyline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyline_file = os.path.join(os.getcwd(),\"VerticalCutPath.csv\")\n",
    "\n",
    "# Load existing stations (format: lon lat)\n",
    "if os.path.exists(polyline_file):\n",
    "    stations_df = pd.read_csv(\n",
    "        polyline_file,\n",
    "        sep=',',\n",
    "        header=None,\n",
    "        names=['lon', 'lat'],\n",
    "        engine='python'\n",
    "    )\n",
    "    print(f\"Loaded points from {polyline_file}\")\n",
    "else:\n",
    "    stations_df = pd.DataFrame(columns=['lon', 'lat'])\n",
    "    with open(polyline_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "    print(\"Starting with zero stations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3.3 Visualise or define a new polyline on the map  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Start timing\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "# -------------------------------\n",
    "# Assume Xr, Yr, zi defined elsewhere\n",
    "# -------------------------------\n",
    "LonGrid = np.array(Xr)\n",
    "LatGrid = np.array(Yr)\n",
    "\n",
    "# -------------------------------\n",
    "# Build discrete colormap\n",
    "# -------------------------------\n",
    "_nbins = 10\n",
    "_bins = None\n",
    "_discrete_colors = None\n",
    "\n",
    "def map_value_to_color(value):\n",
    "    if value == -99:\n",
    "        return \"#ffffff00\"\n",
    "    idx = np.digitize(value, _bins) - 1\n",
    "    idx = int(np.clip(idx, 0, _nbins - 1))\n",
    "    return _discrete_colors[idx]\n",
    "\n",
    "def precompute_color_grid(zi, nbins=10):\n",
    "    global _bins, _nbins, _discrete_colors\n",
    "    _nbins = nbins\n",
    "    valid = zi != -99\n",
    "    if np.any(valid):\n",
    "        vmin, vmax = zi[valid].min(), zi[valid].max()\n",
    "    else:\n",
    "        vmin, vmax = 0, 1\n",
    "    _bins = np.linspace(vmin, vmax, nbins + 1)\n",
    "    cmap = plt.colormaps.get_cmap('viridis')\n",
    "    _discrete_colors = [mcolors.to_hex(c) for c in cmap(np.linspace(0, 1, nbins))]\n",
    "    return np.vectorize(map_value_to_color)(zi)\n",
    "\n",
    "color_mapped_zi = precompute_color_grid(zi)\n",
    "\n",
    "# -------------------------------\n",
    "# Create base map\n",
    "# -------------------------------\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "m = Map(center=(LatGrid.mean(), LonGrid.mean()), zoom=8)\n",
    "\n",
    "# -------------------------------\n",
    "# Split raster into GeoJSON blocks\n",
    "# -------------------------------\n",
    "lon_sw = LonGrid[:-1, :-1]; lon_se = LonGrid[:-1, 1:]\n",
    "lon_ne = LonGrid[1:, 1:];   lon_nw = LonGrid[1:, :-1]\n",
    "lat_sw = LatGrid[:-1, :-1]; lat_se = LatGrid[:-1, 1:]\n",
    "lat_ne = LatGrid[1:, 1:];   lat_nw = LatGrid[1:, :-1]\n",
    "\n",
    "block_size = 10\n",
    "block_layers = {}\n",
    "\n",
    "def generate_block_geojson(br, bc):\n",
    "    features = []\n",
    "    rows, cols = zi.shape\n",
    "    i0, i1 = br * block_size, min((br + 1) * block_size, rows)\n",
    "    j0, j1 = bc * block_size, min((bc + 1) * block_size, cols)\n",
    "    for i in range(i0, i1):\n",
    "        for j in range(j0, j1):\n",
    "            if zi[i, j] == -99:\n",
    "                continue\n",
    "            coords = [[\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])],\n",
    "                [float(lon_se[i, j]), float(lat_se[i, j])],\n",
    "                [float(lon_ne[i, j]), float(lat_ne[i, j])],\n",
    "                [float(lon_nw[i, j]), float(lat_nw[i, j])],\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])]\n",
    "            ]]\n",
    "            feat = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\"type\": \"Polygon\", \"coordinates\": coords},\n",
    "                \"properties\": {\n",
    "                    \"fill\": map_value_to_color(zi[i, j]),\n",
    "                    \"stroke\": \"#000000\",\n",
    "                    \"fill-opacity\": 0.5,\n",
    "                    \"stroke-width\": 0.2,\n",
    "                    \"i\": i, \"j\": j\n",
    "                }\n",
    "            }\n",
    "            features.append(feat)\n",
    "    return {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "def update_all_blocks():\n",
    "    for lyr in block_layers.values():\n",
    "        m.remove_layer(lyr)\n",
    "    block_layers.clear()\n",
    "    rows, cols = zi.shape\n",
    "    n_br = (rows + block_size - 1) // block_size\n",
    "    n_bc = (cols + block_size - 1) // block_size\n",
    "    for br in range(n_br):\n",
    "        for bc in range(n_bc):\n",
    "            fc = generate_block_geojson(br, bc)\n",
    "            if not fc[\"features\"]:\n",
    "                continue\n",
    "            layer = GeoJSON(\n",
    "                data=fc,\n",
    "                style_callback=lambda f: {\n",
    "                    \"fillColor\": f[\"properties\"][\"fill\"],\n",
    "                    \"color\": f[\"properties\"][\"stroke\"],\n",
    "                    \"weight\": f[\"properties\"][\"stroke-width\"],\n",
    "                    \"fillOpacity\": f[\"properties\"][\"fill-opacity\"],\n",
    "                }\n",
    "            )\n",
    "            m.add_layer(layer)\n",
    "            block_layers[(br, bc)] = layer\n",
    "\n",
    "update_all_blocks()\n",
    "print(f\"Raster layering time: {time.time() - start_time:.2f} sec\")\n",
    "\n",
    "# -------------------------------\n",
    "from ipyleaflet import Polyline\n",
    "\n",
    "# Internal state\n",
    "polyline_layer = None\n",
    "polyline_points = []        # list of [lat, lon]\n",
    "preloaded_point_ids = []    # indexes of preloaded vertices (0..n-1)\n",
    "\n",
    "# Build initial polyline from stations_df (preserve order)\n",
    "for idx, row in stations_df.iterrows():\n",
    "    lon = float(row[\"lon\"])\n",
    "    lat = float(row[\"lat\"])\n",
    "    polyline_points.append([lat, lon])\n",
    "    preloaded_point_ids.append(len(polyline_points) - 1)\n",
    "\n",
    "# If there are preloaded vertices, create the Polyline layer\n",
    "def add_or_update_polyline_layer():\n",
    "    global polyline_layer\n",
    "    if polyline_layer is not None:\n",
    "        # update locations in-place to avoid removing/adding layer\n",
    "        polyline_layer.locations = polyline_points\n",
    "    else:\n",
    "        if not polyline_points:\n",
    "            return\n",
    "        polyline_layer = Polyline(\n",
    "            locations=polyline_points,\n",
    "            color=\"blue\",\n",
    "            weight=3,\n",
    "            opacity=0.8\n",
    "        )\n",
    "        m.add_layer(polyline_layer)\n",
    "\n",
    "add_or_update_polyline_layer()\n",
    "\n",
    "# -------------------------------\n",
    "# DrawControl handlers: create / edited / deleted\n",
    "# -------------------------------\n",
    "def handle_draw_polyline(target, action, geo_json):\n",
    "    \"\"\"\n",
    "    DrawControl callback for polylines.\n",
    "    - action == \"created\": geo_json is the created feature GeoJSON\n",
    "    - action == \"edited\": geo_json contains features with new coords\n",
    "    - action == \"deleted\": geo_json contains deleted features\n",
    "    \"\"\"\n",
    "    global polyline_layer, polyline_points, preloaded_point_ids\n",
    "\n",
    "    if action == \"created\":\n",
    "        # Expect a LineString geometry: coordinates are [[lon, lat], ...]\n",
    "        geom = geo_json.get(\"geometry\", {})\n",
    "        if geom.get(\"type\") not in (\"LineString\", \"MultiLineString\"):\n",
    "            print(\"Created geometry is not a polyline; ignoring.\")\n",
    "            return\n",
    "\n",
    "        coords = geom[\"coordinates\"]\n",
    "        # If MultiLineString, take first part\n",
    "        if geom[\"type\"] == \"MultiLineString\":\n",
    "            coords = coords[0] if coords else []\n",
    "\n",
    "        # Replace current polyline with the newly drawn one.\n",
    "        polyline_points = [[float(c[1]), float(c[0])] for c in coords]\n",
    "        # Reset preloaded ids since drawn polyline replaces initial vertices\n",
    "        preloaded_point_ids = []\n",
    "        add_or_update_polyline_layer()\n",
    "\n",
    "        # Remove DrawControl's default temporary point/feature layer(s) if any\n",
    "        # (We remove only GeoJSON layers that look like the DrawControl \"created\" temp features)\n",
    "        for lyr in list(m.layers):\n",
    "            if isinstance(lyr, GeoJSON):\n",
    "                # heuristic: DrawControl's created layer often has feature properties like geometry.type\n",
    "                props = lyr.data.get(\"features\", [lyr.data])[0].get(\"properties\", {})\n",
    "                if props.get(\"_leaflet_id\") or props.get(\"feature_type\") or lyr.data.get(\"geometry\", {}).get(\"type\") in (\"LineString\", \"MultiLineString\"):\n",
    "                    try:\n",
    "                        m.remove_layer(lyr)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        print(f\"Created polyline with {len(polyline_points)} points\")\n",
    "\n",
    "    elif action == \"edited\":\n",
    "        # GeoJSON may contain multiple features; update the first polyline feature found\n",
    "        feats = geo_json.get(\"features\", []) or [geo_json]\n",
    "        updated = False\n",
    "        for f in feats:\n",
    "            geom = f.get(\"geometry\", {})\n",
    "            if geom.get(\"type\") not in (\"LineString\", \"MultiLineString\"):\n",
    "                continue\n",
    "            coords = geom[\"coordinates\"]\n",
    "            if geom[\"type\"] == \"MultiLineString\":\n",
    "                coords = coords[0] if coords else []\n",
    "            polyline_points = [[float(c[1]), float(c[0])] for c in coords]\n",
    "            add_or_update_polyline_layer()\n",
    "            updated = True\n",
    "            break\n",
    "        if updated:\n",
    "            print(f\"Edited polyline now has {len(polyline_points)} points\")\n",
    "        else:\n",
    "            print(\"Edited event contained no polyline features\")\n",
    "\n",
    "    elif action == \"deleted\":\n",
    "        # If user deleted the polyline via DrawControl, clear internal state\n",
    "        # geo_json may contain features with ids; we'll just clear in any case\n",
    "        polyline_points = []\n",
    "        preloaded_point_ids = []\n",
    "        if polyline_layer is not None:\n",
    "            try:\n",
    "                m.remove_layer(polyline_layer)\n",
    "            except Exception:\n",
    "                pass\n",
    "            polyline_layer = None\n",
    "        print(\"Polyline deleted\")\n",
    "\n",
    "\n",
    "draw_control = DrawControl(polygon={}, rectangle={}, circle={}, circlemarker={}, \n",
    "    polyline={\"shapeOptions\": {\"color\": \"#0000FF\"}}, marker={})\n",
    "draw_control.on_draw(handle_draw_polyline)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# -------------------------------\n",
    "# Save polyline vertices preserving preloaded ordering first\n",
    "# -------------------------------\n",
    "def save_polyline(path):\n",
    "    \"\"\"\n",
    "    Save vertices. Writes preloaded vertices first,\n",
    "    then any remaining points are appended.\n",
    "    Format per line: lon,lat\n",
    "    \"\"\"\n",
    "    # Build ordered list: first preloaded indexes (if any), then others in polyline order\n",
    "    if not polyline_points:\n",
    "        print(\"No polyline points to save\")\n",
    "        return\n",
    "\n",
    "    # Determine which indices are preloaded (if preloaded_point_ids is non-empty and within range)\n",
    "    all_indices = list(range(len(polyline_points)))\n",
    "    pre = [i for i in preloaded_point_ids if i in all_indices]\n",
    "    appended = [i for i in all_indices if i not in pre]\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        for i in pre + appended:\n",
    "            lat, lon = polyline_points[i]\n",
    "            f.write(f\"{lon},{lat}\\n\")\n",
    "\n",
    "    print(f\"Saved {len(polyline_points)} polyline points to '{path}'\")\n",
    "\n",
    "save_btn_poly = widgets.Button(description=\"Save polyline\", button_style=\"info\")\n",
    "save_btn_poly.on_click(lambda b: save_polyline(polyline_file))\n",
    "with output:\n",
    "    display(save_btn_poly)\n",
    "\n",
    "# -------------------------------\n",
    "# Show the map\n",
    "# -------------------------------\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3.4 Run script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameters\n",
    "contour_levels   = '[]' #'[0.1, 0.2]'\n",
    "vmin = None # Define None for global color limits\n",
    "vmax = 20 # Define None for global color limits\n",
    "extent_cells = 1        # Number of extra cells added to the plot extent\n",
    "increase_zoom_level = 1 # Increase computed zoom level by this amount to improve background image resolution\n",
    "transparency_factor = 1.\n",
    "dpi = 150               #specify the DPI\n",
    "cmap = \"jet\"       # colour scale (jet, viridis,...)\n",
    "save_frames = True       # Set to False to disable saving individual image frames\n",
    "\n",
    "# Set to True to show velocity vectors, False to hide them.\n",
    "show_vectors = True\n",
    "\n",
    "# Quiver visual tuning (adjust interactively if needed)\n",
    "scale_quiver = 2.0  # larger -> arrows shorter; tune for your units\n",
    "quiver_width = 0.003\n",
    "quiver_color = \"white\"\n",
    "max_arrows_across = 40\n",
    "max_arrows_vertical = 20\n",
    "min_vector_mag = 1e-4  # mask vectors below this magnitude\n",
    "\n",
    "# ============================\n",
    "# DEFINE VARIABLE-LABEL DICTIONARY\n",
    "# ============================\n",
    "variable_label_dict = {\n",
    "    \"velocity modulus\": \"Velocity Modulus(m/s)\",\n",
    "    \"salinity\": \"Salinity(psu)\",\n",
    "    \"temperature\": \"Temperature(°C)\",\n",
    "    \"water level\": \"Water Level(m)\"\n",
    "}\n",
    "\n",
    "label = variable_label_dict.get(variable, \"Unknown Variable\")  # Fetch label from dictionary\n",
    "\n",
    "#Input file to plot_hdf5.py\n",
    "script_folder = os.path.join(os.getcwd(), \"work\", \"Plot_Vertical_Cut\")\n",
    "script_name = os.path.join(script_folder, \"plot_hdf5_vertical_cut.py\")\n",
    "input_file  = os.path.join(script_folder, \"Input_Plot_HDF5_Cut.py\")\n",
    "\n",
    "start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").date()\n",
    "end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\").date()\n",
    "\n",
    "# -----------------------\n",
    "# WRITE Input_Plot_HDF5_Cut.py\n",
    "# -----------------------\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"backup_root       = r'{backup_root}'\\n\")\n",
    "    f.write(f\"hdf5_file         =r'{hdf5_file}'\\n\")\n",
    "    f.write(f\"hdf5_file_vectors =r'{hdf5_file_vectors}'\\n\")\n",
    "    f.write(f\"figures_folder    = r'{out_dir}'\\n\")\n",
    "    f.write(f\"csv_file          = r'{polyline_file}'\\n\")\n",
    "    f.write(f\"start_date_str    = '{start_date_str}'\\n\")\n",
    "    f.write(f\"end_date_str      = '{end_date_str}'\\n\")\n",
    "    f.write(f\"variable          = '{variable}'\\n\")\n",
    "    f.write(f\"label             = '{label}'\\n\")\n",
    "    f.write(f\"variable_vector   = {variable_vector}\\n\")\n",
    "    f.write(f\"show_vectors      = {show_vectors}\\n\")\n",
    "    f.write(f\"save_frames       = {save_frames}\\n\")\n",
    "    f.write(f\"skip_time         = {skip_time}\\n\")\n",
    "    f.write(f\"contour_levels    = {contour_levels}\\n\")\n",
    "    f.write(f\"vmin              = {vmin}\\n\")\n",
    "    f.write(f\"vmax              = {vmax}\\n\")\n",
    "    f.write(f\"extent_cells      = {extent_cells}\\n\")\n",
    "    f.write(f\"increase_zoom_level = {increase_zoom_level}\\n\")\n",
    "    f.write(f\"transparency_factor = {transparency_factor}\\n\")\n",
    "    f.write(f\"dpi               = {dpi}\\n\")\n",
    "    f.write(f\"cmap              = '{cmap}'\\n\")\n",
    "    f.write(f\"quiver_width      = {quiver_width}\\n\")\n",
    "    f.write(f\"scale_quiver      = {scale_quiver}\\n\")\n",
    "    f.write(f\"quiver_color      = '{quiver_color}'\\n\")\n",
    "    f.write(f\"max_arrows_across = {max_arrows_across}\\n\")\n",
    "    f.write(f\"max_arrows_vertical = {max_arrows_vertical}\\n\")\n",
    "    f.write(f\"min_vector_mag     = {min_vector_mag}\\n\")\n",
    "    \n",
    "# -----------------------\n",
    "# RUN THE SCRIPT\n",
    "# -----------------------\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", os.path.basename(script_name)],\n",
    "        cwd=script_folder,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    result.check_returncode()\n",
    "    print(\"STDOUT:\\n\", result.stdout)\n",
    "    print(\"plot_hdf5_vertical_cut completed successfully.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"ERROR: plot_hdf5_vertical_cut.py exited with code\", e.returncode)\n",
    "    print(\"---- STDOUT ----\")\n",
    "    print(e.stdout)\n",
    "    print(\"---- STDERR ----\")\n",
    "    print(e.stderr)\n",
    "    raise\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
