{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOHID Water\n",
    "\n",
    "This Jupyter Notebook aims to help implement and run the MOHID Water model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note 1**: Execute each cell through the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button from the top MENU (or keyboard shortcut `Shift` + `Enter`).<br>\n",
    "<br>\n",
    "**Note 2**: Use the Kernel and Cell menus to restart the kernel and clear outputs.<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "- [1. Import required libraries](#1.-Import-required-libraries)\n",
    "- [2. General options](#2.-General-options)\n",
    "    - [2.1 Set run case](#2.1-Set-run-case)\n",
    "    - [2.2 Load MOHID griddata](#2.2-Load-MOHID-griddata)\n",
    "    - [2.3 Define a bounding box](#2.3-Define-a-bounding-box)\n",
    "    - [2.4 Set dates](#2.4-Set-dates)\n",
    "- [3. Boundary Conditions](#3.-Boundary-Conditions)\n",
    "    - [3.1 Oceanic](#3.1-Oceanic)\n",
    "        - [3.1.1 Create Copernicus Marine credentials file](#3.1.1-Create-Copernicus-Marine-credentials-file)\n",
    "        - [3.1.2 Set CMEMS product](#3.1.2-Set-CMEMS-product)\n",
    "        - [3.1.3 Download CMEMS](#3.1.3-Download-CMEMS)\n",
    "    - [3.2 Meteorological](#3.2-Meteorological)\n",
    "        - [3.2.1 Setup the CDS API personal access token](#3.2.1-Setup-the-CDS-API-personal-access-token)\n",
    "        - [3.2.2 Download ERA5 Reanalysis](#3.2.2-Download-ERA5-Reanalysis)\n",
    "    - [3.3 Tide](#3.3-Tide)\n",
    "        - [3.3.1 Download FES2014.zip](#3.3.1-Download-FES2014.zip)\n",
    "        - [3.3.2 Crop FES2014.hdf5 to your grid area](#3.3.2-Crop-FES2014.hdf5-to-your-grid-area)\n",
    "        - [3.3.3 Plot a specific dataset (e.g., M2 amplitude)](#3.3.3-Plot-a-specific-dataset-(e.g.,-M2-amplitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copernicusmarine\n",
    "import cdsapi\n",
    "import os\n",
    "from ipyleaflet import Map, TileLayer, DrawControl, GeoJSON, Marker\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "from folium.plugins import MeasureControl\n",
    "import glob\n",
    "import zipfile\n",
    "import h5py\n",
    "import requests\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. General options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Set run case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"run_cases\" \n",
    "\n",
    "name = \"Coastal3D_Operational\"\n",
    "\n",
    "case_dir = (os.path.join(os.getcwd(),dirpath, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load MOHID griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grid data from file\n",
    "\n",
    "grid_data = \"Level1.dat\"\n",
    "file_path = (os.path.join(case_dir, \"GeneralData\\\\Batim\\\\\", grid_data))\n",
    "\n",
    "# Load a MOHID grid data file\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "grid_data = []\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "n_rows, n_cols = None, None  # Use None to detect missing values\n",
    "start_reading_xx = False\n",
    "start_reading_yy = False\n",
    "start_reading_grid = False\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()  # Remove leading and trailing spaces\n",
    "    parts = line.split()\n",
    "\n",
    "    if line.startswith(\"ILB_IUB\"):\n",
    "        n_rows = int(parts[3]) \n",
    "    elif line.startswith(\"JLB_JUB\"):\n",
    "        n_cols = int(parts[3])\n",
    "    elif line.startswith(\"ORIGIN\"):\n",
    "        x0 = float(parts[2])\n",
    "        y0 = float(parts[3])\n",
    "    elif line.startswith(\"DX\"):\n",
    "        dx = float(parts[2])\n",
    "    elif line.startswith(\"DY\"):\n",
    "        dy = float(parts[2])\n",
    "    elif \"<BeginXX>\" in line:\n",
    "        start_reading_xx = True\n",
    "        continue\n",
    "    elif \"<EndXX>\" in line:\n",
    "        start_reading_xx = False\n",
    "    elif start_reading_xx:\n",
    "        try:\n",
    "            x_coords.append(float(line))\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping invalid line -> {line}\")\n",
    "    elif \"<BeginYY>\" in line:\n",
    "        start_reading_yy = True\n",
    "        continue\n",
    "    elif \"<EndYY>\" in line:\n",
    "        start_reading_yy = False\n",
    "    elif start_reading_yy:\n",
    "        try:\n",
    "            y_coords.append(float(line))\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping invalid line -> {line}\")\n",
    "    elif \"<BeginGridData2D>\" in line:\n",
    "        start_reading_grid = True\n",
    "        continue\n",
    "    elif \"<EndGridData2D>\" in line:\n",
    "        start_reading_grid = False\n",
    "    elif start_reading_grid:\n",
    "        try:\n",
    "            grid_data.append(float(line))\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping invalid line -> {line}\")\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Extracted Dimensions: n_rows={n_rows}, n_cols={n_cols}\")\n",
    "print(f\"Grid Data Length: {len(grid_data)}\")\n",
    "\n",
    "if not x_coords:\n",
    "    x_coords = np.linspace(x0, x0 + dx * n_cols, n_cols+1)\n",
    "    y_coords = np.linspace(y0, y0 + dy * n_rows, n_rows+1)\n",
    "else:\n",
    "    x_coords = np.array(x_coords) + x0\n",
    "    y_coords = np.array(y_coords) + y0\n",
    "    \n",
    "# Ensure grid dimensions are valid\n",
    "if n_rows is None or n_cols is None:\n",
    "    raise ValueError(\"Grid dimensions could not be determined from the file.\")\n",
    "\n",
    "# Check if data size matches expected shape\n",
    "expected_size = n_rows * n_cols\n",
    "if len(grid_data) != expected_size:\n",
    "    raise ValueError(f\"Mismatch: Grid data size {len(grid_data)} does not match expected ({expected_size}).\")\n",
    "\n",
    "# Convert to NumPy array and reshape correctly\n",
    "zi = np.array(grid_data).reshape(n_rows, n_cols)\n",
    "\n",
    "x_grid, y_grid = np.meshgrid(x_coords, y_coords)\n",
    "    \n",
    "print(f\"Loaded grid data shape: {zi.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Define a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a bounding box based on MOHID grid for boundary conditions download\n",
    "np_x = np.array(x_grid)\n",
    "np_y = np.array(y_grid)\n",
    "\n",
    "c = 0.25 #degrees\n",
    "min_lon=np.min(np_x)-c\n",
    "min_lat=np.min(np_y)-c\n",
    "max_lon=np.max(np_x)+c \n",
    "max_lat=np.max(np_y)+c\n",
    "\n",
    "print(f\"✅ Polygon Bounds:\")\n",
    "print(f\"  - min_lon (West): {min_lon}\")\n",
    "print(f\"  - min_lat (South): {min_lat}\")\n",
    "print(f\"  - max_lon (East): {max_lon}\")\n",
    "print(f\"  - max_lat (North): {max_lat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Set dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dates for boundary conditions download\n",
    "\n",
    "#Define a 5-day interval if it´s the initial run for model warm-up\n",
    "start_date_str = \"2025-1-1\" #\"%Y-%m-%d\"\n",
    "end_date_str = \"2025-1-6\" #\"%Y-%m-%d\"\n",
    "\n",
    "daily = 0 #if daily = 1, one day per file, else just one file for the interval end_date - start_date.   \n",
    "\n",
    "forecast = 0 \n",
    "\n",
    "#The keywords below are only used if forecast = 1\n",
    "refday_to_start = 0 #0 is today, -1 yesterday, 1 tomorrow\n",
    "number_of_runs = 1 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boundary Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Oceanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Create Copernicus Marine credentials file\n",
    "#It has to be done only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The login command will check your Copernicus Marine credentials and create the configuration file. \n",
    "copernicusmarine.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Set CMEMS product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6-hourly instanataneous\n",
    "#product_id = [\"cmems_mod_glo_phy_anfc_0.083deg_PT6H-i\",\"cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i\",\"cmems_mod_glo_phy-so_anfc_0.083deg_PT6H-i\",\"cmems_mod_glo_phy-thetao_anfc_0.083deg_PT6H-i\"]\n",
    "\n",
    "#daily mean\n",
    "product_id = [\"cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m\",\"cmems_mod_glo_phy-so_anfc_0.083deg_P1D-m\",\"cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m\",\"cmems_mod_glo_phy_anfc_0.083deg_P1D-m\"]\n",
    "\n",
    "start_depth = 0.49402499198913574\n",
    "#end_depth = 0.49402499198913574\n",
    "end_depth = 5727.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Download CMEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "backup_path = (os.path.join(case_dir, r\"GeneralData\\\\BoundaryConditions\\\\CMEMS\\\\backup\"))\n",
    "\n",
    "#This file can later be used as input to CMEMS2HDF5.py for operational purposes\n",
    "input_file = os.path.join(os.getcwd(),\"work\",\"CMEMS\",\"Input_CMEMS2HDF5.py\")\n",
    "\n",
    "with open(input_file, 'w') as file:\n",
    "    file.write(f\"backup_path=r'{backup_path}'\\n\")\n",
    "    file.write(f\"daily={daily}\\n\")\n",
    "    file.write(f\"forecast={forecast}\\n\")\n",
    "    file.write(f\"number_of_runs={number_of_runs}\\n\")\n",
    "    file.write(f\"refday_to_start={refday_to_start}\\n\")\n",
    "    file.write(f\"product_id={product_id}\\n\")\n",
    "    file.write(f\"start_depth={start_depth}\\n\")\n",
    "    file.write(f\"end_depth={end_depth}\\n\")\n",
    "    file.write(f\"min_lon={min_lon}\\n\")\n",
    "    file.write(f\"max_lon={max_lon}\\n\")\n",
    "    file.write(f\"min_lat={min_lat}\\n\")\n",
    "    file.write(f\"max_lat={max_lat}\\n\")\n",
    "    file.write(f\"start_date_str='{start_date_str}'\\n\")\n",
    "    file.write(f\"end_date_str='{end_date_str}'\\n\")\n",
    "\n",
    "%cd work/CMEMS/\n",
    "%run CMEMS2HDF5.py\n",
    "\n",
    "# Return to the original directory\n",
    "%cd -\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Meteorological"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Setup the CDS API personal access token\n",
    "It has to be done only once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have an account yet, please register (https://cds.climate.copernicus.eu/).\n",
    "If you are not logged in, please login.\n",
    "Once logged in, copy the URL and key.\n",
    "\n",
    "Create a file named .cdsapirc in your home directory.\n",
    "\n",
    "$HOME/.cdsapirc (in your Unix/Linux environment)\n",
    "\n",
    "%USERPROFILE%\\.cdsapirc file (in your windows environment,%USERPROFILE% is usually located at C:\\Users\\Username folder). \n",
    "\n",
    "Paste the URL and key into .cdsapirc file.\n",
    "\n",
    "The CDS API expects to find the .cdsapirc file in your home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Download ERA5 Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_path = (os.path.join(case_dir, r\"GeneralData\\\\BoundaryConditions\\\\ERA5\\\\backup\"))\n",
    "\n",
    "#This file can later be used as input to ERA52HDF5.py for operational purposes\n",
    "input_file = os.path.join(os.getcwd(),\"work\",\"ERA5\",\"Input_ERA52HDF5.py\")\n",
    "\n",
    "with open(input_file, 'w') as file:\n",
    "    file.write(f\"backup_path=r'{backup_path}'\\n\")\n",
    "    file.write(f\"daily={daily}\\n\")\n",
    "    file.write(f\"forecast={forecast}\\n\")\n",
    "    file.write(f\"number_of_runs={number_of_runs}\\n\")\n",
    "    file.write(f\"refday_to_start={refday_to_start}\\n\")\n",
    "    file.write(f\"min_lon={min_lon}\\n\")\n",
    "    file.write(f\"max_lon={max_lon}\\n\")\n",
    "    file.write(f\"min_lat={min_lat}\\n\")\n",
    "    file.write(f\"max_lat={max_lat}\\n\")\n",
    "    file.write(f\"start_date_str='{start_date_str}'\\n\")\n",
    "    file.write(f\"end_date_str='{end_date_str}'\\n\")\n",
    "\n",
    "%cd work/ERA5/\n",
    "%run ERA52HDF5.py\n",
    "\n",
    "# Return to the original directory\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Download FES2014.zip\n",
    "It has to be done only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the ZIP file.\n",
    "url = \"http://www.mohid.com/PublicData/Products/Software/FES2014.zip\"\n",
    "\n",
    "# Define paths using pathlib for cross-platform compatibility.\n",
    "home_dir = pathlib.Path.home()\n",
    "local_zip_path = home_dir / \"FES2014.zip\"\n",
    "extract_dir = home_dir / \"FES2014\"\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(save_path, 'wb') as f, tqdm(\n",
    "            desc=\"Downloading\",\n",
    "            total=total_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024\n",
    "        ) as bar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "\n",
    "        print(\"Download completed successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Extraction completed. Files are available in '{extract_to}' directory.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: The downloaded file is not a valid ZIP archive.\")\n",
    "        exit(1)\n",
    "\n",
    "def verify_extraction(extract_to):\n",
    "    if not extract_to.exists() or not any(extract_to.iterdir()):\n",
    "        print(\"Error: Extraction failed or directory is empty.\")\n",
    "        exit(1)\n",
    "\n",
    "def cleanup(zip_path):\n",
    "    try:\n",
    "        zip_path.unlink()  # Deletes the ZIP file\n",
    "        print(f\"Deleted {zip_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting ZIP file: {e}\")\n",
    "\n",
    "# Run the process\n",
    "download_file(url, local_zip_path)\n",
    "\n",
    "# Ensure the extraction directory exists\n",
    "extract_dir.mkdir(exist_ok=True)\n",
    "\n",
    "extract_zip(local_zip_path, extract_dir)\n",
    "verify_extraction(extract_dir)\n",
    "cleanup(local_zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Crop FES2014.hdf5 to your grid area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide paths to your input (original) and output (cropped) HDF5 files.\n",
    "home_dir = pathlib.Path.home()\n",
    "input_file = home_dir / \"FES2014\" / 'FES2014.hdf5'\n",
    "output_file = os.path.join(case_dir, r\"GeneralData/BoundaryConditions/FES2014/FES2014.hdf5\")\n",
    "    \n",
    "def get_bbox_indices(lon_arr, lat_arr, min_lon, max_lon, min_lat, max_lat):\n",
    "    \"\"\"\n",
    "    Given 2D longitude and latitude arrays, compute the row and column indices\n",
    "    corresponding to the region defined by the geographic bounding box.\n",
    "    \n",
    "    Parameters:\n",
    "        lon_arr : 2D numpy array of longitudes.\n",
    "        lat_arr : 2D numpy array of latitudes.\n",
    "        min_lon, max_lon : float, desired longitude limits.\n",
    "        min_lat, max_lat : float, desired latitude limits.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (row_start, row_end, col_start, col_end) for slicing.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask selecting grid cells within the bounding box.\n",
    "    mask = (lon_arr >= min_lon) & (lon_arr <= max_lon) & \\\n",
    "           (lat_arr >= min_lat) & (lat_arr <= max_lat)\n",
    "    \n",
    "    if not np.any(mask):\n",
    "        raise ValueError(\"No grid points fall within the provided bounding box.\")\n",
    "    \n",
    "    # Determine the minimal spanning rectangle for the selected grid cells.\n",
    "    rows, cols = np.where(mask)\n",
    "    row_start = rows.min()\n",
    "    row_end = rows.max() + 1  # +1 because Python slices are half-open.\n",
    "    col_start = cols.min()\n",
    "    col_end = cols.max() + 1\n",
    "    \n",
    "    return row_start, row_end, col_start, col_end\n",
    "\n",
    "def create_dataset_with_attrs(out_group, key, data, source_ds):\n",
    "    \"\"\"\n",
    "    Create a new dataset in out_group with the provided data while preserving\n",
    "    the source dataset's compression, chunking, and attributes. If the original \n",
    "    chunk shape is larger than the new data dimensions, adjust the chunk shape to \n",
    "    fit the new data.\n",
    "    \n",
    "    Parameters:\n",
    "        out_group : HDF5 group (or file) where the new dataset will be created.\n",
    "        key       : The name for the new dataset.\n",
    "        data      : The data to be stored.\n",
    "        source_ds : The source dataset from which attributes and storage options are copied.\n",
    "    \"\"\"\n",
    "    ds_kwargs = {}\n",
    "    # Preserve compression if present.\n",
    "    if source_ds.compression:\n",
    "        ds_kwargs['compression'] = source_ds.compression\n",
    "        if source_ds.compression_opts is not None:\n",
    "            ds_kwargs['compression_opts'] = source_ds.compression_opts\n",
    "    # Preserve chunking but adjust so that chunks do not exceed the new data dimensions.\n",
    "    if source_ds.chunks:\n",
    "        # Recalculating the chunk shape so that each chunk dimension is at most the corresponding data dimension.\n",
    "        new_chunks = tuple(min(new_dim, orig_chunk) for new_dim, orig_chunk in zip(data.shape, source_ds.chunks))\n",
    "        ds_kwargs['chunks'] = new_chunks\n",
    "    # Create the new dataset with the options.\n",
    "    new_ds = out_group.create_dataset(key, data=data, **ds_kwargs)\n",
    "    # Copy all dataset attributes.\n",
    "    for attr_key, attr_val in source_ds.attrs.items():\n",
    "        new_ds.attrs[attr_key] = attr_val\n",
    "    return new_ds\n",
    "\n",
    "def recursive_copy(in_group, out_group, crop_slice, grid_shape):\n",
    "    # Extract the row and column start/end indices from crop_slice for clarity.\n",
    "    row_start, row_end = crop_slice[0].start, crop_slice[0].stop\n",
    "    col_start, col_end = crop_slice[1].start, crop_slice[1].stop\n",
    "\n",
    "    for key in in_group:\n",
    "        item = in_group[key]\n",
    "        if isinstance(item, h5py.Dataset):\n",
    "            data = item[()]\n",
    "            if data.ndim >= 2:\n",
    "                # Determine what the spatial dimensions of this dataset are.\n",
    "                data_grid_shape = data.shape[-2:]\n",
    "                if data_grid_shape == grid_shape:\n",
    "                    # Data has the same shape as the lat/lon arrays.\n",
    "                    slicing = (slice(None),) * (data.ndim - 2) + crop_slice\n",
    "                elif data_grid_shape == (grid_shape[0] - 1, grid_shape[1] - 1):\n",
    "                    # Data is defined on grid cells, which is one less in each dimension.\n",
    "                    adjusted_crop_slice = (slice(row_start, row_end - 1), slice(col_start, col_end - 1))\n",
    "                    slicing = (slice(None),) * (data.ndim - 2) + adjusted_crop_slice\n",
    "                else:\n",
    "                    # If the dataset doesn't match either expected shape, copy it unchanged.\n",
    "                    slicing = None\n",
    "\n",
    "                if slicing is not None:\n",
    "                    cropped = data[slicing]\n",
    "                    create_dataset_with_attrs(out_group, key, cropped, item)\n",
    "                else:\n",
    "                    create_dataset_with_attrs(out_group, key, data, item)\n",
    "            else:\n",
    "                create_dataset_with_attrs(out_group, key, data, item)\n",
    "        elif isinstance(item, h5py.Group):\n",
    "            # Recursively copy groups and their attributes.\n",
    "            new_group = out_group.create_group(key)\n",
    "            for attr_key, attr_val in item.attrs.items():\n",
    "                new_group.attrs[attr_key] = attr_val\n",
    "            recursive_copy(item, new_group, crop_slice, grid_shape)\n",
    "\n",
    "def crop_hdf5_file(input_file, output_file, min_lon, max_lon, min_lat, max_lat):\n",
    "    \"\"\"\n",
    "    Crop datasets within an HDF5 file that share a common grid, based on a geographical\n",
    "    bounding box specified by min/max longitudes and latitudes. Datasets whose last two\n",
    "    dimensions match the grid will be cropped.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file  : Path to the original HDF5 file.\n",
    "        output_file : Path to the new (cropped) HDF5 file.\n",
    "        min_lon, max_lon : Geographic longitude limits.\n",
    "        min_lat, max_lat : Geographic latitude limits.\n",
    "    \"\"\"\n",
    "    # First, determine the crop indices using the grid datasets.\n",
    "    with h5py.File(input_file, 'r') as fin:\n",
    "        grid = fin['Grid']  # assumes a 'grid' group exists.\n",
    "        lon_arr = grid['Longitude'][()]\n",
    "        lat_arr = grid['Latitude'][()]\n",
    "        grid_shape = lon_arr.shape\n",
    "        row_start, row_end, col_start, col_end = get_bbox_indices(\n",
    "            lon_arr, lat_arr, min_lon, max_lon, min_lat, max_lat\n",
    "        )\n",
    "        print(f\"Cropping rows: {row_start} to {row_end}, columns: {col_start} to {col_end}\")\n",
    "        crop_slice = (slice(row_start, row_end), slice(col_start, col_end))\n",
    "\n",
    "    # Now recursively copy and crop datasets as needed.\n",
    "    with h5py.File(input_file, 'r') as fin, h5py.File(output_file, 'w') as fout:\n",
    "        recursive_copy(fin, fout, crop_slice, grid_shape)\n",
    "    \n",
    "    print(f\"Cropping completed. Output saved to '{output_file}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Run the cropping function.\n",
    "    crop_hdf5_file(input_file, output_file, min_lon, max_lon, min_lat, max_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Plot a specific dataset (e.g., M2 amplitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the HDF5 file path and dataset key.\n",
    "file_path =  os.path.join(case_dir, \"GeneralData/BoundaryConditions/FES2014/FES2014.hdf5\")\n",
    "dataset_key = '/Results/water level/M2/amplitude'  # Update as needed.\n",
    "    \n",
    "def plot_dataset(file_path, dataset_key):\n",
    "    \"\"\"\n",
    "    Open an HDF5 file and plot the specified dataset with several enhancements:\n",
    "    \n",
    "    - Masks invalid data values below -99.\n",
    "    - Uses coordinate arrays (if available) for georeferenced plotting.\n",
    "    - Automatically obtains a satellite basemap from an online tile service (Esri World Imagery)\n",
    "      and overlays it as the background.\n",
    "    - Saves the output plot as an image file and displays it inline (suitable for a Jupyter Notebook).\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the HDF5 file.\n",
    "        dataset_key (str): The key/path of the dataset to plot.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f[dataset_key][()]\n",
    "        print(f\"Loaded dataset '{dataset_key}' with shape {data.shape}.\")\n",
    "\n",
    "        # Mask invalid values below -99.\n",
    "        data = np.ma.masked_where(data < -99, data)\n",
    "        print(\"Applied mask for values below -99.\")\n",
    "\n",
    "        # Construct an output filename based on the dataset key.\n",
    "        output_filename = f\"plot_{dataset_key.strip('/').replace('/', '_')}.png\"\n",
    "        print(f\"Output image filename will be: {output_filename}\")\n",
    "\n",
    "        # Attempt to load coordinate arrays for georeferencing.\n",
    "        try:\n",
    "            lon_arr = f[\"Grid/Longitude\"][()]\n",
    "            lat_arr = f[\"Grid/Latitude\"][()]\n",
    "            print(f\"Found coordinate arrays with shapes {lon_arr.shape} and {lat_arr.shape}.\")\n",
    "\n",
    "            # Check if the data shape matches the expected cell-centered dimensions.\n",
    "            if data.shape == (lon_arr.shape[0] - 1, lon_arr.shape[1] - 1):\n",
    "                # Set up the figure and axis.\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                \n",
    "                # Draw the data with pcolormesh.\n",
    "                mesh = ax.pcolormesh(lon_arr, lat_arr, data, shading='auto', cmap='viridis', zorder=2)\n",
    "                ax.set_xlabel(\"Longitude\")\n",
    "                ax.set_ylabel(\"Latitude\")\n",
    "                ax.set_title(f\"Plot of {dataset_key}\")\n",
    "                \n",
    "                # Automatically obtain and add a satellite basemap.\n",
    "                # Since our coordinates are in EPSG:4326, we pass that as the CRS.\n",
    "                # The Esri WorldImagery provider offers pretty good satellite images.\n",
    "                ctx.add_basemap(ax, crs=\"EPSG:4326\", source=ctx.providers.Esri.WorldImagery, alpha=1., zorder=1)\n",
    "                \n",
    "                fig.colorbar(mesh, ax=ax, label='Data values')\n",
    "                plt.savefig(output_filename)\n",
    "                print(f\"Saved plot to {output_filename}\")\n",
    "                plt.show()  # Shows the plot inline \n",
    "                plt.close()\n",
    "                return\n",
    "            else:\n",
    "                print(\"Data shape does not match expected cell-centered dimensions. Using fallback imshow.\")\n",
    "        except KeyError:\n",
    "            print(\"Coordinate arrays not found. Using imshow for plotting without basemap.\")\n",
    "\n",
    "        # Fallback: Plot using imshow when georeferenced coordinates are unavailable.\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        im = plt.imshow(data, origin='lower', cmap='viridis')\n",
    "        plt.xlabel(\"Column Index\")\n",
    "        plt.ylabel(\"Row Index\")\n",
    "        plt.title(f\"Plot of {dataset_key}\")\n",
    "        plt.colorbar(im, label='Data values')\n",
    "        plt.savefig(output_filename)\n",
    "        print(f\"Saved plot to {output_filename}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_dataset(file_path, dataset_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Draw markers on the map to define the source coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "LonGrid = np.array(x_grid)\n",
    "LatGrid = np.array(y_grid)\n",
    "min_lon, max_lon = LonGrid.min(), LonGrid.max()\n",
    "min_lat, max_lat = LatGrid.min(), LatGrid.max()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Create the map.\n",
    "m = Map(center=(LatGrid.mean(), LonGrid.mean()), zoom=8)\n",
    "marker = None  # For interactive marker\n",
    "\n",
    "# Store the block (batch) layers in a dictionary, keyed by (block_row, block_col).\n",
    "block_layers = {}\n",
    "\n",
    "# Set a block (batch) size. (This controls the spatial grouping.)\n",
    "block_size = 10  # Adjust as needed.\n",
    "\n",
    "# Precompute grid cell corners (for all cells).\n",
    "# These arrays are of shape (M-1, N-1) if LonGrid and LatGrid are shape (M, N).\n",
    "lon_sw = LonGrid[:-1, :-1]  # Southwest corner\n",
    "lon_se = LonGrid[:-1,  1:]  # Southeast corner\n",
    "lon_ne = LonGrid[1:,   1:]  # Northeast corner\n",
    "lon_nw = LonGrid[1:,  :-1]  # Northwest corner\n",
    "\n",
    "lat_sw = LatGrid[:-1, :-1]\n",
    "lat_se = LatGrid[:-1,  1:]\n",
    "lat_ne = LatGrid[1:,   1:]\n",
    "lat_nw = LatGrid[1:,  :-1]\n",
    "\n",
    "# -------------------------\n",
    "# For discrete (flat) color mapping, we’ll use globals for our binning.\n",
    "_nbins = 10\n",
    "_bins = None\n",
    "_discrete_colors = None\n",
    "\n",
    "def map_value_to_color(value):\n",
    "    if value == -99:\n",
    "        return \"#ffffff00\"  # Transparent for invalid cells.\n",
    "    # np.digitize returns indices in 1.._nbins+1; subtract 1 for 0-index.\n",
    "    bin_index = np.digitize(value, _bins) - 1\n",
    "    bin_index = int(np.clip(bin_index, 0, _nbins - 1))\n",
    "    return _discrete_colors[bin_index]\n",
    "\n",
    "def precompute_color_grid(zi, nbins=10):\n",
    "    \"\"\"\n",
    "    Precompute a flat color mapping for the depth grid zi.\n",
    "    Only valid cells (≠ -99) get a color from the viridis colormap.\n",
    "    Also sets the global _bins, _nbins, and _discrete_colors for later use.\n",
    "    \"\"\"\n",
    "    global _bins, _nbins, _discrete_colors\n",
    "    _nbins = nbins\n",
    "    valid = zi != -99\n",
    "    if np.any(valid):\n",
    "        valid_min = zi[valid].min()\n",
    "        valid_max = zi[valid].max()\n",
    "    else:\n",
    "        valid_min, valid_max = 0, 1\n",
    "    _bins = np.linspace(valid_min, valid_max, nbins + 1)\n",
    "    cmap = plt.colormaps.get_cmap('viridis')\n",
    "    _discrete_colors = [mcolors.to_hex(c) for c in cmap(np.linspace(0, 1, nbins))]\n",
    "    # Optionally, you can return a full color array:\n",
    "    vectorized_map = np.vectorize(map_value_to_color)\n",
    "    return vectorized_map(zi)\n",
    "\n",
    "# Precompute colors on the entire grid (used only for initial reference).\n",
    "color_mapped_zi = precompute_color_grid(zi, nbins=_nbins)\n",
    "\n",
    "# -------------------------\n",
    "# New: Generate a GeoJSON FeatureCollection for a given block (spatial batch).\n",
    "def generate_block_geojson(block_row, block_col, block_size):\n",
    "    \"\"\"\n",
    "    Create a GeoJSON FeatureCollection for cells within one block.\n",
    "    The block covers cell indices:\n",
    "         i from block_row*block_size to min((block_row+1)*block_size, total rows)\n",
    "         j from block_col*block_size to min((block_col+1)*block_size, total cols)\n",
    "    Only cells where zi != -99 are rendered.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    total_rows, total_cols = zi.shape\n",
    "    row_start = block_row * block_size\n",
    "    row_end = min((block_row + 1) * block_size, total_rows)\n",
    "    col_start = block_col * block_size\n",
    "    col_end = min((block_col + 1) * block_size, total_cols)\n",
    "    for i in range(row_start, row_end):\n",
    "        for j in range(col_start, col_end):\n",
    "            # Skip cell if it's invalid.\n",
    "            if zi[i, j] == -99:\n",
    "                continue\n",
    "            coordinates = [[\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])],  # SW\n",
    "                [float(lon_se[i, j]), float(lat_se[i, j])],  # SE\n",
    "                [float(lon_ne[i, j]), float(lat_ne[i, j])],  # NE\n",
    "                [float(lon_nw[i, j]), float(lat_nw[i, j])],  # NW\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])]   # Close ring\n",
    "            ]]\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": coordinates\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"fill\": map_value_to_color(zi[i, j]),\n",
    "                    \"stroke\": \"#000000\",\n",
    "                    \"fill-opacity\": 0.5,\n",
    "                    \"stroke-width\": 0.2,\n",
    "                    \"i\": i,\n",
    "                    \"j\": j\n",
    "                }\n",
    "            }\n",
    "            features.append(feature)\n",
    "    return {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "# -------------------------\n",
    "# Update (or initially generate) all blocks.\n",
    "def update_all_blocks():\n",
    "    global block_layers\n",
    "    # Remove any existing block layers.\n",
    "    for layer in block_layers.values():\n",
    "        m.remove_layer(layer)\n",
    "    block_layers = {}\n",
    "    \n",
    "    total_rows, total_cols = zi.shape\n",
    "    n_block_rows = (total_rows + block_size - 1) // block_size\n",
    "    n_block_cols = (total_cols + block_size - 1) // block_size\n",
    "    \n",
    "    for br in range(n_block_rows):\n",
    "        for bc in range(n_block_cols):\n",
    "            fc = generate_block_geojson(br, bc, block_size)\n",
    "            # Only add a block if it contains any features.\n",
    "            if fc[\"features\"]:\n",
    "                layer = GeoJSON(\n",
    "                    data=fc,\n",
    "                    style_callback=lambda feature: {\n",
    "                        \"fillColor\": feature[\"properties\"][\"fill\"],\n",
    "                        \"color\": feature[\"properties\"][\"stroke\"],\n",
    "                        \"weight\": feature[\"properties\"][\"stroke-width\"],\n",
    "                        \"fillOpacity\": feature[\"properties\"][\"fill-opacity\"],\n",
    "                    }\n",
    "                )\n",
    "                m.add_layer(layer)\n",
    "                block_layers[(br, bc)] = layer\n",
    "\n",
    "# For initial rendering, generate all blocks.\n",
    "update_all_blocks()\n",
    "print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "#display(m)\n",
    "\n",
    "# -------------------------------\n",
    "# Global store for drawn markers\n",
    "# -------------------------------\n",
    "# Dictionary mapping marker_id -> {'location': [lon, lat], 'name': str}\n",
    "markers_dict = {}\n",
    "marker_counter = 0  # Unique marker counter\n",
    "\n",
    "def ask_marker_name(marker, marker_id):\n",
    "    \"\"\"\n",
    "    Display a text input widget to ask for a marker name.\n",
    "    When confirmed, assign the name to the marker and update the global dictionary.\n",
    "    \"\"\"\n",
    "    name_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter source name',\n",
    "        description='Source name:',\n",
    "        disabled=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    confirm_button = widgets.Button(\n",
    "        description='Confirm',\n",
    "        disabled=False,\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        marker_name = name_input.value.strip()\n",
    "        if not marker_name:\n",
    "            marker_name = f\"Marker {marker_id}\"\n",
    "        # Attach the marker name\n",
    "        marker.marker_name = marker_name\n",
    "        # Update the marker entry with the name\n",
    "        markers_dict[marker_id]['name'] = marker_name\n",
    "        print(f\"Marker {marker_id} named '{marker_name}'\")\n",
    "        widget_box.close()  # Close the widget after confirmation\n",
    "        \n",
    "    confirm_button.on_click(on_button_clicked)\n",
    "    widget_box = widgets.VBox([name_input, confirm_button])\n",
    "    display(widget_box)\n",
    "\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global marker_counter, markers_dict\n",
    "    \n",
    "    if action == \"created\":\n",
    "        coords = geo_json[\"geometry\"][\"coordinates\"]  # [lon, lat]\n",
    "        marker_id = marker_counter  # Assign an ID\n",
    "        marker_counter += 1\n",
    "        # Store with location and placeholder for marker name\n",
    "        markers_dict[marker_id] = {'location': [coords[1], coords[0]], 'name': None}\n",
    "    \n",
    "        # Create a draggable marker\n",
    "        custom_marker = Marker(location=[coords[1], coords[0]], draggable=True)\n",
    "        custom_marker.marker_id = marker_id  # Attach marker ID\n",
    "    \n",
    "        # Ask user to enter a name for the new marker\n",
    "        ask_marker_name(custom_marker, marker_id)\n",
    "    \n",
    "        # Function to update stored marker coordinates when moved\n",
    "        def on_location_change(change, m_id=marker_id):\n",
    "            new_location = change[\"new\"]  # New [lat, lon]\n",
    "            markers_dict[m_id]['location'] = [new_location[1], new_location[0]]\n",
    "            print(f\"Marker {m_id} moved to: {markers_dict[m_id]['location']}\")\n",
    "    \n",
    "        # Observe marker movement\n",
    "        custom_marker.observe(on_location_change, names=\"location\")\n",
    "    \n",
    "        # Add the new draggable marker to the map\n",
    "        m.add_layer(custom_marker)\n",
    "        \n",
    "        # Remove the default marker added by DrawControl\n",
    "        for layer in list(m.layers):\n",
    "            if isinstance(layer, GeoJSON) and layer.data.get(\"geometry\", {}).get(\"type\", \"\") == \"Point\":\n",
    "                m.remove_layer(layer)\n",
    "    \n",
    "    elif action == \"edited\":\n",
    "        print(\"Edit event received; marker updates are handled via the location observer.\")\n",
    "    \n",
    "    elif action == \"deleted\":\n",
    "        features = geo_json.get(\"features\", [])\n",
    "        if not features:\n",
    "            features = [geo_json]\n",
    "        for feature in features:\n",
    "            marker_id = feature.get(\"properties\", {}).get(\"marker_id\")\n",
    "            if marker_id is not None and marker_id in markers_dict:\n",
    "                print(f\"Deleted marker {marker_id} named '{markers_dict[marker_id]['name']}' at {markers_dict[marker_id]['location']}\")\n",
    "                markers_dict.pop(marker_id)\n",
    "        print(\"Current markers after deletion:\", markers_dict)\n",
    "\n",
    "draw_control = DrawControl()\n",
    "# For drawing tools we don't need, use empty dictionaries.\n",
    "draw_control.polygon   = {}\n",
    "draw_control.polyline  = {}\n",
    "draw_control.rectangle = {}\n",
    "draw_control.circle    = {}\n",
    "# Enable marker drawing button by assigning a non-empty dictionary.\n",
    "draw_control.marker    = {\"repeatMode\": False}\n",
    "\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# -------------------------------\n",
    "# Display the map \n",
    "# -------------------------------\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup MOHID Water input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(case_dir,\"Level_1/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = 0 #if initial run continuous=0, else continuous=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_date(file_name, next_start_date, next_end_date):\n",
    "    # Read all lines from the file\n",
    "    with open(file_name, 'r') as file:\n",
    "        file_lines = file.readlines()\n",
    "\n",
    "    # Patterns that capture formatting groups:\n",
    "    # This pattern assumes your lines are like:\n",
    "    # [optional leading spaces]START[spaces][:][spaces][the rest...]\n",
    "    pattern_start = re.compile(r\"^(?P<leading>\\s*START)(?P<sep>\\s*:\\s*).*$\")\n",
    "    pattern_end   = re.compile(r\"^(?P<leading>\\s*END)(?P<sep>\\s*:\\s*).*$\")\n",
    "    \n",
    "    for i, line in enumerate(file_lines):\n",
    "        # Match START and preserve groups: the keyword and the spacing around the colon.\n",
    "        m = pattern_start.match(line)\n",
    "        if m:\n",
    "            # The new content uses the original captured keyword and separators.\n",
    "            # Dte format\"YYYY MM DD \" followed by \"0 0 0\".\n",
    "            new_line = f\"{m.group('leading')}{m.group('sep')}{next_start_date.strftime('%Y %m %d ')}0 0 0\\n\"\n",
    "            file_lines[i] = new_line\n",
    "            continue\n",
    "\n",
    "        # Similarly for END lines.\n",
    "        m = pattern_end.match(line)\n",
    "        if m:\n",
    "            new_line = f\"{m.group('leading')}{m.group('sep')}{next_end_date.strftime('%Y %m %d ')}0 0 0\\n\"\n",
    "            file_lines[i] = new_line\n",
    "            continue\n",
    "\n",
    "    # Write the updated lines back to the file.\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.writelines(file_lines)\n",
    "\n",
    "file_name = os.path.join(data_dir, \"Model_1.dat\")\n",
    "write_date(file_name, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run MOHID Water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
