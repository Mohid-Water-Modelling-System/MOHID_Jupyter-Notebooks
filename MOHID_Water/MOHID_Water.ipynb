{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOHID Water\n",
    "\n",
    "This Jupyter Notebook aims to help implement and run the MOHID Water model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note 1**: Execute each cell through the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button from the top MENU (or keyboard shortcut `Shift` + `Enter`).<br>\n",
    "<br>\n",
    "**Note 2**: Use the Kernel and Cell menus to restart the kernel and clear outputs.<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "- [1. Import required libraries](#1.-Import-required-libraries)\n",
    "- [2. General options](#2.-General-options)\n",
    "    - [2.1 Set run case](#2.1-Set-run-case)\n",
    "    - [2.2 Load MOHID griddata](#2.2-Load-MOHID-griddata)\n",
    "    - [2.3 Define a bounding box](#2.3-Define-a-bounding-box)\n",
    "- [3. Boundary Conditions](#3.-Boundary-Conditions)\n",
    "    - [3.1 Oceanic](#3.1-Oceanic)\n",
    "        - [3.1.1 Create Copernicus Marine credentials file](#3.1.1-Create-Copernicus-Marine-credentials-file)\n",
    "        - [3.1.2 Set CMEMS product](#3.1.2-Set-CMEMS-product)\n",
    "        - [3.1.3 Download CMEMS](#3.1.3-Download-CMEMS)\n",
    "        - [3.1.4 Convert to HDF5](#3.1.4-Convert-to-HDF5)\n",
    "        - [3.1.5 Merge HDF5 files](#3.1.5-Merge-HDF5-files)\n",
    "    - [3.2 Meteorological](#3.2-Meteorological)\n",
    "        - [3.2.1 Download ERA5 Reanalysis](#3.2.1-Download-ERA5-Reanalysis)\n",
    "        - [3.2.2 Convert to HDF5](#3.2.2-Convert-to-HDF5)\n",
    "        - [3.2.3 Merge HDF5 files](#3.2.3-Merge-HDF5-files)\n",
    "    - [3.3 Tide](#3.3-Tide)\n",
    "        - [3.3.1 Download FES2014.zip](#3.3.1-Download-FES2014.zip)\n",
    "        - [3.3.2 Crop FES2014.hdf5 to your grid area](#3.3.2-Crop-FES2014.hdf5-to-your-grid-area)\n",
    "        - [3.3.3 Plot a specific dataset (e.g., M2 amplitude)](#3.3.3-Plot-a-specific-dataset-(e.g.,-M2-amplitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copernicusmarine\n",
    "import os\n",
    "from ipyleaflet import Map, TileLayer, DrawControl, GeoJSON, Marker\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import matplotlib as mpl\n",
    "from folium.plugins import MeasureControl\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. General options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Set run case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Coastal3D_Operational\"\n",
    "\n",
    "dirpath = \"run_cases\" \n",
    "\n",
    "# Construct the path and change the working directory\n",
    "os.chdir(os.path.join(dirpath, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load MOHID griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grid data from file\n",
    "file_path = r\"GeneralData\\Batim\\Level1.dat\"\n",
    "#file_path = \"mohid_griddata.dat\"\n",
    "# Load a MOHID grid data file\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "grid_data = []\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "n_rows, n_cols = None, None  # Use None to detect missing values\n",
    "start_reading_xx = False\n",
    "start_reading_yy = False\n",
    "start_reading_grid = False\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()  # Remove leading and trailing spaces\n",
    "    parts = line.split()\n",
    "\n",
    "    if line.startswith(\"ILB_IUB\"):\n",
    "        n_rows = int(parts[3]) \n",
    "    elif line.startswith(\"JLB_JUB\"):\n",
    "        n_cols = int(parts[3])\n",
    "    elif line.startswith(\"ORIGIN\"):\n",
    "        x0 = float(parts[2])\n",
    "        y0 = float(parts[3])\n",
    "    elif line.startswith(\"DX\"):\n",
    "        dx = float(parts[2])\n",
    "    elif line.startswith(\"DY\"):\n",
    "        dy = float(parts[2])\n",
    "    elif \"<BeginXX>\" in line:\n",
    "        start_reading_xx = True\n",
    "        continue\n",
    "    elif \"<EndXX>\" in line:\n",
    "        start_reading_xx = False\n",
    "    elif start_reading_xx:\n",
    "        try:\n",
    "            x_coords.append(float(line))\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping invalid line -> {line}\")\n",
    "    elif \"<BeginYY>\" in line:\n",
    "        start_reading_yy = True\n",
    "        continue\n",
    "    elif \"<EndYY>\" in line:\n",
    "        start_reading_yy = False\n",
    "    elif start_reading_yy:\n",
    "        try:\n",
    "            y_coords.append(float(line))\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping invalid line -> {line}\")\n",
    "    elif \"<BeginGridData2D>\" in line:\n",
    "        start_reading_grid = True\n",
    "        continue\n",
    "    elif \"<EndGridData2D>\" in line:\n",
    "        start_reading_grid = False\n",
    "    elif start_reading_grid:\n",
    "        try:\n",
    "            grid_data.append(float(line))\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping invalid line -> {line}\")\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Extracted Dimensions: n_rows={n_rows}, n_cols={n_cols}\")\n",
    "print(f\"Grid Data Length: {len(grid_data)}\")\n",
    "\n",
    "if not x_coords:\n",
    "    x_coords = np.linspace(x0, x0 + dx * n_cols, n_cols+1)\n",
    "    y_coords = np.linspace(y0, y0 + dy * n_rows, n_rows+1)\n",
    "else:\n",
    "    x_coords = np.array(x_coords) + x0\n",
    "    y_coords = np.array(y_coords) + y0\n",
    "    \n",
    "# Ensure grid dimensions are valid\n",
    "if n_rows is None or n_cols is None:\n",
    "    raise ValueError(\"Grid dimensions could not be determined from the file.\")\n",
    "\n",
    "# Check if data size matches expected shape\n",
    "expected_size = n_rows * n_cols\n",
    "if len(grid_data) != expected_size:\n",
    "    raise ValueError(f\"Mismatch: Grid data size {len(grid_data)} does not match expected ({expected_size}).\")\n",
    "\n",
    "# Convert to NumPy array and reshape correctly\n",
    "zi = np.array(grid_data).reshape(n_rows, n_cols)\n",
    "\n",
    "x_grid, y_grid = np.meshgrid(x_coords, y_coords)\n",
    "    \n",
    "print(f\"Loaded grid data shape: {zi.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Define a bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a bounding box based on MOHID grid for boundary conditions download\n",
    "np_x = np.array(x_grid)\n",
    "np_y = np.array(y_grid)\n",
    "\n",
    "c = 0.25 #degrees\n",
    "min_lon=np.min(np_x)-c\n",
    "min_lat=np.min(np_y)-c\n",
    "max_lon=np.max(np_x)+c \n",
    "max_lat=np.max(np_y)+c\n",
    "\n",
    "print(f\"âœ… Polygon Bounds:\")\n",
    "print(f\"  - min_lon (West): {min_lon}\")\n",
    "print(f\"  - min_lat (South): {min_lat}\")\n",
    "print(f\"  - max_lon (East): {max_lon}\")\n",
    "print(f\"  - max_lat (North): {max_lat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set dates for boundary conditions download and model run \n",
    "start_date = datetime.date(2025,1,1)\n",
    "end_date = datetime.date(2025,1,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Oceanic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Create Copernicus Marine credentials file\n",
    "#It has to be done only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The login command will check your Copernicus Marine credentials and create the configuration file. \n",
    "copernicusmarine.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Set CMEMS product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6-hourly instanataneous\n",
    "#product_id = [\"cmems_mod_glo_phy_anfc_0.083deg_PT6H-i\",\"cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i\",\"cmems_mod_glo_phy-so_anfc_0.083deg_PT6H-i\",\"cmems_mod_glo_phy-thetao_anfc_0.083deg_PT6H-i\"]\n",
    "\n",
    "#daily mean\n",
    "product_id = [\"cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m\",\"cmems_mod_glo_phy-so_anfc_0.083deg_P1D-m\",\"cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m\",\"cmems_mod_glo_phy_anfc_0.083deg_P1D-m\"]\n",
    "\n",
    "start_depth = 0.49402499198913574\n",
    "end_depth = 5727.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Download CMEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import copernicusmarine  # Ensure that your Copernicus Marine Service module is properly imported\n",
    "\n",
    "cmems_dir = os.path.join(os.getcwd(),\"GeneralData/BoundaryConditions/CMEMS\")\n",
    "ConvertToHdf5_dir = os.path.join(cmems_dir, \"ConvertToHdf5\")\n",
    "\n",
    "hdf_files = glob.iglob(os.path.join(ConvertToHdf5_dir,\"*.hdf*\"))\n",
    "nc_files = glob.iglob(os.path.join(ConvertToHdf5_dir,\"*.nc\"))\n",
    "\n",
    "for filename in hdf_files:\n",
    "    os.remove(filename)\n",
    "\n",
    "for filename in nc_files:\n",
    "    os.remove(filename)\n",
    "            \n",
    "# Mapping each product into its specific subset parameters:\n",
    "product_parameters = {\n",
    "    \"cmems_mod_glo_phy_anfc_0.083deg_PT6H-i\": {'variables': ['zos'], 'filename': \"CMEMS_zos.nc\"},\n",
    "    \"cmems_mod_glo_phy_anfc_0.083deg_P1D-m\": {'variables': ['zos'], 'filename': \"CMEMS_zos.nc\"},\n",
    "    \"cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i\": {'variables': ['uo', 'vo'], 'filename': \"CMEMS_cur.nc\"},\n",
    "    \"cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m\": {'variables': ['uo', 'vo'], 'filename': \"CMEMS_cur.nc\"},\n",
    "    \"cmems_mod_glo_phy-so_anfc_0.083deg_PT6H-i\": {'variables': ['so'], 'filename': \"CMEMS_so.nc\"},\n",
    "    \"cmems_mod_glo_phy-so_anfc_0.083deg_P1D-m\": {'variables': ['so'], 'filename': \"CMEMS_so.nc\"},\n",
    "    \"cmems_mod_glo_phy-thetao_anfc_0.083deg_PT6H-i\": {'variables': ['thetao'], 'filename': \"CMEMS_thetao.nc\"},\n",
    "    \"cmems_mod_glo_phy-thetao_anfc_0.083deg_P1D-m\": {'variables': ['thetao'], 'filename': \"CMEMS_thetao.nc\"}\n",
    "}\n",
    "\n",
    "\n",
    "def download_file(product, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Download a data file for a given product between start_date and end_date.\n",
    "    \n",
    "    The start_date is adjusted by subtracting one day, which is a domain-specific\n",
    "    requirement to ensure compatibility with Mohid.\n",
    "    \n",
    "    Parameters:\n",
    "        product (str): The product identifier from Copernicus Marine Service.\n",
    "        start_date (datetime): The original start date.\n",
    "        end_date (datetime): The end date.\n",
    "    \"\"\"\n",
    "    # Subtract one day from the start date.\n",
    "    adj_start_date = start_date - datetime.timedelta(days=1)\n",
    "    \n",
    "    # Retrieve mapping parameters for the product.\n",
    "    if product not in product_parameters:\n",
    "        print(f\"Product {product} not recognized. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    params = product_parameters[product]\n",
    "    variable = params['variables']\n",
    "    output_file_name = params['filename']\n",
    "\n",
    "    print(f\"Downloading: {output_file_name} for {adj_start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Call the subset function and handle potential exceptions.\n",
    "    try:\n",
    "        copernicusmarine.subset(\n",
    "            dataset_id=product,\n",
    "            minimum_longitude=min_lon,\n",
    "            maximum_longitude=max_lon,\n",
    "            minimum_latitude=min_lat,\n",
    "            maximum_latitude=max_lat,\n",
    "            minimum_depth=start_depth,\n",
    "            maximum_depth=end_depth,\n",
    "            start_datetime=adj_start_date.strftime('%Y-%m-%d') + ' 00:00:00',\n",
    "            end_datetime=end_date.strftime('%Y-%m-%d') + ' 00:00:00',\n",
    "            variables=variable,\n",
    "            output_directory=ConvertToHdf5_dir,\n",
    "            output_filename=output_file_name,\n",
    "            netcdf3_compatible = True\n",
    "        )\n",
    "        print(f\"Download successful: {output_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {output_file_name}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure the output directory exists.\n",
    "    if not os.path.exists(cmems_dir):\n",
    "        os.makedirs(cmems_dir)\n",
    "\n",
    "    # Process each product.\n",
    "    for product in product_id:\n",
    "        download_file(product, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Convert to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "\n",
    "ConvertToHdf5_dir = os.path.join(cmems_dir, \"ConvertToHdf5\")\n",
    "\n",
    "# List of input data files for conversion.\n",
    "input_convert_file_names = [\n",
    "    \"ConvertToHDF5Action_zos.dat\",\n",
    "    \"ConvertToHDF5Action_cur.dat\",\n",
    "    \"ConvertToHDF5Action_so.dat\",\n",
    "    \"ConvertToHDF5Action_thetao.dat\"\n",
    "]\n",
    "\n",
    "# Loop over each file in the list.\n",
    "for file_name in input_convert_file_names:\n",
    "    # Construct the full path to the input file.\n",
    "    src_file = os.path.join(ConvertToHdf5_dir, file_name)\n",
    "    # Define the destination file, which the conversion program expects.\n",
    "    dst_file = os.path.join(ConvertToHdf5_dir, \"ConvertToHDF5Action.dat\")\n",
    "    \n",
    "    try:\n",
    "        # Copy the input file to the destination file.\n",
    "        shutil.copy(src_file, dst_file)\n",
    "        #print(f\"Copied {src_file} to {dst_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {src_file} to {dst_file}: {e}\")\n",
    "        continue  # If there's an error with this file, skip to the next iteration\n",
    "    \n",
    "    try:\n",
    "        # Define the executable path.\n",
    "        exe_path = os.path.join(ConvertToHdf5_dir, \"ConvertToHDF5.exe\")\n",
    "        print(f\"Executing {file_name}...\")\n",
    "        # Execute the conversion process.\n",
    "        result = subprocess.run([exe_path], capture_output=True, text=True, cwd=ConvertToHdf5_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Merge HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "# Define directories and file paths\n",
    "cmems_dir = os.path.join(os.getcwd(), \"GeneralData/BoundaryConditions/CMEMS\")\n",
    "convert_to_hdf5_dir = os.path.join(cmems_dir, \"ConvertToHdf5\")\n",
    "file1 = os.path.join(convert_to_hdf5_dir, \"CMEMS_cur.hdf5\")\n",
    "file2 = os.path.join(convert_to_hdf5_dir, \"CMEMS_so.hdf5\")\n",
    "file3 = os.path.join(convert_to_hdf5_dir, \"CMEMS_zos.hdf5\")\n",
    "file4 = os.path.join(convert_to_hdf5_dir, \"CMEMS_thetao.hdf5\")\n",
    "merged_file = os.path.join(cmems_dir, \"CMEMS.hdf5\")\n",
    "\n",
    "# List of HDF5 files to merge\n",
    "files_to_merge = [file1, file2, file3, file4]\n",
    "\n",
    "with h5py.File(merged_file, \"w\") as out_file:\n",
    "    # Iterate through each file to copy and merge groups\n",
    "    for file_name in files_to_merge:\n",
    "        with h5py.File(file_name, \"r\") as in_file:\n",
    "            # Iterate over all top-level groups in the current file\n",
    "            for top_group in in_file.keys():\n",
    "                if top_group == \"Results\":\n",
    "                    # Special handling for \"Results\": merge subgroups individually.\n",
    "                    out_results = out_file.require_group(\"Results\")\n",
    "                    in_results = in_file[\"Results\"]\n",
    "                    for subgroup in in_results:\n",
    "                        if subgroup not in out_results:\n",
    "                            # Copy the subgroup to the merged file under \"Results\"\n",
    "                            in_file.copy(in_results[subgroup], out_results, subgroup)\n",
    "                        #else:\n",
    "                            #print(f\"Subgroup '{subgroup}' under 'Results' already exists. Skipping duplicate.\")\n",
    "                else:\n",
    "                    # For any other top-level group, check if it already exists\n",
    "                    if top_group not in out_file:\n",
    "                        in_file.copy(in_file[top_group], out_file, top_group)\n",
    "                    #else:\n",
    "                        #print(f\"Top-level group '{top_group}' already exists. Skipping duplicate.\")\n",
    "\n",
    "print(\"Merging complete while maintaining all groups!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Meteorological"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Setup the CDS API personal access token\n",
    "It has to be done only once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have an account yet, please register (https://cds.climate.copernicus.eu/).\n",
    "If you are not logged in, please login.\n",
    "Once logged in, copy the URL and key.\n",
    "\n",
    "Create a file named .cdsapirc in your home directory.\n",
    "\n",
    "$HOME/.cdsapirc (in your Unix/Linux environment)\n",
    "%USERPROFILE%\\.cdsapirc file (in your windows environment, %USERPROFILE% is usually located at C:\\Users\\Username folder). \n",
    "\n",
    "Paste the URL and key into .cdsapirc file.\n",
    "\n",
    "The CDS API expects to find the .cdsapirc file in your home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Download ERA5 Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import zipfile\n",
    "\n",
    "era5_dir = os.path.join(os.getcwd(),\"GeneralData/BoundaryConditions/ERA5\")\n",
    "ConvertToHdf5_dir = os.path.join(era5_dir, \"ConvertToHdf5\")\n",
    "target = os.path.join(ConvertToHdf5_dir, \"ERA5.zip\")\n",
    "\n",
    "# Extract days across months correctly\n",
    "days = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    days.append(str(current_date.day))\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "    \n",
    "dataset = \"reanalysis-era5-single-levels\"\n",
    "request = {\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"variable\": [\n",
    "                '10m_u_component_of_wind', \n",
    "                '10m_v_component_of_wind', \n",
    "                '2m_dewpoint_temperature',\n",
    "                '2m_temperature', \n",
    "                'boundary_layer_height', \n",
    "                'forecast_albedo',\n",
    "                'mean_sea_level_pressure',\n",
    "                'mean_surface_downward_short_wave_radiation_flux', \n",
    "                'surface_thermal_radiation_downwards', \n",
    "                'total_cloud_cover',\n",
    "                'total_precipitation',\n",
    "    ],\n",
    "    \"year\": sorted([str(start_date.year), str(end_date.year)]),\n",
    "    \"month\": sorted([str(start_date.month), str(end_date.month)]),\n",
    "    \"day\": days,\n",
    "    \"time\": [\n",
    "        \"00:00\", \"01:00\", \"02:00\",\n",
    "        \"03:00\", \"04:00\", \"05:00\",\n",
    "        \"06:00\", \"07:00\", \"08:00\",\n",
    "        \"09:00\", \"10:00\", \"11:00\",\n",
    "        \"12:00\", \"13:00\", \"14:00\",\n",
    "        \"15:00\", \"16:00\", \"17:00\",\n",
    "        \"18:00\", \"19:00\", \"20:00\",\n",
    "        \"21:00\", \"22:00\", \"23:00\"\n",
    "    ],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"zip\",\n",
    "    \"area\": [max_lat, min_lon, min_lat, max_lon]\n",
    "}\n",
    "\n",
    "#target = 'ERA5.zip'\n",
    "client = cdsapi.Client()\n",
    "\n",
    "try:\n",
    "    client.retrieve(dataset, request, target)\n",
    "    print(\"Download completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "# Open and extract the zip file\n",
    "with zipfile.ZipFile(target, 'r') as zip_ref:\n",
    "    zip_ref.extractall(ConvertToHdf5_dir)\n",
    "\n",
    "print(f\"Files extracted to {ConvertToHdf5_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Convert to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# List of input data files for conversion.\n",
    "input_convert_file_names = [\n",
    "    \"ConvertToHDF5Action_instant.dat\",\n",
    "    \"ConvertToHDF5Action_accum.dat\",\n",
    "    \"ConvertToHDF5Action_avg.dat\"\n",
    "]\n",
    "\n",
    "# Loop over each file in the list.\n",
    "for file_name in input_convert_file_names:\n",
    "    # Construct the full path to the input file.\n",
    "    src_file = os.path.join(ConvertToHdf5_dir, file_name)\n",
    "    # Define the destination file, which the conversion program expects.\n",
    "    dst_file = os.path.join(ConvertToHdf5_dir, \"ConvertToHDF5Action.dat\")\n",
    "    \n",
    "    try:\n",
    "        # Copy the input file to the destination file.\n",
    "        shutil.copy(src_file, dst_file)\n",
    "        #print(f\"Copied {src_file} to {dst_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {src_file} to {dst_file}: {e}\")\n",
    "        continue  # If there's an error with this file, skip to the next iteration\n",
    "    \n",
    "    try:\n",
    "        # Define the executable path.\n",
    "        exe_path = os.path.join(ConvertToHdf5_dir, \"ConvertToHDF5.exe\")\n",
    "        print(f\"Executing {file_name}...\")\n",
    "        # Execute the conversion process.\n",
    "        result = subprocess.run([exe_path], capture_output=True, text=True, cwd=ConvertToHdf5_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Merge HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "# Define directories and file paths\n",
    "era5_dir = os.path.join(os.getcwd(), \"GeneralData/BoundaryConditions/ERA5\")\n",
    "convert_to_hdf5_dir = os.path.join(era5_dir, \"ConvertToHdf5\")\n",
    "file1 = os.path.join(convert_to_hdf5_dir, \"Meteo_instant.hdf5\")\n",
    "file2 = os.path.join(convert_to_hdf5_dir, \"Meteo_accum.hdf5\")\n",
    "file3 = os.path.join(convert_to_hdf5_dir, \"Meteo_avg.hdf5\")\n",
    "merged_file = os.path.join(era5_dir, \"Meteo.hdf5\")\n",
    "\n",
    "# List of HDF5 files to merge\n",
    "files_to_merge = [file1, file2, file3]\n",
    "\n",
    "with h5py.File(merged_file, \"w\") as out_file:\n",
    "    # Iterate through each file to copy and merge groups\n",
    "    for file_name in files_to_merge:\n",
    "        with h5py.File(file_name, \"r\") as in_file:\n",
    "            # Iterate over all top-level groups in the current file\n",
    "            for top_group in in_file.keys():\n",
    "                if top_group == \"Results\":\n",
    "                    # Special handling for \"Results\": merge subgroups individually.\n",
    "                    out_results = out_file.require_group(\"Results\")\n",
    "                    in_results = in_file[\"Results\"]\n",
    "                    for subgroup in in_results:\n",
    "                        if subgroup not in out_results:\n",
    "                            # Copy the subgroup to the merged file under \"Results\"\n",
    "                            in_file.copy(in_results[subgroup], out_results, subgroup)\n",
    "                        #else:\n",
    "                            #print(f\"Subgroup '{subgroup}' under 'Results' already exists. Skipping duplicate.\")\n",
    "                else:\n",
    "                    # For any other top-level group, check if it already exists\n",
    "                    if top_group not in out_file:\n",
    "                        in_file.copy(in_file[top_group], out_file, top_group)\n",
    "                    #else:\n",
    "                        #print(f\"Top-level group '{top_group}' already exists. Skipping duplicate.\")\n",
    "\n",
    "print(\"Merging complete while maintaining all groups!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Download FES2014.zip\n",
    "It has to be done only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# URL of the ZIP file.\n",
    "url = \"http://www.mohid.com/PublicData/Products/Software/FES2014.zip\"\n",
    "\n",
    "# Define paths using pathlib for cross-platform compatibility.\n",
    "home_dir = pathlib.Path.home()\n",
    "local_zip_path = home_dir / \"FES2014.zip\"\n",
    "extract_dir = home_dir / \"FES2014\"\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(save_path, 'wb') as f, tqdm(\n",
    "            desc=\"Downloading\",\n",
    "            total=total_size,\n",
    "            unit='B',\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024\n",
    "        ) as bar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "\n",
    "        print(\"Download completed successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Extraction completed. Files are available in '{extract_to}' directory.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"Error: The downloaded file is not a valid ZIP archive.\")\n",
    "        exit(1)\n",
    "\n",
    "def verify_extraction(extract_to):\n",
    "    if not extract_to.exists() or not any(extract_to.iterdir()):\n",
    "        print(\"Error: Extraction failed or directory is empty.\")\n",
    "        exit(1)\n",
    "\n",
    "def cleanup(zip_path):\n",
    "    try:\n",
    "        zip_path.unlink()  # Deletes the ZIP file\n",
    "        print(f\"Deleted {zip_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting ZIP file: {e}\")\n",
    "\n",
    "# Run the process\n",
    "download_file(url, local_zip_path)\n",
    "\n",
    "# Ensure the extraction directory exists\n",
    "extract_dir.mkdir(exist_ok=True)\n",
    "\n",
    "extract_zip(local_zip_path, extract_dir)\n",
    "verify_extraction(extract_dir)\n",
    "cleanup(local_zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Crop FES2014.hdf5 to your grid area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Provide paths to your input (original) and output (cropped) HDF5 files.\n",
    "home_dir = pathlib.Path.home()\n",
    "input_file = home_dir / \"FES2014\" / 'FES2014.hdf5'\n",
    "output_file = os.path.join(os.getcwd(), \"GeneralData/BoundaryConditions/FES2014/FES2014.hdf5\")\n",
    "    \n",
    "def get_bbox_indices(lon_arr, lat_arr, min_lon, max_lon, min_lat, max_lat):\n",
    "    \"\"\"\n",
    "    Given 2D longitude and latitude arrays, compute the row and column indices\n",
    "    corresponding to the region defined by the geographic bounding box.\n",
    "    \n",
    "    Parameters:\n",
    "        lon_arr : 2D numpy array of longitudes.\n",
    "        lat_arr : 2D numpy array of latitudes.\n",
    "        min_lon, max_lon : float, desired longitude limits.\n",
    "        min_lat, max_lat : float, desired latitude limits.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (row_start, row_end, col_start, col_end) for slicing.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask selecting grid cells within the bounding box.\n",
    "    mask = (lon_arr >= min_lon) & (lon_arr <= max_lon) & \\\n",
    "           (lat_arr >= min_lat) & (lat_arr <= max_lat)\n",
    "    \n",
    "    if not np.any(mask):\n",
    "        raise ValueError(\"No grid points fall within the provided bounding box.\")\n",
    "    \n",
    "    # Determine the minimal spanning rectangle for the selected grid cells.\n",
    "    rows, cols = np.where(mask)\n",
    "    row_start = rows.min()\n",
    "    row_end = rows.max() + 1  # +1 because Python slices are half-open.\n",
    "    col_start = cols.min()\n",
    "    col_end = cols.max() + 1\n",
    "    \n",
    "    return row_start, row_end, col_start, col_end\n",
    "\n",
    "def create_dataset_with_attrs(out_group, key, data, source_ds):\n",
    "    \"\"\"\n",
    "    Create a new dataset in out_group with the provided data while preserving\n",
    "    the source dataset's compression, chunking, and attributes. If the original \n",
    "    chunk shape is larger than the new data dimensions, adjust the chunk shape to \n",
    "    fit the new data.\n",
    "    \n",
    "    Parameters:\n",
    "        out_group : HDF5 group (or file) where the new dataset will be created.\n",
    "        key       : The name for the new dataset.\n",
    "        data      : The data to be stored.\n",
    "        source_ds : The source dataset from which attributes and storage options are copied.\n",
    "    \"\"\"\n",
    "    ds_kwargs = {}\n",
    "    # Preserve compression if present.\n",
    "    if source_ds.compression:\n",
    "        ds_kwargs['compression'] = source_ds.compression\n",
    "        if source_ds.compression_opts is not None:\n",
    "            ds_kwargs['compression_opts'] = source_ds.compression_opts\n",
    "    # Preserve chunking but adjust so that chunks do not exceed the new data dimensions.\n",
    "    if source_ds.chunks:\n",
    "        # Recalculating the chunk shape so that each chunk dimension is at most the corresponding data dimension.\n",
    "        new_chunks = tuple(min(new_dim, orig_chunk) for new_dim, orig_chunk in zip(data.shape, source_ds.chunks))\n",
    "        ds_kwargs['chunks'] = new_chunks\n",
    "    # Create the new dataset with the options.\n",
    "    new_ds = out_group.create_dataset(key, data=data, **ds_kwargs)\n",
    "    # Copy all dataset attributes.\n",
    "    for attr_key, attr_val in source_ds.attrs.items():\n",
    "        new_ds.attrs[attr_key] = attr_val\n",
    "    return new_ds\n",
    "\n",
    "def recursive_copy(in_group, out_group, crop_slice, grid_shape):\n",
    "    # Extract the row and column start/end indices from crop_slice for clarity.\n",
    "    row_start, row_end = crop_slice[0].start, crop_slice[0].stop\n",
    "    col_start, col_end = crop_slice[1].start, crop_slice[1].stop\n",
    "\n",
    "    for key in in_group:\n",
    "        item = in_group[key]\n",
    "        if isinstance(item, h5py.Dataset):\n",
    "            data = item[()]\n",
    "            if data.ndim >= 2:\n",
    "                # Determine what the spatial dimensions of this dataset are.\n",
    "                data_grid_shape = data.shape[-2:]\n",
    "                if data_grid_shape == grid_shape:\n",
    "                    # Data has the same shape as the lat/lon arrays.\n",
    "                    slicing = (slice(None),) * (data.ndim - 2) + crop_slice\n",
    "                elif data_grid_shape == (grid_shape[0] - 1, grid_shape[1] - 1):\n",
    "                    # Data is defined on grid cells, which is one less in each dimension.\n",
    "                    adjusted_crop_slice = (slice(row_start, row_end - 1), slice(col_start, col_end - 1))\n",
    "                    slicing = (slice(None),) * (data.ndim - 2) + adjusted_crop_slice\n",
    "                else:\n",
    "                    # If the dataset doesn't match either expected shape, copy it unchanged.\n",
    "                    slicing = None\n",
    "\n",
    "                if slicing is not None:\n",
    "                    cropped = data[slicing]\n",
    "                    create_dataset_with_attrs(out_group, key, cropped, item)\n",
    "                else:\n",
    "                    create_dataset_with_attrs(out_group, key, data, item)\n",
    "            else:\n",
    "                create_dataset_with_attrs(out_group, key, data, item)\n",
    "        elif isinstance(item, h5py.Group):\n",
    "            # Recursively copy groups and their attributes.\n",
    "            new_group = out_group.create_group(key)\n",
    "            for attr_key, attr_val in item.attrs.items():\n",
    "                new_group.attrs[attr_key] = attr_val\n",
    "            recursive_copy(item, new_group, crop_slice, grid_shape)\n",
    "\n",
    "def crop_hdf5_file(input_file, output_file, min_lon, max_lon, min_lat, max_lat):\n",
    "    \"\"\"\n",
    "    Crop datasets within an HDF5 file that share a common grid, based on a geographical\n",
    "    bounding box specified by min/max longitudes and latitudes. Datasets whose last two\n",
    "    dimensions match the grid will be cropped.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file  : Path to the original HDF5 file.\n",
    "        output_file : Path to the new (cropped) HDF5 file.\n",
    "        min_lon, max_lon : Geographic longitude limits.\n",
    "        min_lat, max_lat : Geographic latitude limits.\n",
    "    \"\"\"\n",
    "    # First, determine the crop indices using the grid datasets.\n",
    "    with h5py.File(input_file, 'r') as fin:\n",
    "        grid = fin['Grid']  # assumes a 'grid' group exists.\n",
    "        lon_arr = grid['Longitude'][()]\n",
    "        lat_arr = grid['Latitude'][()]\n",
    "        grid_shape = lon_arr.shape\n",
    "        row_start, row_end, col_start, col_end = get_bbox_indices(\n",
    "            lon_arr, lat_arr, min_lon, max_lon, min_lat, max_lat\n",
    "        )\n",
    "        print(f\"Cropping rows: {row_start} to {row_end}, columns: {col_start} to {col_end}\")\n",
    "        crop_slice = (slice(row_start, row_end), slice(col_start, col_end))\n",
    "\n",
    "    # Now recursively copy and crop datasets as needed.\n",
    "    with h5py.File(input_file, 'r') as fin, h5py.File(output_file, 'w') as fout:\n",
    "        recursive_copy(fin, fout, crop_slice, grid_shape)\n",
    "    \n",
    "    print(f\"Cropping completed. Output saved to '{output_file}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Run the cropping function.\n",
    "    crop_hdf5_file(input_file, output_file, min_lon, max_lon, min_lat, max_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Plot a specific dataset (e.g., M2 amplitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "# Specify the HDF5 file path and dataset key.\n",
    "file_path =  os.path.join(os.getcwd(), \"GeneralData/BoundaryConditions/FES2014/FES2014.hdf5\")\n",
    "dataset_key = '/Results/water level/M2/amplitude'  # Update as needed.\n",
    "    \n",
    "def plot_dataset(file_path, dataset_key):\n",
    "    \"\"\"\n",
    "    Open an HDF5 file and plot the specified dataset with several enhancements:\n",
    "    \n",
    "    - Masks invalid data values below -99.\n",
    "    - Uses coordinate arrays (if available) for georeferenced plotting.\n",
    "    - Automatically obtains a satellite basemap from an online tile service (Esri World Imagery)\n",
    "      and overlays it as the background.\n",
    "    - Saves the output plot as an image file and displays it inline (suitable for a Jupyter Notebook).\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the HDF5 file.\n",
    "        dataset_key (str): The key/path of the dataset to plot.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        data = f[dataset_key][()]\n",
    "        print(f\"Loaded dataset '{dataset_key}' with shape {data.shape}.\")\n",
    "\n",
    "        # Mask invalid values below -99.\n",
    "        data = np.ma.masked_where(data < -99, data)\n",
    "        print(\"Applied mask for values below -99.\")\n",
    "\n",
    "        # Construct an output filename based on the dataset key.\n",
    "        output_filename = f\"plot_{dataset_key.strip('/').replace('/', '_')}.png\"\n",
    "        print(f\"Output image filename will be: {output_filename}\")\n",
    "\n",
    "        # Attempt to load coordinate arrays for georeferencing.\n",
    "        try:\n",
    "            lon_arr = f[\"Grid/Longitude\"][()]\n",
    "            lat_arr = f[\"Grid/Latitude\"][()]\n",
    "            print(f\"Found coordinate arrays with shapes {lon_arr.shape} and {lat_arr.shape}.\")\n",
    "\n",
    "            # Check if the data shape matches the expected cell-centered dimensions.\n",
    "            if data.shape == (lon_arr.shape[0] - 1, lon_arr.shape[1] - 1):\n",
    "                # Set up the figure and axis.\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                \n",
    "                # Draw the data with pcolormesh.\n",
    "                mesh = ax.pcolormesh(lon_arr, lat_arr, data, shading='auto', cmap='viridis', zorder=2)\n",
    "                ax.set_xlabel(\"Longitude\")\n",
    "                ax.set_ylabel(\"Latitude\")\n",
    "                ax.set_title(f\"Plot of {dataset_key}\")\n",
    "                \n",
    "                # Automatically obtain and add a satellite basemap.\n",
    "                # Since our coordinates are in EPSG:4326, we pass that as the CRS.\n",
    "                # The Esri WorldImagery provider offers pretty good satellite images.\n",
    "                ctx.add_basemap(ax, crs=\"EPSG:4326\", source=ctx.providers.Esri.WorldImagery, alpha=1., zorder=1)\n",
    "                \n",
    "                fig.colorbar(mesh, ax=ax, label='Data values')\n",
    "                plt.savefig(output_filename)\n",
    "                print(f\"Saved plot to {output_filename}\")\n",
    "                plt.show()  # Shows the plot inline \n",
    "                plt.close()\n",
    "                return\n",
    "            else:\n",
    "                print(\"Data shape does not match expected cell-centered dimensions. Using fallback imshow.\")\n",
    "        except KeyError:\n",
    "            print(\"Coordinate arrays not found. Using imshow for plotting without basemap.\")\n",
    "\n",
    "        # Fallback: Plot using imshow when georeferenced coordinates are unavailable.\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        im = plt.imshow(data, origin='lower', cmap='viridis')\n",
    "        plt.xlabel(\"Column Index\")\n",
    "        plt.ylabel(\"Row Index\")\n",
    "        plt.title(f\"Plot of {dataset_key}\")\n",
    "        plt.colorbar(im, label='Data values')\n",
    "        plt.savefig(output_filename)\n",
    "        print(f\"Saved plot to {output_filename}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    plot_dataset(file_path, dataset_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Draw markers on the map to define the source coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "LonGrid = np.array(x_grid)\n",
    "LatGrid = np.array(y_grid)\n",
    "min_lon, max_lon = LonGrid.min(), LonGrid.max()\n",
    "min_lat, max_lat = LatGrid.min(), LatGrid.max()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Create the map.\n",
    "m = Map(center=(LatGrid.mean(), LonGrid.mean()), zoom=8)\n",
    "marker = None  # For interactive marker\n",
    "\n",
    "# Store the block (batch) layers in a dictionary, keyed by (block_row, block_col).\n",
    "block_layers = {}\n",
    "\n",
    "# Set a block (batch) size. (This controls the spatial grouping.)\n",
    "block_size = 10  # Adjust as needed.\n",
    "\n",
    "# Precompute grid cell corners (for all cells).\n",
    "# These arrays are of shape (M-1, N-1) if LonGrid and LatGrid are shape (M, N).\n",
    "lon_sw = LonGrid[:-1, :-1]  # Southwest corner\n",
    "lon_se = LonGrid[:-1,  1:]  # Southeast corner\n",
    "lon_ne = LonGrid[1:,   1:]  # Northeast corner\n",
    "lon_nw = LonGrid[1:,  :-1]  # Northwest corner\n",
    "\n",
    "lat_sw = LatGrid[:-1, :-1]\n",
    "lat_se = LatGrid[:-1,  1:]\n",
    "lat_ne = LatGrid[1:,   1:]\n",
    "lat_nw = LatGrid[1:,  :-1]\n",
    "\n",
    "# -------------------------\n",
    "# For discrete (flat) color mapping, weâ€™ll use globals for our binning.\n",
    "_nbins = 10\n",
    "_bins = None\n",
    "_discrete_colors = None\n",
    "\n",
    "def map_value_to_color(value):\n",
    "    if value == -99:\n",
    "        return \"#ffffff00\"  # Transparent for invalid cells.\n",
    "    # np.digitize returns indices in 1.._nbins+1; subtract 1 for 0-index.\n",
    "    bin_index = np.digitize(value, _bins) - 1\n",
    "    bin_index = int(np.clip(bin_index, 0, _nbins - 1))\n",
    "    return _discrete_colors[bin_index]\n",
    "\n",
    "def precompute_color_grid(zi, nbins=10):\n",
    "    \"\"\"\n",
    "    Precompute a flat color mapping for the depth grid zi.\n",
    "    Only valid cells (â‰  -99) get a color from the viridis colormap.\n",
    "    Also sets the global _bins, _nbins, and _discrete_colors for later use.\n",
    "    \"\"\"\n",
    "    global _bins, _nbins, _discrete_colors\n",
    "    _nbins = nbins\n",
    "    valid = zi != -99\n",
    "    if np.any(valid):\n",
    "        valid_min = zi[valid].min()\n",
    "        valid_max = zi[valid].max()\n",
    "    else:\n",
    "        valid_min, valid_max = 0, 1\n",
    "    _bins = np.linspace(valid_min, valid_max, nbins + 1)\n",
    "    cmap = plt.colormaps.get_cmap('viridis')\n",
    "    _discrete_colors = [mcolors.to_hex(c) for c in cmap(np.linspace(0, 1, nbins))]\n",
    "    # Optionally, you can return a full color array:\n",
    "    vectorized_map = np.vectorize(map_value_to_color)\n",
    "    return vectorized_map(zi)\n",
    "\n",
    "# Precompute colors on the entire grid (used only for initial reference).\n",
    "color_mapped_zi = precompute_color_grid(zi, nbins=_nbins)\n",
    "\n",
    "# -------------------------\n",
    "# New: Generate a GeoJSON FeatureCollection for a given block (spatial batch).\n",
    "def generate_block_geojson(block_row, block_col, block_size):\n",
    "    \"\"\"\n",
    "    Create a GeoJSON FeatureCollection for cells within one block.\n",
    "    The block covers cell indices:\n",
    "         i from block_row*block_size to min((block_row+1)*block_size, total rows)\n",
    "         j from block_col*block_size to min((block_col+1)*block_size, total cols)\n",
    "    Only cells where zi != -99 are rendered.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    total_rows, total_cols = zi.shape\n",
    "    row_start = block_row * block_size\n",
    "    row_end = min((block_row + 1) * block_size, total_rows)\n",
    "    col_start = block_col * block_size\n",
    "    col_end = min((block_col + 1) * block_size, total_cols)\n",
    "    for i in range(row_start, row_end):\n",
    "        for j in range(col_start, col_end):\n",
    "            # Skip cell if it's invalid.\n",
    "            if zi[i, j] == -99:\n",
    "                continue\n",
    "            coordinates = [[\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])],  # SW\n",
    "                [float(lon_se[i, j]), float(lat_se[i, j])],  # SE\n",
    "                [float(lon_ne[i, j]), float(lat_ne[i, j])],  # NE\n",
    "                [float(lon_nw[i, j]), float(lat_nw[i, j])],  # NW\n",
    "                [float(lon_sw[i, j]), float(lat_sw[i, j])]   # Close ring\n",
    "            ]]\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": coordinates\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"fill\": map_value_to_color(zi[i, j]),\n",
    "                    \"stroke\": \"#000000\",\n",
    "                    \"fill-opacity\": 0.5,\n",
    "                    \"stroke-width\": 0.2,\n",
    "                    \"i\": i,\n",
    "                    \"j\": j\n",
    "                }\n",
    "            }\n",
    "            features.append(feature)\n",
    "    return {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "# -------------------------\n",
    "# Update (or initially generate) all blocks.\n",
    "def update_all_blocks():\n",
    "    global block_layers\n",
    "    # Remove any existing block layers.\n",
    "    for layer in block_layers.values():\n",
    "        m.remove_layer(layer)\n",
    "    block_layers = {}\n",
    "    \n",
    "    total_rows, total_cols = zi.shape\n",
    "    n_block_rows = (total_rows + block_size - 1) // block_size\n",
    "    n_block_cols = (total_cols + block_size - 1) // block_size\n",
    "    \n",
    "    for br in range(n_block_rows):\n",
    "        for bc in range(n_block_cols):\n",
    "            fc = generate_block_geojson(br, bc, block_size)\n",
    "            # Only add a block if it contains any features.\n",
    "            if fc[\"features\"]:\n",
    "                layer = GeoJSON(\n",
    "                    data=fc,\n",
    "                    style_callback=lambda feature: {\n",
    "                        \"fillColor\": feature[\"properties\"][\"fill\"],\n",
    "                        \"color\": feature[\"properties\"][\"stroke\"],\n",
    "                        \"weight\": feature[\"properties\"][\"stroke-width\"],\n",
    "                        \"fillOpacity\": feature[\"properties\"][\"fill-opacity\"],\n",
    "                    }\n",
    "                )\n",
    "                m.add_layer(layer)\n",
    "                block_layers[(br, bc)] = layer\n",
    "\n",
    "# For initial rendering, generate all blocks.\n",
    "update_all_blocks()\n",
    "print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "#display(m)\n",
    "\n",
    "# -------------------------------\n",
    "# Global store for drawn markers\n",
    "# -------------------------------\n",
    "# Dictionary mapping marker_id -> {'location': [lon, lat], 'name': str}\n",
    "markers_dict = {}\n",
    "marker_counter = 0  # Unique marker counter\n",
    "\n",
    "def ask_marker_name(marker, marker_id):\n",
    "    \"\"\"\n",
    "    Display a text input widget to ask for a marker name.\n",
    "    When confirmed, assign the name to the marker and update the global dictionary.\n",
    "    \"\"\"\n",
    "    name_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Enter source name',\n",
    "        description='Source name:',\n",
    "        disabled=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    confirm_button = widgets.Button(\n",
    "        description='Confirm',\n",
    "        disabled=False,\n",
    "        button_style='success'\n",
    "    )\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        marker_name = name_input.value.strip()\n",
    "        if not marker_name:\n",
    "            marker_name = f\"Marker {marker_id}\"\n",
    "        # Attach the marker name\n",
    "        marker.marker_name = marker_name\n",
    "        # Update the marker entry with the name\n",
    "        markers_dict[marker_id]['name'] = marker_name\n",
    "        print(f\"Marker {marker_id} named '{marker_name}'\")\n",
    "        widget_box.close()  # Close the widget after confirmation\n",
    "        \n",
    "    confirm_button.on_click(on_button_clicked)\n",
    "    widget_box = widgets.VBox([name_input, confirm_button])\n",
    "    display(widget_box)\n",
    "\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global marker_counter, markers_dict\n",
    "    \n",
    "    if action == \"created\":\n",
    "        coords = geo_json[\"geometry\"][\"coordinates\"]  # [lon, lat]\n",
    "        marker_id = marker_counter  # Assign an ID\n",
    "        marker_counter += 1\n",
    "        # Store with location and placeholder for marker name\n",
    "        markers_dict[marker_id] = {'location': [coords[1], coords[0]], 'name': None}\n",
    "    \n",
    "        # Create a draggable marker\n",
    "        custom_marker = Marker(location=[coords[1], coords[0]], draggable=True)\n",
    "        custom_marker.marker_id = marker_id  # Attach marker ID\n",
    "    \n",
    "        # Ask user to enter a name for the new marker\n",
    "        ask_marker_name(custom_marker, marker_id)\n",
    "    \n",
    "        # Function to update stored marker coordinates when moved\n",
    "        def on_location_change(change, m_id=marker_id):\n",
    "            new_location = change[\"new\"]  # New [lat, lon]\n",
    "            markers_dict[m_id]['location'] = [new_location[1], new_location[0]]\n",
    "            print(f\"Marker {m_id} moved to: {markers_dict[m_id]['location']}\")\n",
    "    \n",
    "        # Observe marker movement\n",
    "        custom_marker.observe(on_location_change, names=\"location\")\n",
    "    \n",
    "        # Add the new draggable marker to the map\n",
    "        m.add_layer(custom_marker)\n",
    "        \n",
    "        # Remove the default marker added by DrawControl\n",
    "        for layer in list(m.layers):\n",
    "            if isinstance(layer, GeoJSON) and layer.data.get(\"geometry\", {}).get(\"type\", \"\") == \"Point\":\n",
    "                m.remove_layer(layer)\n",
    "    \n",
    "    elif action == \"edited\":\n",
    "        print(\"Edit event received; marker updates are handled via the location observer.\")\n",
    "    \n",
    "    elif action == \"deleted\":\n",
    "        features = geo_json.get(\"features\", [])\n",
    "        if not features:\n",
    "            features = [geo_json]\n",
    "        for feature in features:\n",
    "            marker_id = feature.get(\"properties\", {}).get(\"marker_id\")\n",
    "            if marker_id is not None and marker_id in markers_dict:\n",
    "                print(f\"Deleted marker {marker_id} named '{markers_dict[marker_id]['name']}' at {markers_dict[marker_id]['location']}\")\n",
    "                markers_dict.pop(marker_id)\n",
    "        print(\"Current markers after deletion:\", markers_dict)\n",
    "\n",
    "draw_control = DrawControl()\n",
    "# For drawing tools we don't need, use empty dictionaries.\n",
    "draw_control.polygon   = {}\n",
    "draw_control.polyline  = {}\n",
    "draw_control.rectangle = {}\n",
    "draw_control.circle    = {}\n",
    "# Enable marker drawing button by assigning a non-empty dictionary.\n",
    "draw_control.marker    = {\"repeatMode\": False}\n",
    "\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# -------------------------------\n",
    "# Display the map \n",
    "# -------------------------------\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup MOHID Water input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "write_date(\"Model_2.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run MOHID Water"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
