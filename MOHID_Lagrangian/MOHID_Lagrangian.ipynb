{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOHID Lagrangian\n",
    "\n",
    "This Jupyter Notebook aims to help implement and run the MOHID Lagrangian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "***\n",
    "**Note 1**: Execute each cell through the <button class=\"btn btn-default btn-xs\"><i class=\"icon-play fa fa-play\"></i></button> button from the top MENU (or keyboard shortcut `Shift` + `Enter`).<br>\n",
    "<br>\n",
    "**Note 2**: Use the Kernel and Cell menus to restart the kernel and clear outputs.<br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "- [1. Import required libraries](#1.-Import-required-libraries)\n",
    "- [2. General options](#2.-General-options)\n",
    "    - [2.1 Set run case](#2.1-Set-run-case)\n",
    "    - [2.2 Set dates](#2.2-Set-dates)\n",
    "    - [2.3 Draw a polygon to select the area of interest](#2.3-Draw-a-polygon-to-select-the-area-of-interest)\n",
    "- [3. Download currents for the area of interest](#3.-Download-currents-for-the-area-of-interest)\n",
    "    - [3.1 Create Copernicus Marine credentials file](#3.1-Create-Copernicus-Marine-credentials-file)\n",
    "    - [3.2 Set CMEMS product and depths](#3.2-Set-CMEMS-product-and-depths) \n",
    "    - [3.3 Download CMEMS](#3.3-Download-CMEMS)\n",
    "- [4. Download wind field for the area of interest](#4.-Download-wind-field-for-the-area-of-interest)\n",
    "    - [4.1 Setup the CDS API personal access token](#4.1-Setup-the-CDS-API-personal-access-token)\n",
    "    - [4.2 Download ERA5 Reanalysis](#4.2-Download-ERA5-Reanalysis)\n",
    "- [5. Load the hydrodynamic solution](#5.-Load-the-hydrodynamic-solution)\n",
    "    - [5.1 Load a NetCDF file from CMEMS](#5.1-Load-a-NetCDF-file-from-CMEMS)\n",
    "    - [5.2 Load a HDF5 file from MOHID](#5.2-Load-a-HDF5-file-from-MOHID)\n",
    "- [6. Define sources location](#6.-Define-sources-location)\n",
    "    - [6.1 Draw markers on the map to define the source coordinates](#6.1-Draw-markers-on-the-map-to-define-the-source-coordinates)\n",
    "- [7. Setup MOHID Lagrangian xml input files](#6.-Setup-MOHID-Lagrangian-xml-input-files)\n",
    "    - [7.1 Parameter definitions](#7.1-Parameter-definitions)\n",
    "    - [7.2 Simulation definitions](#7.2-Simulation-definitions)\n",
    "    - [7.3 Source definitions](#7.3-Source-definitions)\n",
    "- [8. Run MOHID Lagrangian](#8.-Run-MOHID-Lagrangian)\n",
    "- [9. Visualize results](#9.-Visualize-results)\n",
    "    - [9.1 Particles](#9.1-Particles)\n",
    "    - [9.2 Heatmap](#9.2-Heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from update_xml_case import *\n",
    "import copernicusmarine\n",
    "import cdsapi\n",
    "import zipfile\n",
    "import os\n",
    "from ipyleaflet import Map, TileLayer, DrawControl, GeoJSON, Marker\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import vtk\n",
    "import folium\n",
    "import matplotlib as mpl\n",
    "from folium.plugins import HeatMap, HeatMapWithTime, MeasureControl\n",
    "import glob\n",
    "import h5py\n",
    "import netCDF4\n",
    "from pathlib import Path\n",
    "from folium.plugins import TimestampedGeoJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. General options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Set run case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Plastic_Case\"\n",
    "\n",
    "dirpath = \"run_cases\" \n",
    "\n",
    "xml_file_path = f\"{name}.xml\"\n",
    "\n",
    "# Construct the path and change the working directory\n",
    "os.chdir(os.path.join(dirpath, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2025,1,1,0,0,0)\n",
    "end_date = datetime.datetime(2025,2,1,0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Draw a polygon to select the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ipyleaflet map centered at (0,0)\n",
    "m = Map(center=[0, 0], zoom=2)\n",
    "\n",
    "# Define WMTS Layer\n",
    "wmts_layer = TileLayer(\n",
    "    url=\"https://wmts.marine.copernicus.eu/teroWmts/?\"\n",
    "        \"service=WMTS&request=GetTile&version=1.0.0&\"\n",
    "        \"layer=GLOBAL_ANALYSISFORECAST_PHY_001_024/\"\n",
    "        \"cmems_mod_glo_phy-cur_anfc_0.083deg_P1M-m_202406/sea_water_velocity&\"\n",
    "        \"tilematrixset=EPSG:3857&tilematrix={z}&tilerow={y}&tilecol={x}&\"\n",
    "        \"format=image/png&transparent=True\",\n",
    "    name=\"Sea Water Velocity\",\n",
    "    no_wrap=True\n",
    ")\n",
    "\n",
    "# Add WMTS layer to the map\n",
    "m.add_layer(wmts_layer)\n",
    "\n",
    "# Create a DrawControl for user interaction\n",
    "draw_control = DrawControl(\n",
    "    polygon={},  # Empty dict disables polygon\n",
    "    rectangle={\"shapeOptions\": {\"color\": \"blue\"}},  # Enable rectangles\n",
    "    circle={},  # Empty dict disables circles\n",
    "    polyline={},  # Empty dict disables polylines\n",
    "    marker={}  # Empty dict disables markers\n",
    ")\n",
    "\n",
    "# Function to handle drawn shapes and print bounds\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global min_lat, max_lat, min_lon, max_lon\n",
    "    \n",
    "    if action == \"created\":\n",
    "        \n",
    "        # Extract coordinates from the drawn shape\n",
    "        coords = geo_json[\"geometry\"][\"coordinates\"][0]\n",
    "\n",
    "        # Get min/max latitude and longitude\n",
    "        lats = [point[1] for point in coords]  \n",
    "        lons = [point[0] for point in coords]  \n",
    "\n",
    "        min_lat, max_lat = min(lats), max(lats)\n",
    "        min_lon, max_lon = min(lons), max(lons)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"✅ Polygon Bounds:\")\n",
    "        print(f\"  - min_lon (West): {min_lon}\")\n",
    "        print(f\"  - min_lat (South): {min_lat}\")\n",
    "        print(f\"  - max_lon (East): {max_lon}\")\n",
    "        print(f\"  - max_lat (North): {max_lat}\")\n",
    "\n",
    "# Attach the handler correctly\n",
    "draw_control.on_draw(handle_draw)\n",
    "\n",
    "# Add DrawControl to the map\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# Display the interactive map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Download currents for the area of interest\n",
    "Skip this step if you have another hydrodynamic dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Copernicus Marine credentials file\n",
    "#It has to be done only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The login command will check your Copernicus Marine credentials and create the configuration file. \n",
    "copernicusmarine.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Set CMEMS product and depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_cmems = os.path.join(os.getcwd(),\"nc_fields\",\"currents\")\n",
    "\n",
    "#hourly instanataneous\n",
    "#product_id = \"cmems_mod_glo_phy_anfc_merged-uv_PT1H-i\"\n",
    "\n",
    "#6-hourly instanataneous\n",
    "#product_id = \"cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i\"\n",
    "\n",
    "#daily mean\n",
    "product_id = \"cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m\"\n",
    "\n",
    "start_depth = 0.49402499198913574\n",
    "end_depth = 0.49402499198913574\n",
    "#end_depth = 5727.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Download CMEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "def download_file():\n",
    " \n",
    "        variable = ['uo','vo']            \n",
    "                                \n",
    "        copernicusmarine.subset(\n",
    "           dataset_id = product_id,\n",
    "           minimum_longitude = min_lon, maximum_longitude = max_lon,\n",
    "           minimum_latitude = min_lat, maximum_latitude = max_lat,\n",
    "           minimum_depth = start_depth, maximum_depth = end_depth, \n",
    "           start_datetime = str(start_date.strftime('%Y-%m-%d'))+' 00:00:00', \n",
    "           end_datetime = str(end_date.strftime('%Y-%m-%d'))+' 00:00:00',\n",
    "           variables = variable, \n",
    "           output_directory = output_dir_cmems,\n",
    "           output_filename = output_file_cmems,\n",
    "           netcdf3_compatible = True)\n",
    "                \n",
    "#####################################################\n",
    "     \n",
    "output_file_cmems = \"cmems_\"+str(start_date.strftime(\"%Y%m%d\")) + \"_\" + str(end_date.strftime(\"%Y%m%d\") + \".nc\")\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir_cmems):\n",
    "        os.makedirs(output_dir_cmems)\n",
    "\n",
    "nc_files = glob.iglob(os.path.join(output_dir_cmems,\"*.nc\"))\n",
    "        \n",
    "for filename in nc_files:\n",
    "    os.remove(filename)\n",
    "    \n",
    "download_file()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Download wind field for the area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.1 Setup the CDS API personal access token\n",
    "It has to be done only once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have an account yet, please register (https://cds.climate.copernicus.eu/). If you are not logged in, please login. Once logged in, copy the URL and key.\n",
    "\n",
    "Create a file named .cdsapirc in your home directory.\n",
    "\n",
    "$HOME/.cdsapirc (in your Unix/Linux environment)\n",
    "\n",
    "%USERPROFILE%.cdsapirc file (in your windows environment,%USERPROFILE% is usually located at C:\\Users\\Username folder).\n",
    "\n",
    "Paste the URL and key into .cdsapirc file.\n",
    "\n",
    "The CDS API expects to find the .cdsapirc file in your home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Download ERA5 Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_dir = os.path.join(os.getcwd(),\"nc_fields\",\"winds\")\n",
    "target = os.path.join(os.getcwd(),\"nc_fields\",\"winds\", \"ERA5.zip\")\n",
    "\n",
    "if not os.path.exists(era5_dir):\n",
    "        os.makedirs(era5_dir)\n",
    "\n",
    "nc_files = glob.iglob(os.path.join(era5_dir,\"*.nc\"))\n",
    "        \n",
    "for filename in nc_files:\n",
    "    os.remove(filename)  \n",
    "            \n",
    "# Extract days across months correctly\n",
    "days = []\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    days.append(str(current_date.day))\n",
    "    current_date += datetime.timedelta(days=1)\n",
    "    \n",
    "dataset = \"reanalysis-era5-single-levels\"\n",
    "request = {\n",
    "    \"product_type\": [\"reanalysis\"],\n",
    "    \"variable\": [\n",
    "                '10m_u_component_of_wind', \n",
    "                '10m_v_component_of_wind',\n",
    "    ],\n",
    "    \"year\": sorted([str(start_date.year), str(end_date.year)]),\n",
    "    \"month\": sorted([str(start_date.month), str(end_date.month)]),\n",
    "    \"day\": days,\n",
    "    \"time\": [\n",
    "        \"00:00\", \"01:00\", \"02:00\",\n",
    "        \"03:00\", \"04:00\", \"05:00\",\n",
    "        \"06:00\", \"07:00\", \"08:00\",\n",
    "        \"09:00\", \"10:00\", \"11:00\",\n",
    "        \"12:00\", \"13:00\", \"14:00\",\n",
    "        \"15:00\", \"16:00\", \"17:00\",\n",
    "        \"18:00\", \"19:00\", \"20:00\",\n",
    "        \"21:00\", \"22:00\", \"23:00\"\n",
    "    ],\n",
    "    \"data_format\": \"netcdf\",\n",
    "    \"download_format\": \"zip\",\n",
    "    \"area\": [max_lat, min_lon, min_lat, max_lon]\n",
    "}\n",
    "\n",
    "#target = 'ERA5.zip'\n",
    "client = cdsapi.Client()\n",
    "\n",
    "try:\n",
    "    client.retrieve(dataset, request, target)\n",
    "    print(\"Download completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "try:\n",
    "    # Open and extract the ZIP file\n",
    "    with zipfile.ZipFile(target, 'r') as zip_ref:\n",
    "        zip_ref.extractall(era5_dir)\n",
    "    print(\"Extraction completed successfully.\")\n",
    "    \n",
    "    # Remove the ZIP file after successful extraction\n",
    "    os.remove(target)\n",
    "    print(\"Zip file has been removed.\")\n",
    "    \n",
    "except zipfile.BadZipFile:\n",
    "    print(\"Error: The file is not a valid ZIP archive.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The ZIP file was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "original_file_name = os.path.join(era5_dir,\"data_stream-oper_stepType-instant.nc\")\n",
    "output_file_era5 = os.path.join(era5_dir,\"era5_\"+str(start_date.strftime(\"%Y%m%d\")) + \"_\" + str(end_date.strftime(\"%Y%m%d\") + \".nc\"))\n",
    "os.rename(original_file_name, output_file_era5)\n",
    "\n",
    "## Open the file in update mode ('r+' allows in-place modifications)\n",
    "#with netCDF4.Dataset(output_file_era5, mode='r+') as ds:\n",
    "#    ds.renameVariable('valid_time', 'time')\n",
    "#    ds.renameDimension('valid_time', 'time')\n",
    "\n",
    "print(f\"Files extracted to {era5_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Load the hydrodynamic solution\n",
    "Load a NetCDF file from CMEMS or a HDF5 file from MOHID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load a NetCDF file from CMEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_cmems = os.path.join(os.getcwd(),\"nc_fields\",\"currents\")\n",
    "output_file_cmems = \"cmems_\"+str(start_date.strftime(\"%Y%m%d\")) + \"_\" + str(end_date.strftime(\"%Y%m%d\") + \".nc\")\n",
    "#output_file_cmems = \"cmems_20250101_20250105.nc\" #Load your own NetCDF instead of CMEMS, otherwise comment out this line\n",
    "CurFName = os.path.join(output_dir_cmems, output_file_cmems)\n",
    "dataset = xr.open_dataset(CurFName)\n",
    "\n",
    "CurFName = os.path.join(output_dir_cmems, output_file_cmems) \n",
    "# Open the datafile\n",
    "CurDS = xr.open_dataset(CurFName, engine=\"netcdf4\")\n",
    "\n",
    "U = dataset['uo'].isel(time=0).isel(depth=0).squeeze()  # Result is 2D: (lat, lon)\n",
    "V = dataset['vo'].isel(time=0).isel(depth=0).squeeze()  # Result is 2D: (lat, lon)\n",
    "\n",
    "# Show info of dataset\n",
    "CurDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2 Load a HDF5 file from MOHID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory and file path\n",
    "output_dir = os.path.join(os.getcwd(), \"Boundary_Conditions\")\n",
    "output_file_name = \"Hydrodynamic_2_Surface.hdf5\"\n",
    "CurFName = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "# Open the HDF5 file using h5py and load data into NumPy arrays.\n",
    "with h5py.File(CurFName, 'r') as f:\n",
    "    # Access the 'Results' group for velocity data.\n",
    "    res_group = f['Results']\n",
    "    instant = \"00001\"  # Define the time instant.\n",
    "    vel_u_key = f\"velocity U_{instant}\"\n",
    "    vel_v_key = f\"velocity V_{instant}\"\n",
    "    \n",
    "    # Extract and squeeze the velocity modulus data.\n",
    "    velocity_u = np.squeeze(res_group['velocity U'][vel_u_key][-1,:,:])\n",
    "    velocity_v = np.squeeze(res_group['velocity V'][vel_v_key][-1,:,:])\n",
    "    \n",
    "    # Extract grid corner coordinates for longitude and latitude.\n",
    "    lon_corners = (f['Grid']['Longitude'][:,0])\n",
    "    lat_corners = (f['Grid']['Latitude'][0,:])\n",
    "\n",
    "velocity_u = velocity_u.T\n",
    "velocity_v = velocity_v.T\n",
    "\n",
    "# Compute the center values by averaging adjacent elements\n",
    "lon_center = (lon_corners[:-1] + lon_corners[1:]) / 2\n",
    "lat_center = (lat_corners[:-1] + lat_corners[1:]) / 2\n",
    "\n",
    "# Create an xarray Dataset with matching dimensions.\n",
    "dataset = xr.Dataset(\n",
    "    {\n",
    "        \"uo\": ((\"latitude\",\"longitude\"), velocity_u),\n",
    "        \"vo\": ((\"latitude\",\"longitude\"), velocity_v),\n",
    "    },\n",
    "    coords={\n",
    "        \"longitude\": lon_center,\n",
    "        \"latitude\": lat_center,\n",
    "    }\n",
    ")\n",
    "\n",
    "U = dataset['uo']\n",
    "V = dataset['vo']\n",
    "\n",
    "U = U.where((U > -99) & (U != 0), np.nan)\n",
    "V = V.where((V > -99) & (V != 0), np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6. Define sources location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Draw markers on the map to define the source coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Start timing\n",
    "# -------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "zi = np.sqrt(U**2 + V**2)  # Velocity magnitude, 2D array\n",
    "\n",
    "# Create an Output widget to capture the callback prints\n",
    "output = widgets.Output()\n",
    "display(output)\n",
    "\n",
    "# -------------------------------\n",
    "# Optional Downsampling for large datasets\n",
    "# -------------------------------\n",
    "downsample_factor = 4  # Adjust as needed.\n",
    "zi = zi[::downsample_factor, ::downsample_factor]\n",
    "lon = dataset['longitude'].values[::downsample_factor]\n",
    "lat = dataset['latitude'].values[::downsample_factor]\n",
    "\n",
    "#print(\"zi:\", zi.shape)\n",
    "#print(\"Latitude size:\", lat.shape)\n",
    "#print(\"Longitude size:\", lon.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# Get coordinate grids\n",
    "# -------------------------------\n",
    "LonGrid, LatGrid = np.meshgrid(lon, lat)\n",
    "        \n",
    "# -------------------------------\n",
    "# Precompute color mapping (vectorized)\n",
    "# -------------------------------\n",
    "colormap = plt.cm.viridis\n",
    "norm = Normalize(vmin=np.nanmin(zi), vmax=np.nanmax(zi))\n",
    "flat_colors = [to_hex(c) for c in colormap(norm(zi.values.ravel()))]\n",
    "colors = np.array(flat_colors).reshape(zi.shape)\n",
    "#print(f\"Time spent on color mapping: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# -------------------------------\n",
    "# Precompute grid cell corners for GeoJSON layers\n",
    "# -------------------------------\n",
    "lon_sw = LonGrid[:-1, :-1]   # Southwest corner\n",
    "lon_se = LonGrid[:-1,  1:]   # Southeast corner\n",
    "lon_ne = LonGrid[1:,   1:]   # Northeast corner\n",
    "lon_nw = LonGrid[1:,  :-1]   # Northwest corner\n",
    "\n",
    "lat_sw = LatGrid[:-1, :-1]\n",
    "lat_se = LatGrid[:-1,  1:]\n",
    "lat_ne = LatGrid[1:,   1:]\n",
    "lat_nw = LatGrid[1:,  :-1]\n",
    "\n",
    "zi_cells = zi.values[:-1, :-1]\n",
    "#print(\"zi_cells:\", zi_cells.shape)\n",
    "def generate_geojson_batches(batch_size=50):\n",
    "    \"\"\"\n",
    "    Yields GeoJSON FeatureCollections for each batch of grid cells.\n",
    "    Cells with NaN velocity magnitude are skipped.\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = lon_sw.shape\n",
    "    for i in range(0, n_rows, batch_size):\n",
    "        for j in range(0, n_cols, batch_size):\n",
    "            i_end = min(i + batch_size, n_rows)\n",
    "            j_end = min(j + batch_size, n_cols)\n",
    "            features = []\n",
    "            for r in range(i, i_end):\n",
    "                for c in range(j, j_end):\n",
    "                    if np.isnan(zi_cells[r, c]):\n",
    "                        continue\n",
    "                    feature = {\n",
    "                        \"type\": \"Feature\",\n",
    "                        \"geometry\": {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                                [lon_sw[r, c], lat_sw[r, c]],\n",
    "                                [lon_se[r, c], lat_se[r, c]],\n",
    "                                [lon_ne[r, c], lat_ne[r, c]],\n",
    "                                [lon_nw[r, c], lat_nw[r, c]]\n",
    "                            ]]\n",
    "                        },\n",
    "                        \"properties\": {\n",
    "                            \"fill\": colors[r, c],\n",
    "                            \"stroke\": \"#000000\",\n",
    "                            \"fill-opacity\": 0.5,\n",
    "                            \"stroke-width\": 0.2\n",
    "                        }\n",
    "                    }\n",
    "                    features.append(feature)\n",
    "            if features:\n",
    "                yield {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "# -------------------------------\n",
    "# Create the map and add dataset layers\n",
    "# -------------------------------\n",
    "map_center = [(lat.min() + lat.max()) / 2, (lon.min() + lon.max()) / 2]\n",
    "m = Map(center=map_center, zoom=4)\n",
    "\n",
    "for batch_geojson in generate_geojson_batches():\n",
    "    geojson_layer = GeoJSON(\n",
    "        data=batch_geojson,\n",
    "        style_callback=lambda feature: {\n",
    "            \"fillColor\": feature[\"properties\"][\"fill\"],\n",
    "            \"color\": feature[\"properties\"][\"stroke\"],\n",
    "            \"weight\": feature[\"properties\"][\"stroke-width\"],\n",
    "            \"fillOpacity\": feature[\"properties\"][\"fill-opacity\"]\n",
    "        }\n",
    "    )\n",
    "    m.add_layer(geojson_layer)\n",
    "\n",
    "#print(f\"Total time for GeoJSON layers: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# -------------------------------\n",
    "# Global store for drawn markers\n",
    "# -------------------------------\n",
    "# Dictionary mapping marker_id -> {'location': [lon, lat], 'name': str}\n",
    "markers_dict = {}\n",
    "marker_counter = 0  # Unique marker counter\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def ask_marker_name(marker, marker_id):\n",
    "    \"\"\"\n",
    "    Display a text input widget to ask for a marker name.\n",
    "    When confirmed, assign the name to the marker and update the global dictionary.\n",
    "    \"\"\"\n",
    "    with output:\n",
    "        print(\"ask_marker_name invoked for marker\", marker_id)\n",
    "        \n",
    "        # Create the input widget and confirm button.\n",
    "        name_input = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Enter source name',\n",
    "            description='Source name:',\n",
    "            disabled=False,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        confirm_button = widgets.Button(\n",
    "            description='Confirm',\n",
    "            disabled=False,\n",
    "            button_style='success'\n",
    "        )\n",
    "        \n",
    "        # Create the widget container _before_ defining the callback.\n",
    "        widget_box = widgets.VBox([name_input, confirm_button])\n",
    "        \n",
    "        # Define the callback, explicitly capturing widget_box in the default argument.\n",
    "        def on_button_clicked(b, widget=widget_box):\n",
    "            marker_name = name_input.value.strip()\n",
    "            if not marker_name:\n",
    "                marker_name = f\"Marker {marker_id}\"\n",
    "            # Attach the marker name.\n",
    "            marker.marker_name = marker_name\n",
    "            # Update the marker entry with the name.\n",
    "            markers_dict[marker_id]['name'] = marker_name\n",
    "            print(f\"Marker {marker_id} named '{marker_name}'\")\n",
    "            widget.close()  # Close the widget after confirmation.\n",
    "        \n",
    "        confirm_button.on_click(on_button_clicked)\n",
    "        # Display the widget, so it shows in the notebook.\n",
    "        display(widget_box)\n",
    "\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global marker_counter, markers_dict\n",
    "    \n",
    "    if action == \"created\":\n",
    "        coords = geo_json[\"geometry\"][\"coordinates\"]  # [lon, lat]\n",
    "        marker_id = marker_counter  # Assign an ID\n",
    "        marker_counter += 1\n",
    "        # Store with location and placeholder for marker name\n",
    "        markers_dict[marker_id] = {'location': [coords[1], coords[0]], 'name': None}\n",
    "    \n",
    "        # Create a draggable marker\n",
    "        custom_marker = Marker(location=[coords[1], coords[0]], draggable=True)\n",
    "        custom_marker.marker_id = marker_id  # Attach marker ID\n",
    "    \n",
    "        # Ask user to enter a name for the new marker\n",
    "        ask_marker_name(custom_marker, marker_id)\n",
    "    \n",
    "        # Function to update stored marker coordinates when moved\n",
    "        def on_location_change(change, m_id=marker_id):\n",
    "            new_location = change[\"new\"]  # New [lat, lon]\n",
    "            markers_dict[m_id]['location'] = [new_location[1], new_location[0]]\n",
    "            print(f\"Marker {m_id} moved to: {markers_dict[m_id]['location']}\")\n",
    "    \n",
    "        # Observe marker movement\n",
    "        custom_marker.observe(on_location_change, names=\"location\")\n",
    "    \n",
    "        # Add the new draggable marker to the map\n",
    "        m.add_layer(custom_marker)\n",
    "        \n",
    "        # Remove the default marker added by DrawControl\n",
    "        for layer in list(m.layers):\n",
    "            if isinstance(layer, GeoJSON) and layer.data.get(\"geometry\", {}).get(\"type\", \"\") == \"Point\":\n",
    "                m.remove_layer(layer)\n",
    "    \n",
    "    elif action == \"edited\":\n",
    "        print(\"Edit event received; marker updates are handled via the location observer.\")\n",
    "    \n",
    "    elif action == \"deleted\":\n",
    "        features = geo_json.get(\"features\", [])\n",
    "        if not features:\n",
    "            features = [geo_json]\n",
    "        for feature in features:\n",
    "            marker_id = feature.get(\"properties\", {}).get(\"marker_id\")\n",
    "            if marker_id is not None and marker_id in markers_dict:\n",
    "                print(f\"Deleted marker {marker_id} named '{markers_dict[marker_id]['name']}' at {markers_dict[marker_id]['location']}\")\n",
    "                markers_dict.pop(marker_id)\n",
    "        print(\"Current markers after deletion:\", markers_dict)\n",
    "\n",
    "draw_control = DrawControl()\n",
    "# For drawing tools we don't need, use empty dictionaries.\n",
    "draw_control.polygon   = {}\n",
    "draw_control.polyline  = {}\n",
    "draw_control.rectangle = {}\n",
    "draw_control.circle    = {}\n",
    "# Enable marker drawing button by assigning a non-empty dictionary.\n",
    "draw_control.marker    = {\"repeatMode\": False}\n",
    "\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# -------------------------------\n",
    "# Display the map \n",
    "# -------------------------------\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7. Setup MOHID Lagrangian xml input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Parameter definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time limits (min and max time values in the dataset's 'time' variable)\n",
    "#start, end = min(dataset['time'].values), max(dataset['time'].values)\n",
    "\n",
    "# Convert the numpy.datetime64 to a Python datetime object using pandas\n",
    "Start = pd.to_datetime(start_date) \n",
    "End = pd.to_datetime(end_date)\n",
    "#Start = datetime.datetime(2024, 1, 1, 0, 0, 0)\n",
    "#End = datetime.datetime(2024, 1, 2, 0, 0, 0)\n",
    "\n",
    "Integrator = 2 #Integration Algorithm 1:Euler, 2:Multi-Step Euler, 3:RK4 (default=1)\n",
    "Threads = \"auto\" #Computation threads for shared memory computation (default=auto)\n",
    "OutputWriteTime = 86400 #Time out data (seconds)\n",
    "BufferSize = 86400 #control the amount of hydrodynamic data to store in RAM memory (seconds)\n",
    "\n",
    "# Run the update function\n",
    "update_parameter_definitions(xml_file_path,Start,End,Integrator,Threads,OutputWriteTime,BufferSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Simulation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Get area limits\n",
    "# -------------------------------\n",
    "min_lat, max_lat = min(dataset['latitude'].values), max(dataset['latitude'].values)\n",
    "min_lon, max_lon = min(dataset['longitude'].values), max(dataset['longitude'].values)\n",
    "\n",
    "resolution = 8000 #metres (m)\n",
    "timestep = 180 #seconds (s)\n",
    "BoundingBoxMin = min_lon,min_lat,-1 #defines the corners of your simulation domain x,y,z (deg,deg,m)\n",
    "BoundingBoxMax = max_lon, max_lat,1 #defines the corners of your simulation domain x,y,z (deg,deg,m)\n",
    "VerticalVelMethod = 3 #1:From velocity fields, 2:Divergence based, 3:Disabled. Default = 1\n",
    "BathyminNetcdf = 0 #bathymetry is a property in the netcdf. 1:true, 0:false (computes from layer depth and openPoints. Default = 1\n",
    "RemoveLandTracer = 0 #Remove tracers on land 0:No, 1:Yes. Default = 1\n",
    "TracerMaxAge = 0 #maximum tracer age. Default = 0.0. read if > 0\n",
    "\n",
    "# Run the update function\n",
    "update_simulation_definitions(xml_file_path,resolution,timestep,BoundingBoxMin,BoundingBoxMax,VerticalVelMethod,BathyminNetcdf,RemoveLandTracer,TracerMaxAge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Source definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(markers_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_emission = 0 #start of emission in seconds, considering start_date\n",
    "end_emission = 86400 #end of emission in seconds, considering start_date. Define end='end' to consider end_date\n",
    "\n",
    "rate_seconds = 3600 #emission step in seconds\n",
    "rate_trcPerEmission = 10 #number of tracers emitted every rate_seconds\n",
    "\n",
    "update_source_definitions(xml_file_path, markers_dict,rate_seconds,rate_trcPerEmission,start_emission,end_emission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8. Run MOHID Lagrangian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout = f\"{name}_out\"\n",
    "\n",
    "# Paths to executables and scripts\n",
    "tools = r\"../build/bin/RELEASE\"\n",
    "mohidlagrangian = os.path.join(tools, \"MOHIDLagrangian.exe\")\n",
    "\n",
    "preprocessor_dir = r\"../src/MOHIDLagrangianPreProcessor\"\n",
    "preprocessor = os.path.join(preprocessor_dir, \"MOHIDLagrangianPreProcessor.py\")\n",
    "\n",
    "postprocessor_dir = r\"../src/MOHIDLagrangianPostProcessor\"\n",
    "postprocessor = os.path.join(postprocessor_dir, \"MOHIDLagrangianPostprocessor.py\")\n",
    "\n",
    "# Manage output directory\n",
    "if os.path.exists(dirout):\n",
    "    shutil.rmtree(dirout)\n",
    "os.makedirs(dirout)\n",
    "\n",
    "# Copy XML configuration file\n",
    "shutil.copy(f\"{name}.xml\", dirout)\n",
    "\n",
    "# Run Preprocessing\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [sys.executable, preprocessor, \"-i\", f\"{dirout}/{name}.xml\", \"-o\", dirout],\n",
    "        check=True,\n",
    "    )\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Preprocessing failed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Run Main Executable\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [mohidlagrangian, \"-i\", f\"{dirout}/{name}.xml\", \"-o\", dirout],\n",
    "        check=True,\n",
    "    )\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Execution of MOHIDLagrangian failed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Run Postprocessing\n",
    "try:\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-W\", \"ignore\", postprocessor, \"-i\", f\"{name}.xml\", \"-o\", dirout],\n",
    "        check=True,\n",
    "    )\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"Postprocessing failed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 9. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- PARAMETERS ----\n",
    "dirout  = f\"{name}_out\"              # e.g. \"Plastic_Case_out\"\n",
    "pattern = os.path.join(dirout, f\"{name}_*.vtu\")\n",
    "\n",
    "# ---- 1) Collect & sort only numeric‐suffix VTUs ----\n",
    "all_vtu = [\n",
    "    fn for fn in glob.glob(pattern)\n",
    "    if re.search(rf'{re.escape(name)}_[0-9]+\\.vtu$', os.path.basename(fn))\n",
    "]\n",
    "if not all_vtu:\n",
    "    raise FileNotFoundError(f\"No timestep VTU files in {dirout!r}\")\n",
    "\n",
    "def seq(fn):\n",
    "    return int(re.search(r'_(\\d+)\\.vtu$', os.path.basename(fn)).group(1))\n",
    "\n",
    "all_vtu.sort(key=seq)\n",
    "\n",
    "# ---- 2) Scan for unique 'source' values ----\n",
    "unique_sources = set()\n",
    "for fn in all_vtu:\n",
    "    rdr = vtk.vtkXMLUnstructuredGridReader()\n",
    "    rdr.SetFileName(fn)\n",
    "    rdr.Update()\n",
    "    arr = rdr.GetOutput().GetPointData().GetArray(\"source\")\n",
    "    if arr:\n",
    "        for i in range(arr.GetNumberOfTuples()):\n",
    "            unique_sources.add(arr.GetValue(i))\n",
    "    else:\n",
    "        print(f\"  ⚠️  Skipping scan for {os.path.basename(fn)} (no 'source')\")\n",
    "\n",
    "unique_sources = sorted(unique_sources)\n",
    "if not unique_sources:\n",
    "    raise RuntimeError(\"No 'source' values found in any VTU.\")\n",
    "\n",
    "# Assign each source a hex color from Dark2\n",
    "cmap      = plt.get_cmap(\"Dark2\")\n",
    "colors    = [mpl.colors.rgb2hex(cmap(i/len(unique_sources))) \n",
    "             for i in range(len(unique_sources))]\n",
    "color_map = {src: colors[i] for i, src in enumerate(unique_sources)}\n",
    "\n",
    "# ---- 3) Build time‐stamped GeoJSON features ----\n",
    "dt = datetime.timedelta(seconds=OutputWriteTime)\n",
    "\n",
    "features=[]\n",
    "current_time = start_date\n",
    "\n",
    "for step, fn in enumerate(all_vtu):\n",
    "    rdr = vtk.vtkXMLUnstructuredGridReader()\n",
    "    rdr.SetFileName(fn); rdr.Update()\n",
    "    data = rdr.GetOutput()\n",
    "    pts  = data.GetPoints()\n",
    "\n",
    "    # pull out the array, then bail out if it’s missing\n",
    "    src = data.GetPointData().GetArray(\"source\")\n",
    "    if src is None:\n",
    "        # no points to render this frame → skip timestamp bump if you like\n",
    "        continue\n",
    "\n",
    "    # advance your clock\n",
    "    current_time += dt\n",
    "    tstr = current_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    for i in range(pts.GetNumberOfPoints()):\n",
    "        lon, lat, _ = pts.GetPoint(i)\n",
    "        val         = src.GetValue(i)\n",
    "        features.append({\n",
    "            \"type\":\"Feature\",\n",
    "            \"geometry\":{\"type\":\"Point\",\"coordinates\":[lon,lat]},\n",
    "            \"properties\":{\n",
    "                \"time\": tstr,\n",
    "                \"icon\": \"circle\",\n",
    "                \"iconstyle\":{\n",
    "                    \"fillColor\": color_map[val],\n",
    "                    \"fillOpacity\": 0.7,\n",
    "                    \"stroke\": False,\n",
    "                    \"radius\": 4\n",
    "                },\n",
    "                \"popup\": f\"Step: {step}, Source: {val}\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "if not features:\n",
    "    raise RuntimeError(\"No features generated—check your VTUs and 'source' array.\")\n",
    "\n",
    "times = sorted({f['properties']['time'] for f in features})\n",
    "#print(\"All timestamps:\", times)\n",
    "\n",
    "geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "# ---- 4) Create & save the animated Folium map ----\n",
    "# center on the first feature\n",
    "first_lon, first_lat = features[0][\"geometry\"][\"coordinates\"]\n",
    "m = folium.Map(location=[first_lat, first_lon],\n",
    "               zoom_start=4, control_scale=True)\n",
    "\n",
    "measure_control = MeasureControl(\n",
    "    position=\"topleft\",\n",
    "    primary_length_unit=\"meters\",\n",
    "    secondary_length_unit=\"miles\",\n",
    "    active_color=\"red\",\n",
    "    completed_color=\"green\"\n",
    ")\n",
    "m.add_child(measure_control)\n",
    "\n",
    "TimestampedGeoJson(\n",
    "    data=geojson,\n",
    "    period=f\"PT{OutputWriteTime}S\",\n",
    "    duration       = \"PT0S\",   # ← show only the instant\n",
    "    transition_time=200,         # ms fade time\n",
    "    add_last_point=True,\n",
    "    auto_play=True,\n",
    "    loop=True,\n",
    "    time_slider_drag_update=True\n",
    ").add_to(m)\n",
    "\n",
    "out_html = os.path.join(dirout, \"animated_map.html\")\n",
    "m.save(out_html)\n",
    "print(\"✅ Saved animated map to\", out_html)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "dirout          = f\"{name}_out\"\n",
    "os.makedirs(dirout, exist_ok=True)\n",
    "\n",
    "# Collect & sort only numeric‐suffix VTUs ----\n",
    "all_vtu = [\n",
    "    fn for fn in glob.glob(pattern)\n",
    "    if re.search(rf'{re.escape(name)}_[0-9]+\\.vtu$', os.path.basename(fn))\n",
    "]\n",
    "if not all_vtu:\n",
    "    raise FileNotFoundError(f\"No timestep VTU files in {dirout!r}\")\n",
    "\n",
    "def seq(fn):\n",
    "    return int(re.search(r'_(\\d+)\\.vtu$', os.path.basename(fn)).group(1))\n",
    "\n",
    "all_vtu.sort(key=seq)\n",
    "\n",
    "# accumulate frames & timestamps\n",
    "dt          = datetime.timedelta(seconds=OutputWriteTime)\n",
    "heat_data   = []\n",
    "timestamps  = []\n",
    "all_lats    = []\n",
    "all_lons    = []\n",
    "current_time = start_date\n",
    "\n",
    "for fn in all_vtu:\n",
    "    rdr = vtk.vtkXMLUnstructuredGridReader()\n",
    "    rdr.SetFileName(fn); rdr.Update()\n",
    "    pts = rdr.GetOutput().GetPoints()\n",
    "    if not pts or pts.GetNumberOfPoints()==0:\n",
    "        continue\n",
    "\n",
    "    coords = []\n",
    "    for i in range(pts.GetNumberOfPoints()):\n",
    "        lon, lat, _ = pts.GetPoint(i)\n",
    "        coords.append([lat, lon])\n",
    "        all_lats.append(lat)\n",
    "        all_lons.append(lon)\n",
    "\n",
    "    current_time += dt\n",
    "    timestamps.append(current_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "    heat_data.append(coords)\n",
    "\n",
    "if not heat_data:\n",
    "    raise RuntimeError(\"No data extracted from VTUs\")\n",
    "\n",
    "# center map\n",
    "center_lat = np.mean(all_lats)\n",
    "center_lon = np.mean(all_lons)\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "\n",
    "# add measure tool\n",
    "m.add_child(MeasureControl())\n",
    "\n",
    "# static density layer\n",
    "HeatMap(\n",
    "    data=[pt for frame in heat_data for pt in frame],\n",
    "    min_opacity=0.3, radius=8, blur=12\n",
    ").add_to(folium.FeatureGroup(name=\"Overall Density\").add_to(m))\n",
    "\n",
    "# animated layer\n",
    "HeatMapWithTime(\n",
    "    data        = heat_data,\n",
    "    index       = timestamps,\n",
    "    name        = \"Time-series Heat\",\n",
    "    auto_play   = True,\n",
    "    max_opacity = 0.8,\n",
    "    radius      = 7,\n",
    "    position    = \"bottomright\"\n",
    ").add_to(m)\n",
    "\n",
    "# toggle control + save\n",
    "folium.LayerControl().add_to(m)\n",
    "out_path = os.path.join(dirout, \"heatmap_with_time.html\")\n",
    "m.save(out_path)\n",
    "print(\"✅ Saved animated heatmap to:\", out_path)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
